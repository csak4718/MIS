2015-10-19 08:50:29,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-19 08:50:29,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 08:50:30,819 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-10-19 08:50:31,606 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 08:50:31,822 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 08:50:31,822 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 08:50:31,872 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-19 08:50:31,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-10-19 08:50:31,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 08:50:31,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 08:50:32,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 08:50:32,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 08:50:32,321 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 08:50:32,352 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-19 08:50:32,381 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 08:50:32,393 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 08:50:32,395 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 08:50:32,395 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 08:50:32,395 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 08:50:32,423 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40925
2015-10-19 08:50:32,423 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 08:50:32,846 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40925
2015-10-19 08:50:33,175 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-19 08:50:33,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-10-19 08:50:33,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 08:50:33,445 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 08:50:33,494 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 08:50:33,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 08:50:33,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 08:50:33,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 08:50:33,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-10-19 08:50:33,724 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 08:50:33,730 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 08:50:34,219 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 3409@hadoopmaster
2015-10-19 08:50:34,229 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop-2.7.1/hdfs/datanode: namenode clusterID = CID-004b7cbc-e9c4-452a-b216-f67dc7b16a96; datanode clusterID = CID-acf69b3d-c21e-4af3-b19f-7c0a0cd2aed1
2015-10-19 08:50:34,231 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-19 08:50:34,243 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000
2015-10-19 08:50:34,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-19 08:50:36,246 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-19 08:50:36,248 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-19 08:50:36,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-10-19 09:39:15,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-19 09:39:15,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 09:39:16,197 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-10-19 09:39:16,702 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 09:39:16,862 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 09:39:16,862 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 09:39:16,880 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-19 09:39:16,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-10-19 09:39:16,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 09:39:16,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 09:39:16,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 09:39:16,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 09:39:17,175 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 09:39:17,205 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-19 09:39:17,216 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 09:39:17,228 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 09:39:17,233 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 09:39:17,233 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 09:39:17,233 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 09:39:17,252 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34950
2015-10-19 09:39:17,252 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 09:39:17,545 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34950
2015-10-19 09:39:17,734 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-19 09:39:17,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-10-19 09:39:17,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 09:39:17,970 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 09:39:18,006 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 09:39:18,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 09:39:18,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 09:39:18,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 09:39:18,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-10-19 09:39:18,179 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 09:39:18,181 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 09:39:18,536 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 4565@hadoopmaster
2015-10-19 09:39:18,541 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop-2.7.1/hdfs/datanode: namenode clusterID = CID-004b7cbc-e9c4-452a-b216-f67dc7b16a96; datanode clusterID = CID-acf69b3d-c21e-4af3-b19f-7c0a0cd2aed1
2015-10-19 09:39:18,542 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-19 09:39:18,546 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000
2015-10-19 09:39:18,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-19 09:39:20,553 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-19 09:39:20,556 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-19 09:39:20,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-10-19 16:35:15,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-19 16:35:15,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 16:35:16,489 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-10-19 16:35:17,117 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 16:35:17,296 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 16:35:17,296 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 16:35:17,315 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-19 16:35:17,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-10-19 16:35:17,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 16:35:17,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 16:35:17,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 16:35:17,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 16:35:17,587 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 16:35:17,611 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-19 16:35:17,629 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 16:35:17,641 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 16:35:17,643 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 16:35:17,644 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 16:35:17,644 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 16:35:17,660 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42914
2015-10-19 16:35:17,660 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 16:35:17,968 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42914
2015-10-19 16:35:18,170 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-19 16:35:18,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-10-19 16:35:18,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 16:35:18,297 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 16:35:18,341 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 16:35:18,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 16:35:18,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 16:35:18,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 16:35:18,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-10-19 16:35:18,585 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 16:35:18,588 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 16:35:18,989 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 9037@hadoopmaster
2015-10-19 16:35:19,120 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1892222280-192.168.54.130-1445296806381
2015-10-19 16:35:19,121 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381
2015-10-19 16:35:19,121 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381 is not formatted for BP-1892222280-192.168.54.130-1445296806381
2015-10-19 16:35:19,121 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-19 16:35:19,121 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1892222280-192.168.54.130-1445296806381 directory /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current
2015-10-19 16:35:19,122 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 16:35:19,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=693764022;bpid=BP-1892222280-192.168.54.130-1445296806381;lv=-56;nsInfo=lv=-63;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0;bpid=BP-1892222280-192.168.54.130-1445296806381;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-10-19 16:35:19,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-10-19 16:35:19,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-10-19 16:35:19,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 16:35:19,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-19 16:35:19,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-19 16:35:19,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1892222280-192.168.54.130-1445296806381 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 26ms
2015-10-19 16:35:19,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1892222280-192.168.54.130-1445296806381: 34ms
2015-10-19 16:35:19,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-19 16:35:19,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 0ms
2015-10-19 16:35:19,301 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2015-10-19 16:35:19,506 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode
2015-10-19 16:35:19,512 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445298857512 with interval 21600000
2015-10-19 16:35:19,514 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): finished scanning block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-19 16:35:19,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-19 16:35:19,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-19 16:35:19,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 16:35:19,625 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1814399880 ms.
2015-10-19 16:35:19,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=29
2015-10-19 16:35:19,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-10-19 16:35:19,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x12ce43c10dbb,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 55 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-19 16:35:19,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-19 16:45:34,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741825_1001 src: /192.168.54.130:56374 dest: /192.168.54.130:50010
2015-10-19 16:45:34,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56374, dest: /192.168.54.130:50010, bytes: 41, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1507935534_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741825_1001, duration: 85544873
2015-10-19 16:45:34,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:11,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741826_1002 src: /192.168.54.130:56382 dest: /192.168.54.130:50010
2015-10-19 16:49:11,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56382, dest: /192.168.54.130:50010, bytes: 273437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-303222980_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741826_1002, duration: 49083170
2015-10-19 16:49:11,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:11,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741827_1003 src: /192.168.54.130:56383 dest: /192.168.54.130:50010
2015-10-19 16:49:11,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56383, dest: /192.168.54.130:50010, bytes: 107, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-303222980_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741827_1003, duration: 46407100
2015-10-19 16:49:11,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-19 16:49:11,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741828_1004 src: /192.168.54.130:56385 dest: /192.168.54.130:50010
2015-10-19 16:49:11,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56385, dest: /192.168.54.130:50010, bytes: 26, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-303222980_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741828_1004, duration: 892012
2015-10-19 16:49:11,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:11,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741829_1005 src: /192.168.54.130:56386 dest: /192.168.54.130:50010
2015-10-19 16:49:11,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56386, dest: /192.168.54.130:50010, bytes: 95665, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-303222980_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741829_1005, duration: 2437667
2015-10-19 16:49:11,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:12,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741826_1002 to 192.168.54.131:50010 
2015-10-19 16:49:12,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741826_1002 (numBytes=273437) to /192.168.54.131:50010
2015-10-19 16:49:20,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741830_1006 src: /192.168.54.130:56396 dest: /192.168.54.130:50010
2015-10-19 16:49:20,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56396, dest: /192.168.54.130:50010, bytes: 113776, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1179230992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741830_1006, duration: 29882893
2015-10-19 16:49:20,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:26,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741831_1007 src: /192.168.54.130:56402 dest: /192.168.54.130:50010
2015-10-19 16:49:34,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56402, dest: /192.168.54.130:50010, bytes: 33530, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1179230992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741831_1007, duration: 8371440305
2015-10-19 16:49:34,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:34,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741833_1009 src: /192.168.54.130:56405 dest: /192.168.54.130:50010
2015-10-19 16:49:34,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56405, dest: /192.168.54.130:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1179230992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741833_1009, duration: 3307006
2015-10-19 16:49:34,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:34,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741834_1010 src: /192.168.54.130:56407 dest: /192.168.54.130:50010
2015-10-19 16:49:34,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56407, dest: /192.168.54.130:50010, bytes: 33530, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1179230992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741834_1010, duration: 3795539
2015-10-19 16:49:34,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:34,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741835_1011 src: /192.168.54.130:56408 dest: /192.168.54.130:50010
2015-10-19 16:49:34,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56408, dest: /192.168.54.130:50010, bytes: 113776, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1179230992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741835_1011, duration: 3628544
2015-10-19 16:49:34,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-19 16:49:36,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2015-10-19 16:49:36,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2015-10-19 16:49:36,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2015-10-19 16:49:36,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-10-19 16:49:36,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-10-19 16:49:36,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-10-19 16:49:36,710 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741826_1002 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741826
2015-10-19 16:49:36,713 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741827_1003 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741827
2015-10-19 16:49:36,719 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741828_1004 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741828
2015-10-19 16:49:36,724 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741829_1005 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741829
2015-10-19 16:49:36,728 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741830_1006 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741830
2015-10-19 16:49:36,728 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741831_1007 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741831
2015-10-19 16:54:17,542 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-19 17:12:00,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x14ceb353a132,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 10 msec to generate and 10 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-19 17:12:00,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-23 09:19:41,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741836_1012 src: /192.168.54.130:56563 dest: /192.168.54.130:50010
2015-10-23 09:19:41,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56563, dest: /192.168.54.130:50010, bytes: 41, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1302319805_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741836_1012, duration: 83840415
2015-10-23 09:19:41,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:19:42,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741837_1013 src: /192.168.54.130:56564 dest: /192.168.54.130:50010
2015-10-23 09:19:42,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56564, dest: /192.168.54.130:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1302319805_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741837_1013, duration: 1051652
2015-10-23 09:19:42,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:19:42,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741838_1014 src: /192.168.54.130:56565 dest: /192.168.54.130:50010
2015-10-23 09:19:42,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56565, dest: /192.168.54.130:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1302319805_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741838_1014, duration: 607183
2015-10-23 09:19:42,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:20:57,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-10-23 09:20:57,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-10-23 09:20:57,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-10-23 09:20:57,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-10-23 09:20:57,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741825_1001 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741825
2015-10-23 09:20:57,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741836_1012 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741836
2015-10-23 09:20:57,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741837_1013 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741837
2015-10-23 09:20:57,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741838_1014 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741838
2015-10-23 09:21:07,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741839_1015 src: /192.168.54.130:56572 dest: /192.168.54.130:50010
2015-10-23 09:21:07,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56572, dest: /192.168.54.130:50010, bytes: 41, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-450667999_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741839_1015, duration: 32659018
2015-10-23 09:21:07,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:21:07,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741840_1016 src: /192.168.54.130:56573 dest: /192.168.54.130:50010
2015-10-23 09:21:07,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56573, dest: /192.168.54.130:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-450667999_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741840_1016, duration: 2291962
2015-10-23 09:21:07,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:21:07,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741841_1017 src: /192.168.54.130:56574 dest: /192.168.54.130:50010
2015-10-23 09:21:07,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56574, dest: /192.168.54.130:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-450667999_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741841_1017, duration: 1637760
2015-10-23 09:21:07,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:26,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741842_1018 src: /192.168.54.130:56584 dest: /192.168.54.130:50010
2015-10-23 09:25:26,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56584, dest: /192.168.54.130:50010, bytes: 3164, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-508251874_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741842_1018, duration: 71463075
2015-10-23 09:25:26,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:26,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741843_1019 src: /192.168.54.130:56585 dest: /192.168.54.130:50010
2015-10-23 09:25:26,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56585, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-508251874_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741843_1019, duration: 14980049
2015-10-23 09:25:26,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741843_1019, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-23 09:25:26,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741844_1020 src: /192.168.54.130:56587 dest: /192.168.54.130:50010
2015-10-23 09:25:26,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56587, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-508251874_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741844_1020, duration: 832167
2015-10-23 09:25:26,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:26,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741845_1021 src: /192.168.54.130:56588 dest: /192.168.54.130:50010
2015-10-23 09:25:26,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56588, dest: /192.168.54.130:50010, bytes: 95741, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-508251874_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741845_1021, duration: 57706724
2015-10-23 09:25:26,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:30,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741842_1018 to 192.168.54.131:50010 
2015-10-23 09:25:30,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741842_1018 (numBytes=3164) to /192.168.54.131:50010
2015-10-23 09:25:31,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741846_1022 src: /192.168.54.130:56597 dest: /192.168.54.130:50010
2015-10-23 09:25:31,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56597, dest: /192.168.54.130:50010, bytes: 113876, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1331937552_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741846_1022, duration: 35834347
2015-10-23 09:25:31,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:41,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741847_1023 src: /192.168.54.130:56608 dest: /192.168.54.130:50010
2015-10-23 09:25:48,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56608, dest: /192.168.54.130:50010, bytes: 46908, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1331937552_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741847_1023, duration: 6892373820
2015-10-23 09:25:48,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:48,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741849_1025 src: /192.168.54.130:56617 dest: /192.168.54.130:50010
2015-10-23 09:25:48,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56617, dest: /192.168.54.130:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1331937552_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741849_1025, duration: 3846286
2015-10-23 09:25:48,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:48,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741850_1026 src: /192.168.54.130:56619 dest: /192.168.54.130:50010
2015-10-23 09:25:48,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56619, dest: /192.168.54.130:50010, bytes: 46908, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1331937552_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741850_1026, duration: 8342945
2015-10-23 09:25:48,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:48,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741851_1027 src: /192.168.54.130:56620 dest: /192.168.54.130:50010
2015-10-23 09:25:48,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:56620, dest: /192.168.54.130:50010, bytes: 113876, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1331937552_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741851_1027, duration: 16879875
2015-10-23 09:25:48,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-23 09:25:57,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-10-23 09:25:57,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-10-23 09:25:57,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741842_1018 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741842
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741843_1019 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741843
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741844_1020 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741844
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741845_1021 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741845
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741846_1022 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741846
2015-10-23 09:25:57,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741847_1023 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741847
2015-10-23 13:25:08,500 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 9, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-23 13:42:51,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2873d7189bc2,  containing 1 storage report(s), of which we sent 1. The reports had 9 total blocks and used 1 RPC(s). This took 1 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-23 13:42:51,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-24 10:56:20,402 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 9, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-24 12:43:30,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-24 12:43:30,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-24 12:43:30,767 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-10-24 12:43:31,301 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-24 12:43:31,439 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-24 12:43:31,439 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-24 12:43:31,456 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-24 12:43:31,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-10-24 12:43:31,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-24 12:43:31,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-24 12:43:31,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-24 12:43:31,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-24 12:43:31,716 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-24 12:43:31,739 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-24 12:43:31,758 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-24 12:43:31,772 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-24 12:43:31,778 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-24 12:43:31,778 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-24 12:43:31,778 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-24 12:43:31,796 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 53939
2015-10-24 12:43:31,797 INFO org.mortbay.log: jetty-6.1.26
2015-10-24 12:43:32,128 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:53939
2015-10-24 12:43:32,299 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-24 12:43:32,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-10-24 12:43:32,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-24 12:43:32,408 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-24 12:43:32,439 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-24 12:43:32,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-24 12:43:32,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-24 12:43:32,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-24 12:43:32,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-10-24 12:43:32,603 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-24 12:43:32,607 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-24 12:43:32,869 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 4071@hadoopmaster
2015-10-24 12:43:32,983 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1892222280-192.168.54.130-1445296806381
2015-10-24 12:43:32,983 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381
2015-10-24 12:43:32,984 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-24 12:43:32,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=693764022;bpid=BP-1892222280-192.168.54.130-1445296806381;lv=-56;nsInfo=lv=-63;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0;bpid=BP-1892222280-192.168.54.130-1445296806381;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-10-24 12:43:33,117 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-10-24 12:43:33,117 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-10-24 12:43:33,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-24 12:43:33,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-24 12:43:33,127 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-24 12:43:33,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1892222280-192.168.54.130-1445296806381 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 16ms
2015-10-24 12:43:33,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1892222280-192.168.54.130-1445296806381: 17ms
2015-10-24 12:43:33,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-24 12:43:33,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 3ms
2015-10-24 12:43:33,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-24 12:43:33,442 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1396306063 ms.
2015-10-24 12:43:33,444 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445734961444 with interval 21600000
2015-10-24 12:43:33,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-24 12:43:33,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-24 12:43:33,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-24 12:43:33,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=1759
2015-10-24 12:43:33,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-10-24 12:43:33,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3316d6f32cd,  containing 1 storage report(s), of which we sent 1. The reports had 9 total blocks and used 1 RPC(s). This took 6 msec to generate and 40 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-24 12:43:33,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-24 12:46:02,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741852_1028 src: /192.168.54.130:53090 dest: /192.168.54.130:50010
2015-10-24 12:46:02,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53090, dest: /192.168.54.130:50010, bytes: 3742, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_352160919_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741852_1028, duration: 118376088
2015-10-24 12:46:02,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 12:46:02,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741853_1029 src: /192.168.54.130:53091 dest: /192.168.54.130:50010
2015-10-24 12:46:02,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53091, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_352160919_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741853_1029, duration: 11395732
2015-10-24 12:46:02,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741853_1029, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 12:46:02,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741854_1030 src: /192.168.54.130:53093 dest: /192.168.54.130:50010
2015-10-24 12:46:02,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53093, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_352160919_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741854_1030, duration: 777551
2015-10-24 12:46:02,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 12:46:02,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741855_1031 src: /192.168.54.130:53094 dest: /192.168.54.130:50010
2015-10-24 12:46:02,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53094, dest: /192.168.54.130:50010, bytes: 95711, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_352160919_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741855_1031, duration: 7565617
2015-10-24 12:46:02,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 12:46:05,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741852_1028 to 192.168.54.131:50010 
2015-10-24 12:46:05,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741852_1028 (numBytes=3742) to /192.168.54.131:50010
2015-10-24 12:46:47,613 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2015-10-24 12:46:47,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-10-24 12:46:47,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-10-24 12:46:47,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2015-10-24 12:46:47,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741852_1028 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741852
2015-10-24 12:46:47,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741853_1029 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741853
2015-10-24 12:46:47,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741854_1030 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741854
2015-10-24 12:46:47,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741855_1031 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741855
2015-10-24 13:00:06,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741861_1037 src: /192.168.54.130:53149 dest: /192.168.54.130:50010
2015-10-24 13:00:06,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53149, dest: /192.168.54.130:50010, bytes: 3742, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1778444364_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741861_1037, duration: 37707376
2015-10-24 13:00:06,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:00:06,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741862_1038 src: /192.168.54.130:53150 dest: /192.168.54.130:50010
2015-10-24 13:00:06,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53150, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1778444364_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741862_1038, duration: 4412449
2015-10-24 13:00:06,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741862_1038, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 13:00:06,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741863_1039 src: /192.168.54.130:53152 dest: /192.168.54.130:50010
2015-10-24 13:00:06,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53152, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1778444364_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741863_1039, duration: 1167048
2015-10-24 13:00:06,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:00:06,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741864_1040 src: /192.168.54.130:53153 dest: /192.168.54.130:50010
2015-10-24 13:00:06,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53153, dest: /192.168.54.130:50010, bytes: 95711, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1778444364_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741864_1040, duration: 2693850
2015-10-24 13:00:06,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:00:08,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741861_1037 to 192.168.54.131:50010 
2015-10-24 13:00:08,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741861_1037 (numBytes=3742) to /192.168.54.131:50010
2015-10-24 13:00:50,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2015-10-24 13:00:50,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2015-10-24 13:00:50,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2015-10-24 13:00:50,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2015-10-24 13:00:50,604 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741861_1037 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741861
2015-10-24 13:00:50,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741862_1038 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741862
2015-10-24 13:00:50,609 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741863_1039 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741863
2015-10-24 13:00:50,609 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741864_1040 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741864
2015-10-24 13:01:52,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741870_1046 src: /192.168.54.130:53183 dest: /192.168.54.130:50010
2015-10-24 13:01:52,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53183, dest: /192.168.54.130:50010, bytes: 3777, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_578444518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741870_1046, duration: 27249406
2015-10-24 13:01:52,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:01:52,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741871_1047 src: /192.168.54.130:53184 dest: /192.168.54.130:50010
2015-10-24 13:01:52,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53184, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_578444518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741871_1047, duration: 8225919
2015-10-24 13:01:52,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741871_1047, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 13:01:52,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741872_1048 src: /192.168.54.130:53186 dest: /192.168.54.130:50010
2015-10-24 13:01:52,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53186, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_578444518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741872_1048, duration: 1085559
2015-10-24 13:01:52,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:01:53,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741873_1049 src: /192.168.54.130:53187 dest: /192.168.54.130:50010
2015-10-24 13:01:53,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53187, dest: /192.168.54.130:50010, bytes: 95984, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_578444518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741873_1049, duration: 2178731
2015-10-24 13:01:53,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:01:56,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741870_1046 to 192.168.54.131:50010 
2015-10-24 13:01:56,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741870_1046 (numBytes=3777) to /192.168.54.131:50010
2015-10-24 13:02:26,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2015-10-24 13:02:26,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2015-10-24 13:02:26,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2015-10-24 13:02:26,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2015-10-24 13:02:26,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741872_1048 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741872
2015-10-24 13:02:26,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741873_1049 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741873
2015-10-24 13:02:26,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741870_1046 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741870
2015-10-24 13:02:26,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741871_1047 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741871
2015-10-24 13:05:40,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741879_1055 src: /192.168.54.130:53227 dest: /192.168.54.130:50010
2015-10-24 13:05:40,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53227, dest: /192.168.54.130:50010, bytes: 3756, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_961239900_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741879_1055, duration: 29894328
2015-10-24 13:05:40,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:05:40,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741880_1056 src: /192.168.54.130:53228 dest: /192.168.54.130:50010
2015-10-24 13:05:40,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53228, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_961239900_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741880_1056, duration: 7631677
2015-10-24 13:05:40,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741880_1056, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 13:05:40,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741881_1057 src: /192.168.54.130:53230 dest: /192.168.54.130:50010
2015-10-24 13:05:40,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53230, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_961239900_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741881_1057, duration: 729012
2015-10-24 13:05:40,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:05:40,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741882_1058 src: /192.168.54.130:53231 dest: /192.168.54.130:50010
2015-10-24 13:05:40,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53231, dest: /192.168.54.130:50010, bytes: 95865, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_961239900_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741882_1058, duration: 3568446
2015-10-24 13:05:40,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:05:41,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741879_1055 to 192.168.54.131:50010 
2015-10-24 13:05:41,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741879_1055 (numBytes=3756) to /192.168.54.131:50010
2015-10-24 13:06:01,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741885_1061 src: /192.168.54.130:53254 dest: /192.168.54.130:50010
2015-10-24 13:06:01,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53254, dest: /192.168.54.130:50010, bytes: 89, op: HDFS_WRITE, cliID: DFSClient_attempt_1445715765354_0004_r_000000_0_1190521357_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741885_1061, duration: 37162883
2015-10-24 13:06:01,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 13:06:08,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2015-10-24 13:06:08,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2015-10-24 13:06:08,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2015-10-24 13:06:08,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2015-10-24 13:06:08,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741879_1055 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741879
2015-10-24 13:06:08,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741880_1056 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741880
2015-10-24 13:06:08,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741881_1057 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741881
2015-10-24 13:06:08,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741882_1058 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741882
2015-10-24 14:14:20,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2015-10-24 14:14:20,604 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741885_1061 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741885
2015-10-24 14:14:33,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741889_1065 src: /192.168.54.130:53433 dest: /192.168.54.130:50010
2015-10-24 14:14:33,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53433, dest: /192.168.54.130:50010, bytes: 8083, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_39815787_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741889_1065, duration: 26899664
2015-10-24 14:14:33,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:14:33,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741890_1066 src: /192.168.54.130:53434 dest: /192.168.54.130:50010
2015-10-24 14:14:33,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53434, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_39815787_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741890_1066, duration: 7754345
2015-10-24 14:14:33,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741890_1066, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 14:14:33,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741891_1067 src: /192.168.54.130:53436 dest: /192.168.54.130:50010
2015-10-24 14:14:33,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53436, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_39815787_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741891_1067, duration: 1020865
2015-10-24 14:14:33,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:14:33,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741892_1068 src: /192.168.54.130:53437 dest: /192.168.54.130:50010
2015-10-24 14:14:33,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53437, dest: /192.168.54.130:50010, bytes: 95865, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_39815787_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741892_1068, duration: 8036228
2015-10-24 14:14:33,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741892_1068, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:14:38,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741889_1065 to 192.168.54.131:50010 
2015-10-24 14:14:38,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741889_1065 (numBytes=8083) to /192.168.54.131:50010
2015-10-24 14:14:54,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741895_1071 src: /192.168.54.130:53460 dest: /192.168.54.130:50010
2015-10-24 14:14:54,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53460, dest: /192.168.54.130:50010, bytes: 89, op: HDFS_WRITE, cliID: DFSClient_attempt_1445715765354_0005_r_000000_0_1175064542_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741895_1071, duration: 32507621
2015-10-24 14:14:54,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741895_1071, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:14:59,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2015-10-24 14:14:59,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741889_1065 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741889
2015-10-24 14:14:59,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2015-10-24 14:14:59,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741890_1066 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741890
2015-10-24 14:14:59,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2015-10-24 14:14:59,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741891_1067 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741891
2015-10-24 14:14:59,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2015-10-24 14:14:59,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741892_1068 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741892
2015-10-24 14:21:56,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2015-10-24 14:21:56,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741895_1071 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741895
2015-10-24 14:29:47,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741899_1075 src: /192.168.54.130:53487 dest: /192.168.54.130:50010
2015-10-24 14:29:48,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53487, dest: /192.168.54.130:50010, bytes: 4503, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-455034318_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741899_1075, duration: 60280689
2015-10-24 14:29:48,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:29:48,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741900_1076 src: /192.168.54.130:53488 dest: /192.168.54.130:50010
2015-10-24 14:29:48,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53488, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-455034318_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741900_1076, duration: 15707680
2015-10-24 14:29:48,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741900_1076, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 14:29:48,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741901_1077 src: /192.168.54.130:53490 dest: /192.168.54.130:50010
2015-10-24 14:29:48,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53490, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-455034318_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741901_1077, duration: 711763
2015-10-24 14:29:48,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:29:48,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741902_1078 src: /192.168.54.130:53491 dest: /192.168.54.130:50010
2015-10-24 14:29:48,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53491, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-455034318_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741902_1078, duration: 3564212
2015-10-24 14:29:48,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741902_1078, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:29:53,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741903_1079 src: /192.168.54.130:53500 dest: /192.168.54.130:50010
2015-10-24 14:29:53,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53500, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1041693583_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741903_1079, duration: 27157311
2015-10-24 14:29:53,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:29:53,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741899_1075 to 192.168.54.131:50010 
2015-10-24 14:29:53,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741899_1075 (numBytes=4503) to /192.168.54.131:50010
2015-10-24 14:30:09,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741904_1080 src: /192.168.54.130:53521 dest: /192.168.54.130:50010
2015-10-24 14:30:14,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53521, dest: /192.168.54.130:50010, bytes: 46353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1041693583_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741904_1080, duration: 4695825487
2015-10-24 14:30:14,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741904_1080, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:30:14,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741906_1082 src: /192.168.54.130:53524 dest: /192.168.54.130:50010
2015-10-24 14:30:14,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53524, dest: /192.168.54.130:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1041693583_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741906_1082, duration: 2998276
2015-10-24 14:30:14,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:30:14,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741907_1083 src: /192.168.54.130:53526 dest: /192.168.54.130:50010
2015-10-24 14:30:14,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53526, dest: /192.168.54.130:50010, bytes: 46353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1041693583_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741907_1083, duration: 5201332
2015-10-24 14:30:14,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741907_1083, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:30:14,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741908_1084 src: /192.168.54.130:53527 dest: /192.168.54.130:50010
2015-10-24 14:30:14,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53527, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1041693583_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741908_1084, duration: 7925777
2015-10-24 14:30:14,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741908_1084, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:30:17,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741904_1080 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741904 for deletion
2015-10-24 14:30:17,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2015-10-24 14:30:17,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2015-10-24 14:30:17,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2015-10-24 14:30:17,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2015-10-24 14:30:17,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2015-10-24 14:30:17,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741904_1080 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741904
2015-10-24 14:30:17,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741899_1075 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741899
2015-10-24 14:30:17,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741900_1076 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741900
2015-10-24 14:30:17,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741901_1077 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741901
2015-10-24 14:30:17,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741902_1078 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741902
2015-10-24 14:30:17,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741903_1079 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741903
2015-10-24 14:40:27,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741909_1085 src: /192.168.54.130:53546 dest: /192.168.54.130:50010
2015-10-24 14:40:27,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53546, dest: /192.168.54.130:50010, bytes: 4475, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-55450623_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741909_1085, duration: 31100734
2015-10-24 14:40:27,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741909_1085, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:40:28,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741910_1086 src: /192.168.54.130:53547 dest: /192.168.54.130:50010
2015-10-24 14:40:28,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53547, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-55450623_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741910_1086, duration: 5996816
2015-10-24 14:40:28,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741910_1086, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 14:40:28,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741911_1087 src: /192.168.54.130:53549 dest: /192.168.54.130:50010
2015-10-24 14:40:28,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53549, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-55450623_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741911_1087, duration: 753709
2015-10-24 14:40:28,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741911_1087, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:40:28,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741912_1088 src: /192.168.54.130:53550 dest: /192.168.54.130:50010
2015-10-24 14:40:28,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53550, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-55450623_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741912_1088, duration: 1989342
2015-10-24 14:40:28,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741912_1088, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:40:29,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741909_1085 to 192.168.54.131:50010 
2015-10-24 14:40:29,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741909_1085 (numBytes=4475) to /192.168.54.131:50010
2015-10-24 14:40:47,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741915_1091 src: /192.168.54.130:53574 dest: /192.168.54.130:50010
2015-10-24 14:40:47,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53574, dest: /192.168.54.130:50010, bytes: 377, op: HDFS_WRITE, cliID: DFSClient_attempt_1445715765354_0007_r_000000_0_2078527935_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741915_1091, duration: 27441864
2015-10-24 14:40:47,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741915_1091, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:40:53,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2015-10-24 14:40:53,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2015-10-24 14:40:53,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2015-10-24 14:40:53,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2015-10-24 14:40:53,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741909_1085 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741909
2015-10-24 14:40:53,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741910_1086 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741910
2015-10-24 14:40:53,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741911_1087 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741911
2015-10-24 14:40:53,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741912_1088 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741912
2015-10-24 14:48:11,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2015-10-24 14:48:11,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741915_1091 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741915
2015-10-24 14:48:37,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741919_1095 src: /192.168.54.130:53590 dest: /192.168.54.130:50010
2015-10-24 14:48:38,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53590, dest: /192.168.54.130:50010, bytes: 4513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1300208218_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741919_1095, duration: 26773737
2015-10-24 14:48:38,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741919_1095, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:48:38,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741920_1096 src: /192.168.54.130:53591 dest: /192.168.54.130:50010
2015-10-24 14:48:38,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53591, dest: /192.168.54.130:50010, bytes: 272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1300208218_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741920_1096, duration: 5218150
2015-10-24 14:48:38,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741920_1096, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 14:48:38,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741921_1097 src: /192.168.54.130:53593 dest: /192.168.54.130:50010
2015-10-24 14:48:38,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53593, dest: /192.168.54.130:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1300208218_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741921_1097, duration: 764264
2015-10-24 14:48:38,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741921_1097, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:48:38,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741922_1098 src: /192.168.54.130:53594 dest: /192.168.54.130:50010
2015-10-24 14:48:38,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53594, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1300208218_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741922_1098, duration: 3836232
2015-10-24 14:48:38,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741922_1098, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:48:41,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741919_1095 to 192.168.54.131:50010 
2015-10-24 14:48:41,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741919_1095 (numBytes=4513) to /192.168.54.131:50010
2015-10-24 14:49:05,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2015-10-24 14:49:05,602 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741920_1096 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741920
2015-10-24 14:49:05,602 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2015-10-24 14:49:05,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741921_1097 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741921
2015-10-24 14:49:05,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2015-10-24 14:49:05,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741922_1098 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741922
2015-10-24 14:49:05,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2015-10-24 14:49:05,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741919_1095 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741919
2015-10-24 14:53:41,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-10-24 14:53:41,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-10-24 14:53:41,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-10-24 14:53:41,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741840_1016 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741840
2015-10-24 14:53:41,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741841_1017 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741841
2015-10-24 14:53:41,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741839_1015 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741839
2015-10-24 14:55:01,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741929_1105 src: /192.168.54.130:53626 dest: /192.168.54.130:50010
2015-10-24 14:55:01,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53626, dest: /192.168.54.130:50010, bytes: 663, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741929_1105, duration: 38104349
2015-10-24 14:55:01,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741929_1105, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741930_1106 src: /192.168.54.130:53627 dest: /192.168.54.130:50010
2015-10-24 14:55:01,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53627, dest: /192.168.54.130:50010, bytes: 4491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741930_1106, duration: 3536874
2015-10-24 14:55:01,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741930_1106, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741931_1107 src: /192.168.54.130:53628 dest: /192.168.54.130:50010
2015-10-24 14:55:01,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53628, dest: /192.168.54.130:50010, bytes: 4299, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741931_1107, duration: 1255315
2015-10-24 14:55:01,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741931_1107, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741932_1108 src: /192.168.54.130:53629 dest: /192.168.54.130:50010
2015-10-24 14:55:01,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53629, dest: /192.168.54.130:50010, bytes: 9525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741932_1108, duration: 1795601
2015-10-24 14:55:01,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741932_1108, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741933_1109 src: /192.168.54.130:53630 dest: /192.168.54.130:50010
2015-10-24 14:55:01,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53630, dest: /192.168.54.130:50010, bytes: 5957, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741933_1109, duration: 1152921
2015-10-24 14:55:01,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741933_1109, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741934_1110 src: /192.168.54.130:53631 dest: /192.168.54.130:50010
2015-10-24 14:55:01,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53631, dest: /192.168.54.130:50010, bytes: 5268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741934_1110, duration: 1308825
2015-10-24 14:55:01,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741934_1110, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741935_1111 src: /192.168.54.130:53632 dest: /192.168.54.130:50010
2015-10-24 14:55:01,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53632, dest: /192.168.54.130:50010, bytes: 13016, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741935_1111, duration: 1211898
2015-10-24 14:55:01,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741935_1111, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741936_1112 src: /192.168.54.130:53633 dest: /192.168.54.130:50010
2015-10-24 14:55:01,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53633, dest: /192.168.54.130:50010, bytes: 11168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741936_1112, duration: 5880871
2015-10-24 14:55:01,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741936_1112, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741937_1113 src: /192.168.54.130:53634 dest: /192.168.54.130:50010
2015-10-24 14:55:01,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53634, dest: /192.168.54.130:50010, bytes: 10990, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741937_1113, duration: 4498148
2015-10-24 14:55:01,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741937_1113, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741938_1114 src: /192.168.54.130:53635 dest: /192.168.54.130:50010
2015-10-24 14:55:01,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53635, dest: /192.168.54.130:50010, bytes: 9742, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741938_1114, duration: 897129
2015-10-24 14:55:01,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741938_1114, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741939_1115 src: /192.168.54.130:53636 dest: /192.168.54.130:50010
2015-10-24 14:55:01,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53636, dest: /192.168.54.130:50010, bytes: 12483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741939_1115, duration: 2046797
2015-10-24 14:55:01,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741939_1115, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:01,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741940_1116 src: /192.168.54.130:53637 dest: /192.168.54.130:50010
2015-10-24 14:55:02,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53637, dest: /192.168.54.130:50010, bytes: 8938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741940_1116, duration: 739015
2015-10-24 14:55:02,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741940_1116, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741941_1117 src: /192.168.54.130:53638 dest: /192.168.54.130:50010
2015-10-24 14:55:02,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53638, dest: /192.168.54.130:50010, bytes: 3948, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741941_1117, duration: 1618464
2015-10-24 14:55:02,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741941_1117, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741942_1118 src: /192.168.54.130:53639 dest: /192.168.54.130:50010
2015-10-24 14:55:02,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53639, dest: /192.168.54.130:50010, bytes: 9222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741942_1118, duration: 769478
2015-10-24 14:55:02,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741942_1118, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741943_1119 src: /192.168.54.130:53640 dest: /192.168.54.130:50010
2015-10-24 14:55:02,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53640, dest: /192.168.54.130:50010, bytes: 6459, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741943_1119, duration: 1033291
2015-10-24 14:55:02,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741943_1119, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741944_1120 src: /192.168.54.130:53641 dest: /192.168.54.130:50010
2015-10-24 14:55:02,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53641, dest: /192.168.54.130:50010, bytes: 9811, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741944_1120, duration: 1404652
2015-10-24 14:55:02,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741944_1120, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741945_1121 src: /192.168.54.130:53642 dest: /192.168.54.130:50010
2015-10-24 14:55:02,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53642, dest: /192.168.54.130:50010, bytes: 19000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741945_1121, duration: 897869
2015-10-24 14:55:02,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741945_1121, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741946_1122 src: /192.168.54.130:53643 dest: /192.168.54.130:50010
2015-10-24 14:55:02,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53643, dest: /192.168.54.130:50010, bytes: 7279, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741946_1122, duration: 886383
2015-10-24 14:55:02,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741946_1122, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741947_1123 src: /192.168.54.130:53644 dest: /192.168.54.130:50010
2015-10-24 14:55:02,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53644, dest: /192.168.54.130:50010, bytes: 29112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741947_1123, duration: 2050291
2015-10-24 14:55:02,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741947_1123, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741948_1124 src: /192.168.54.130:53645 dest: /192.168.54.130:50010
2015-10-24 14:55:02,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53645, dest: /192.168.54.130:50010, bytes: 10673, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741948_1124, duration: 1801118
2015-10-24 14:55:02,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741948_1124, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741949_1125 src: /192.168.54.130:53646 dest: /192.168.54.130:50010
2015-10-24 14:55:02,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53646, dest: /192.168.54.130:50010, bytes: 9176, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741949_1125, duration: 1083247
2015-10-24 14:55:02,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741949_1125, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741950_1126 src: /192.168.54.130:53647 dest: /192.168.54.130:50010
2015-10-24 14:55:02,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53647, dest: /192.168.54.130:50010, bytes: 11230, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741950_1126, duration: 1404035
2015-10-24 14:55:02,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741950_1126, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741951_1127 src: /192.168.54.130:53648 dest: /192.168.54.130:50010
2015-10-24 14:55:02,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53648, dest: /192.168.54.130:50010, bytes: 9954, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741951_1127, duration: 562893
2015-10-24 14:55:02,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741951_1127, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741952_1128 src: /192.168.54.130:53649 dest: /192.168.54.130:50010
2015-10-24 14:55:02,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53649, dest: /192.168.54.130:50010, bytes: 9328, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741952_1128, duration: 1028190
2015-10-24 14:55:02,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741952_1128, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741953_1129 src: /192.168.54.130:53650 dest: /192.168.54.130:50010
2015-10-24 14:55:02,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53650, dest: /192.168.54.130:50010, bytes: 10827, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741953_1129, duration: 609059
2015-10-24 14:55:02,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741953_1129, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741954_1130 src: /192.168.54.130:53651 dest: /192.168.54.130:50010
2015-10-24 14:55:02,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53651, dest: /192.168.54.130:50010, bytes: 8573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741954_1130, duration: 612652
2015-10-24 14:55:02,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741954_1130, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741955_1131 src: /192.168.54.130:53652 dest: /192.168.54.130:50010
2015-10-24 14:55:02,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53652, dest: /192.168.54.130:50010, bytes: 12751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741955_1131, duration: 720415
2015-10-24 14:55:02,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741955_1131, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741956_1132 src: /192.168.54.130:53653 dest: /192.168.54.130:50010
2015-10-24 14:55:02,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53653, dest: /192.168.54.130:50010, bytes: 7216, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741956_1132, duration: 495582
2015-10-24 14:55:02,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741956_1132, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741957_1133 src: /192.168.54.130:53654 dest: /192.168.54.130:50010
2015-10-24 14:55:02,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53654, dest: /192.168.54.130:50010, bytes: 8245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741957_1133, duration: 525774
2015-10-24 14:55:02,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741957_1133, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741958_1134 src: /192.168.54.130:53655 dest: /192.168.54.130:50010
2015-10-24 14:55:02,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53655, dest: /192.168.54.130:50010, bytes: 13713, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741958_1134, duration: 544427
2015-10-24 14:55:02,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741958_1134, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741959_1135 src: /192.168.54.130:53656 dest: /192.168.54.130:50010
2015-10-24 14:55:02,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53656, dest: /192.168.54.130:50010, bytes: 7074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741959_1135, duration: 530731
2015-10-24 14:55:02,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741959_1135, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741960_1136 src: /192.168.54.130:53657 dest: /192.168.54.130:50010
2015-10-24 14:55:02,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53657, dest: /192.168.54.130:50010, bytes: 8670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741960_1136, duration: 563550
2015-10-24 14:55:02,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741960_1136, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741961_1137 src: /192.168.54.130:53658 dest: /192.168.54.130:50010
2015-10-24 14:55:02,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53658, dest: /192.168.54.130:50010, bytes: 8483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741961_1137, duration: 584439
2015-10-24 14:55:02,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741961_1137, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741962_1138 src: /192.168.54.130:53659 dest: /192.168.54.130:50010
2015-10-24 14:55:02,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53659, dest: /192.168.54.130:50010, bytes: 10347, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741962_1138, duration: 562716
2015-10-24 14:55:02,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741962_1138, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741963_1139 src: /192.168.54.130:53660 dest: /192.168.54.130:50010
2015-10-24 14:55:02,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53660, dest: /192.168.54.130:50010, bytes: 12018, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741963_1139, duration: 1544420
2015-10-24 14:55:02,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741963_1139, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741964_1140 src: /192.168.54.130:53661 dest: /192.168.54.130:50010
2015-10-24 14:55:02,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53661, dest: /192.168.54.130:50010, bytes: 16999, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741964_1140, duration: 570249
2015-10-24 14:55:02,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741964_1140, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741965_1141 src: /192.168.54.130:53662 dest: /192.168.54.130:50010
2015-10-24 14:55:02,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53662, dest: /192.168.54.130:50010, bytes: 11799, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741965_1141, duration: 980142
2015-10-24 14:55:02,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741965_1141, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741966_1142 src: /192.168.54.130:53663 dest: /192.168.54.130:50010
2015-10-24 14:55:02,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53663, dest: /192.168.54.130:50010, bytes: 7797, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741966_1142, duration: 569547
2015-10-24 14:55:02,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741966_1142, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741967_1143 src: /192.168.54.130:53664 dest: /192.168.54.130:50010
2015-10-24 14:55:02,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53664, dest: /192.168.54.130:50010, bytes: 5937, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741967_1143, duration: 609451
2015-10-24 14:55:02,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741967_1143, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741968_1144 src: /192.168.54.130:53665 dest: /192.168.54.130:50010
2015-10-24 14:55:02,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53665, dest: /192.168.54.130:50010, bytes: 8751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741968_1144, duration: 481011
2015-10-24 14:55:02,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741968_1144, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741969_1145 src: /192.168.54.130:53666 dest: /192.168.54.130:50010
2015-10-24 14:55:02,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53666, dest: /192.168.54.130:50010, bytes: 9168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741969_1145, duration: 920017
2015-10-24 14:55:02,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741969_1145, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741970_1146 src: /192.168.54.130:53667 dest: /192.168.54.130:50010
2015-10-24 14:55:02,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53667, dest: /192.168.54.130:50010, bytes: 13006, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741970_1146, duration: 521147
2015-10-24 14:55:02,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741970_1146, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741971_1147 src: /192.168.54.130:53668 dest: /192.168.54.130:50010
2015-10-24 14:55:02,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53668, dest: /192.168.54.130:50010, bytes: 10662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741971_1147, duration: 596832
2015-10-24 14:55:02,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741971_1147, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741972_1148 src: /192.168.54.130:53669 dest: /192.168.54.130:50010
2015-10-24 14:55:02,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53669, dest: /192.168.54.130:50010, bytes: 27491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741972_1148, duration: 505495
2015-10-24 14:55:02,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741972_1148, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741973_1149 src: /192.168.54.130:53670 dest: /192.168.54.130:50010
2015-10-24 14:55:02,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53670, dest: /192.168.54.130:50010, bytes: 13387, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741973_1149, duration: 780082
2015-10-24 14:55:02,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741973_1149, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741974_1150 src: /192.168.54.130:53671 dest: /192.168.54.130:50010
2015-10-24 14:55:02,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53671, dest: /192.168.54.130:50010, bytes: 10094, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741974_1150, duration: 466835
2015-10-24 14:55:02,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741974_1150, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741975_1151 src: /192.168.54.130:53672 dest: /192.168.54.130:50010
2015-10-24 14:55:02,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53672, dest: /192.168.54.130:50010, bytes: 16889, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741975_1151, duration: 802263
2015-10-24 14:55:02,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741975_1151, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741976_1152 src: /192.168.54.130:53673 dest: /192.168.54.130:50010
2015-10-24 14:55:02,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53673, dest: /192.168.54.130:50010, bytes: 22332, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741976_1152, duration: 387851
2015-10-24 14:55:02,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741976_1152, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741977_1153 src: /192.168.54.130:53674 dest: /192.168.54.130:50010
2015-10-24 14:55:02,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53674, dest: /192.168.54.130:50010, bytes: 12552, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741977_1153, duration: 2513327
2015-10-24 14:55:02,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741977_1153, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741978_1154 src: /192.168.54.130:53675 dest: /192.168.54.130:50010
2015-10-24 14:55:02,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53675, dest: /192.168.54.130:50010, bytes: 12248, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741978_1154, duration: 486306
2015-10-24 14:55:02,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741978_1154, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741979_1155 src: /192.168.54.130:53676 dest: /192.168.54.130:50010
2015-10-24 14:55:02,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53676, dest: /192.168.54.130:50010, bytes: 12497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741979_1155, duration: 1106862
2015-10-24 14:55:02,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741979_1155, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741980_1156 src: /192.168.54.130:53677 dest: /192.168.54.130:50010
2015-10-24 14:55:02,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53677, dest: /192.168.54.130:50010, bytes: 11243, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741980_1156, duration: 674645
2015-10-24 14:55:02,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741980_1156, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741981_1157 src: /192.168.54.130:53678 dest: /192.168.54.130:50010
2015-10-24 14:55:02,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53678, dest: /192.168.54.130:50010, bytes: 16578, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741981_1157, duration: 442333
2015-10-24 14:55:02,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741981_1157, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741982_1158 src: /192.168.54.130:53679 dest: /192.168.54.130:50010
2015-10-24 14:55:02,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53679, dest: /192.168.54.130:50010, bytes: 16110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741982_1158, duration: 479298
2015-10-24 14:55:02,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741982_1158, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741983_1159 src: /192.168.54.130:53680 dest: /192.168.54.130:50010
2015-10-24 14:55:02,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53680, dest: /192.168.54.130:50010, bytes: 8866, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741983_1159, duration: 917622
2015-10-24 14:55:02,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741983_1159, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741984_1160 src: /192.168.54.130:53681 dest: /192.168.54.130:50010
2015-10-24 14:55:02,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53681, dest: /192.168.54.130:50010, bytes: 12994, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741984_1160, duration: 501288
2015-10-24 14:55:02,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741984_1160, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741985_1161 src: /192.168.54.130:53682 dest: /192.168.54.130:50010
2015-10-24 14:55:02,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53682, dest: /192.168.54.130:50010, bytes: 15319, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741985_1161, duration: 511713
2015-10-24 14:55:02,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741985_1161, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741986_1162 src: /192.168.54.130:53683 dest: /192.168.54.130:50010
2015-10-24 14:55:02,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53683, dest: /192.168.54.130:50010, bytes: 9408, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741986_1162, duration: 489909
2015-10-24 14:55:02,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741986_1162, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741987_1163 src: /192.168.54.130:53684 dest: /192.168.54.130:50010
2015-10-24 14:55:02,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53684, dest: /192.168.54.130:50010, bytes: 13657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741987_1163, duration: 471532
2015-10-24 14:55:02,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741987_1163, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741988_1164 src: /192.168.54.130:53685 dest: /192.168.54.130:50010
2015-10-24 14:55:02,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53685, dest: /192.168.54.130:50010, bytes: 13575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741988_1164, duration: 487259
2015-10-24 14:55:02,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741988_1164, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741989_1165 src: /192.168.54.130:53686 dest: /192.168.54.130:50010
2015-10-24 14:55:02,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53686, dest: /192.168.54.130:50010, bytes: 8722, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741989_1165, duration: 396562
2015-10-24 14:55:02,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741989_1165, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:55:02,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741990_1166 src: /192.168.54.130:53687 dest: /192.168.54.130:50010
2015-10-24 14:55:02,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53687, dest: /192.168.54.130:50010, bytes: 25882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1104710735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741990_1166, duration: 488000
2015-10-24 14:55:02,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741990_1166, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:57:14,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741991_1167 src: /192.168.54.130:53694 dest: /192.168.54.130:50010
2015-10-24 14:57:14,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53694, dest: /192.168.54.130:50010, bytes: 4513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-401718126_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741991_1167, duration: 47815572
2015-10-24 14:57:14,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741991_1167, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:57:14,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741992_1168 src: /192.168.54.130:53695 dest: /192.168.54.130:50010
2015-10-24 14:57:14,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53695, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-401718126_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741992_1168, duration: 4112181
2015-10-24 14:57:14,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741992_1168, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 14:57:14,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741993_1169 src: /192.168.54.130:53697 dest: /192.168.54.130:50010
2015-10-24 14:57:14,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53697, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-401718126_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741993_1169, duration: 432106
2015-10-24 14:57:14,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741993_1169, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:57:14,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741994_1170 src: /192.168.54.130:53698 dest: /192.168.54.130:50010
2015-10-24 14:57:14,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53698, dest: /192.168.54.130:50010, bytes: 95860, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-401718126_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741994_1170, duration: 1499055
2015-10-24 14:57:14,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741994_1170, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:57:17,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073741991_1167 to 192.168.54.131:50010 
2015-10-24 14:57:17,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073741991_1167 (numBytes=4513) to /192.168.54.131:50010
2015-10-24 14:57:19,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741995_1171 src: /192.168.54.130:53708 dest: /192.168.54.130:50010
2015-10-24 14:57:19,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53708, dest: /192.168.54.130:50010, bytes: 114019, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1221173465_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741995_1171, duration: 34250940
2015-10-24 14:57:19,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741995_1171, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 14:57:46,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741996_1172 src: /192.168.54.130:53754 dest: /192.168.54.130:50010
2015-10-24 14:59:22,019 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1087ms
No GCs detected
2015-10-24 15:00:37,530 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1080ms
No GCs detected
2015-10-24 15:01:16,615 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1206ms
No GCs detected
2015-10-24 15:01:25,448 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1436ms
GC pool 'Copy' had collection(s): count=1 time=1772ms
2015-10-24 15:04:20,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53754, dest: /192.168.54.130:50010, bytes: 506367, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1221173465_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741996_1172, duration: 394002421170
2015-10-24 15:04:20,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741996_1172, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:04:20,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741998_1174 src: /192.168.54.130:54119 dest: /192.168.54.130:50010
2015-10-24 15:04:20,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54119, dest: /192.168.54.130:50010, bytes: 358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1221173465_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741998_1174, duration: 591706
2015-10-24 15:04:20,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741998_1174, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:04:20,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073741999_1175 src: /192.168.54.130:54121 dest: /192.168.54.130:50010
2015-10-24 15:04:20,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54121, dest: /192.168.54.130:50010, bytes: 506367, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1221173465_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073741999_1175, duration: 25889263
2015-10-24 15:04:20,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073741999_1175, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:04:20,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742000_1176 src: /192.168.54.130:54122 dest: /192.168.54.130:50010
2015-10-24 15:04:20,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54122, dest: /192.168.54.130:50010, bytes: 114019, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1221173465_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742000_1176, duration: 4842036
2015-10-24 15:04:20,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742000_1176, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:04:26,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741991_1167 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741991 for deletion
2015-10-24 15:04:26,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741992_1168 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741992 for deletion
2015-10-24 15:04:26,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741993_1169 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741993 for deletion
2015-10-24 15:04:26,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741994_1170 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741994 for deletion
2015-10-24 15:04:26,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741995_1171 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741995 for deletion
2015-10-24 15:04:26,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741996_1172 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741996 for deletion
2015-10-24 15:04:26,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741991_1167 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741991
2015-10-24 15:04:26,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741992_1168 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741992
2015-10-24 15:04:26,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741993_1169 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741993
2015-10-24 15:04:26,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741994_1170 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741994
2015-10-24 15:04:26,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741995_1171 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741995
2015-10-24 15:04:26,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741996_1172 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741996
2015-10-24 15:29:05,604 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2015-10-24 15:29:05,609 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741956_1132 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741956 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741958_1134 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741958 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741959_1135 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741959 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741960_1136 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741960 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741961_1137 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741961 for deletion
2015-10-24 15:29:05,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741962_1138 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741962 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741968_1144 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741968 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741970_1146 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741970 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741972_1148 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741972 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741973_1149 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741973 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741974_1150 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741974 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741975_1151 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741975 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2015-10-24 15:29:05,611 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741977_1153 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741977 for deletion
2015-10-24 15:29:05,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741978_1154 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741978 for deletion
2015-10-24 15:29:05,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741979_1155 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741979 for deletion
2015-10-24 15:29:05,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741980_1156 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741980 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741981_1157 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741981 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741982_1158 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741982 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741983_1159 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741983 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741984_1160 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741984 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741985_1161 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741985 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741986_1162 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741986 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741987_1163 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741987 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741988_1164 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741988 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741989_1165 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741989 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741990_1166 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741990 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2015-10-24 15:29:05,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741936_1112 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741936 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741938_1114 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741938 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741939_1115 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741939 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741940_1116 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741940 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741942_1118 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741942 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741945_1121 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741945 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741946_1122 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741946 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741947_1123 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741947 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741948_1124 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741948 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741949_1125 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741949 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741950_1126 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741950 for deletion
2015-10-24 15:29:05,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2015-10-24 15:29:05,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741952_1128 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741952
2015-10-24 15:29:05,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741953_1129 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741953
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741954_1130 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741954
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741955_1131 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741955
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741956_1132 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741956
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741957_1133 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741957
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741958_1134 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741958
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741959_1135 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741959
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741960_1136 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741960
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741961_1137 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741961
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741962_1138 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741962
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741963_1139 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741963
2015-10-24 15:29:05,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741964_1140 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741964
2015-10-24 15:29:05,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741965_1141 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741965
2015-10-24 15:29:05,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741966_1142 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741966
2015-10-24 15:29:05,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741967_1143 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741967
2015-10-24 15:29:05,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741968_1144 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741968
2015-10-24 15:29:05,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741969_1145 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741969
2015-10-24 15:29:05,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741970_1146 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741970
2015-10-24 15:29:05,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741971_1147 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741971
2015-10-24 15:29:05,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741972_1148 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741972
2015-10-24 15:29:05,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741973_1149 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741973
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741974_1150 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741974
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741975_1151 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741975
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741976_1152 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741976
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741977_1153 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741977
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741978_1154 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741978
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741979_1155 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741979
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741980_1156 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741980
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741981_1157 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741981
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741982_1158 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741982
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741983_1159 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741983
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741984_1160 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741984
2015-10-24 15:29:05,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741985_1161 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741985
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741986_1162 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741986
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741987_1163 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741987
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741988_1164 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741988
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741989_1165 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741989
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741990_1166 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741990
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741929_1105 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741929
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741930_1106 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741930
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741931_1107 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741931
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741932_1108 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741932
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741933_1109 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741933
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741934_1110 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741934
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741935_1111 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741935
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741936_1112 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741936
2015-10-24 15:29:05,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741937_1113 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741937
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741938_1114 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741938
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741939_1115 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741939
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741940_1116 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741940
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741941_1117 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741941
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741942_1118 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741942
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741943_1119 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741943
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741944_1120 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741944
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741945_1121 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741945
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741946_1122 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741946
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741947_1123 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741947
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741948_1124 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741948
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741949_1125 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741949
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741950_1126 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741950
2015-10-24 15:29:05,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073741951_1127 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073741951
2015-10-24 15:30:18,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742001_1177 src: /192.168.54.130:54192 dest: /192.168.54.130:50010
2015-10-24 15:30:18,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54192, dest: /192.168.54.130:50010, bytes: 663, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1011389983_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742001_1177, duration: 32369463
2015-10-24 15:30:18,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742001_1177, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:31:40,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742002_1178 src: /192.168.54.130:54197 dest: /192.168.54.130:50010
2015-10-24 15:31:40,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54197, dest: /192.168.54.130:50010, bytes: 4519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795537060_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742002_1178, duration: 29338421
2015-10-24 15:31:40,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742002_1178, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:31:40,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742003_1179 src: /192.168.54.130:54198 dest: /192.168.54.130:50010
2015-10-24 15:31:40,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54198, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795537060_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742003_1179, duration: 5862932
2015-10-24 15:31:40,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742003_1179, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 15:31:40,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742004_1180 src: /192.168.54.130:54200 dest: /192.168.54.130:50010
2015-10-24 15:31:40,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54200, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795537060_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742004_1180, duration: 579556
2015-10-24 15:31:40,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742004_1180, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:31:40,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742005_1181 src: /192.168.54.130:54201 dest: /192.168.54.130:50010
2015-10-24 15:31:40,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54201, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795537060_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742005_1181, duration: 4026245
2015-10-24 15:31:40,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742005_1181, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:31:44,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742002_1178 to 192.168.54.131:50010 
2015-10-24 15:31:44,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742002_1178 (numBytes=4519) to /192.168.54.131:50010
2015-10-24 15:31:46,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742006_1182 src: /192.168.54.130:54211 dest: /192.168.54.130:50010
2015-10-24 15:31:46,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54211, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2072111984_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742006_1182, duration: 31758873
2015-10-24 15:31:46,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742006_1182, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:31:55,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742007_1183 src: /192.168.54.130:54220 dest: /192.168.54.130:50010
2015-10-24 15:32:00,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742008_1184 src: /192.168.54.130:54229 dest: /192.168.54.130:50010
2015-10-24 15:32:00,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54229, dest: /192.168.54.130:50010, bytes: 8102, op: HDFS_WRITE, cliID: DFSClient_attempt_1445715765354_0010_r_000000_0_897707154_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742008_1184, duration: 59429878
2015-10-24 15:32:00,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742008_1184, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:32:00,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54220, dest: /192.168.54.130:50010, bytes: 39930, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2072111984_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742007_1183, duration: 5284904147
2015-10-24 15:32:00,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742007_1183, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:32:00,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742009_1185 src: /192.168.54.130:54231 dest: /192.168.54.130:50010
2015-10-24 15:32:00,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54231, dest: /192.168.54.130:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2072111984_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742009_1185, duration: 1796838
2015-10-24 15:32:00,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742009_1185, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:32:00,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742010_1186 src: /192.168.54.130:54233 dest: /192.168.54.130:50010
2015-10-24 15:32:00,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54233, dest: /192.168.54.130:50010, bytes: 39930, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2072111984_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742010_1186, duration: 4580702
2015-10-24 15:32:00,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742010_1186, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:32:00,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742011_1187 src: /192.168.54.130:54234 dest: /192.168.54.130:50010
2015-10-24 15:32:00,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54234, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2072111984_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742011_1187, duration: 3614565
2015-10-24 15:32:00,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742011_1187, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:32:08,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742002_1178 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742002 for deletion
2015-10-24 15:32:08,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742003_1179 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742003 for deletion
2015-10-24 15:32:08,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742004_1180 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742004 for deletion
2015-10-24 15:32:08,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742005_1181 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742005 for deletion
2015-10-24 15:32:08,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742006_1182 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742006 for deletion
2015-10-24 15:32:08,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742007_1183 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742007 for deletion
2015-10-24 15:32:08,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742002_1178 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742002
2015-10-24 15:32:08,593 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742003_1179 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742003
2015-10-24 15:32:08,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742004_1180 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742004
2015-10-24 15:32:08,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742005_1181 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742005
2015-10-24 15:32:08,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742006_1182 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742006
2015-10-24 15:32:08,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742007_1183 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742007
2015-10-24 15:54:50,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742008_1184 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742008 for deletion
2015-10-24 15:54:50,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742008_1184 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742008
2015-10-24 15:55:59,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742012_1188 src: /192.168.54.130:54267 dest: /192.168.54.130:50010
2015-10-24 15:55:59,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54267, dest: /192.168.54.130:50010, bytes: 4696, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1745661160_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742012_1188, duration: 35376950
2015-10-24 15:55:59,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742012_1188, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:55:59,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742013_1189 src: /192.168.54.130:54268 dest: /192.168.54.130:50010
2015-10-24 15:55:59,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54268, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1745661160_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742013_1189, duration: 6447701
2015-10-24 15:55:59,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742013_1189, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-24 15:55:59,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742014_1190 src: /192.168.54.130:54270 dest: /192.168.54.130:50010
2015-10-24 15:55:59,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54270, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1745661160_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742014_1190, duration: 557219
2015-10-24 15:55:59,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742014_1190, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:55:59,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742015_1191 src: /192.168.54.130:54271 dest: /192.168.54.130:50010
2015-10-24 15:55:59,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54271, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1745661160_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742015_1191, duration: 4539309
2015-10-24 15:55:59,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742015_1191, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:56:02,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742012_1188 to 192.168.54.131:50010 
2015-10-24 15:56:02,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742012_1188 (numBytes=4696) to /192.168.54.131:50010
2015-10-24 15:56:07,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742016_1192 src: /192.168.54.130:54280 dest: /192.168.54.130:50010
2015-10-24 15:56:07,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54280, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-17304186_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742016_1192, duration: 106104596
2015-10-24 15:56:07,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742016_1192, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:56:17,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742017_1193 src: /192.168.54.130:54292 dest: /192.168.54.130:50010
2015-10-24 15:56:23,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742018_1194 src: /192.168.54.130:54297 dest: /192.168.54.130:50010
2015-10-24 15:56:23,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54297, dest: /192.168.54.130:50010, bytes: 8102, op: HDFS_WRITE, cliID: DFSClient_attempt_1445715765354_0011_r_000000_0_-695267569_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742018_1194, duration: 45247423
2015-10-24 15:56:23,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742018_1194, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:56:24,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54292, dest: /192.168.54.130:50010, bytes: 39942, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-17304186_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742017_1193, duration: 6518900446
2015-10-24 15:56:24,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742017_1193, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:56:24,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742019_1195 src: /192.168.54.130:54299 dest: /192.168.54.130:50010
2015-10-24 15:56:24,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54299, dest: /192.168.54.130:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-17304186_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742019_1195, duration: 777121
2015-10-24 15:56:24,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742019_1195, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:56:24,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742020_1196 src: /192.168.54.130:54301 dest: /192.168.54.130:50010
2015-10-24 15:56:24,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54301, dest: /192.168.54.130:50010, bytes: 39942, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-17304186_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742020_1196, duration: 5556252
2015-10-24 15:56:24,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742020_1196, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:56:24,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742021_1197 src: /192.168.54.130:54302 dest: /192.168.54.130:50010
2015-10-24 15:56:24,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54302, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-17304186_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742021_1197, duration: 6155929
2015-10-24 15:56:24,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742021_1197, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-24 15:56:32,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742016_1192 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742016 for deletion
2015-10-24 15:56:32,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742017_1193 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742017 for deletion
2015-10-24 15:56:32,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742012_1188 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742012 for deletion
2015-10-24 15:56:32,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742013_1189 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742013 for deletion
2015-10-24 15:56:32,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742014_1190 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742014 for deletion
2015-10-24 15:56:32,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742015_1191 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742015 for deletion
2015-10-24 15:56:32,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742016_1192 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742016
2015-10-24 15:56:32,602 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742017_1193 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742017
2015-10-24 15:56:32,602 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742012_1188 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742012
2015-10-24 15:56:32,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742013_1189 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742013
2015-10-24 15:56:32,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742014_1190 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742014
2015-10-24 15:56:32,604 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742015_1191 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742015
2015-10-24 16:55:26,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10f034356821,  containing 1 storage report(s), of which we sent 1. The reports had 20 total blocks and used 1 RPC(s). This took 1 msec to generate and 30 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-24 16:55:26,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-24 18:02:41,518 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 20, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-25 15:41:20,102 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742018_1194 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742018 for deletion
2015-10-25 15:41:20,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742018_1194 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742018
2015-10-25 15:42:06,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742022_1198 src: /192.168.54.130:54624 dest: /192.168.54.130:50010
2015-10-25 15:42:07,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54624, dest: /192.168.54.130:50010, bytes: 4708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58135157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742022_1198, duration: 70634397
2015-10-25 15:42:07,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742022_1198, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:42:07,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742023_1199 src: /192.168.54.130:54625 dest: /192.168.54.130:50010
2015-10-25 15:42:07,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54625, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58135157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742023_1199, duration: 7350285
2015-10-25 15:42:07,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742023_1199, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 15:42:07,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742024_1200 src: /192.168.54.130:54627 dest: /192.168.54.130:50010
2015-10-25 15:42:07,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54627, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58135157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742024_1200, duration: 515133
2015-10-25 15:42:07,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742024_1200, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:42:07,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742025_1201 src: /192.168.54.130:54628 dest: /192.168.54.130:50010
2015-10-25 15:42:07,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54628, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58135157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742025_1201, duration: 9593085
2015-10-25 15:42:07,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742025_1201, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:42:08,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742022_1198 to 192.168.54.131:50010 
2015-10-25 15:42:08,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742022_1198 (numBytes=4708) to /192.168.54.131:50010
2015-10-25 15:42:29,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742028_1204 src: /192.168.54.130:54648 dest: /192.168.54.130:50010
2015-10-25 15:42:29,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54648, dest: /192.168.54.130:50010, bytes: 7859, op: HDFS_WRITE, cliID: DFSClient_attempt_1445715765354_0012_r_000000_0_298468725_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742028_1204, duration: 37873498
2015-10-25 15:42:29,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742028_1204, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:42:35,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742022_1198 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742022 for deletion
2015-10-25 15:42:35,078 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742023_1199 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742023 for deletion
2015-10-25 15:42:35,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742024_1200 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742024 for deletion
2015-10-25 15:42:35,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742025_1201 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742025 for deletion
2015-10-25 15:42:35,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742022_1198 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742022
2015-10-25 15:42:35,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742023_1199 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742023
2015-10-25 15:42:35,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742024_1200 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742024
2015-10-25 15:42:35,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742025_1201 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742025
2015-10-25 15:47:53,076 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742001_1177 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742001 for deletion
2015-10-25 15:47:53,084 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742001_1177 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742001
2015-10-25 15:48:20,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742028_1204 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742028 for deletion
2015-10-25 15:48:20,076 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742028_1204 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742028
2015-10-25 15:50:16,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742032_1208 src: /192.168.54.130:54665 dest: /192.168.54.130:50010
2015-10-25 15:50:16,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54665, dest: /192.168.54.130:50010, bytes: 663, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742032_1208, duration: 35526206
2015-10-25 15:50:16,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742032_1208, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742033_1209 src: /192.168.54.130:54666 dest: /192.168.54.130:50010
2015-10-25 15:50:16,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54666, dest: /192.168.54.130:50010, bytes: 4491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742033_1209, duration: 1093516
2015-10-25 15:50:16,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742033_1209, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742034_1210 src: /192.168.54.130:54667 dest: /192.168.54.130:50010
2015-10-25 15:50:16,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54667, dest: /192.168.54.130:50010, bytes: 4299, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742034_1210, duration: 2631804
2015-10-25 15:50:16,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742034_1210, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742035_1211 src: /192.168.54.130:54668 dest: /192.168.54.130:50010
2015-10-25 15:50:16,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54668, dest: /192.168.54.130:50010, bytes: 9525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742035_1211, duration: 1938848
2015-10-25 15:50:16,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742035_1211, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742036_1212 src: /192.168.54.130:54669 dest: /192.168.54.130:50010
2015-10-25 15:50:16,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54669, dest: /192.168.54.130:50010, bytes: 5957, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742036_1212, duration: 808665
2015-10-25 15:50:16,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742036_1212, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742037_1213 src: /192.168.54.130:54670 dest: /192.168.54.130:50010
2015-10-25 15:50:16,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54670, dest: /192.168.54.130:50010, bytes: 5268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742037_1213, duration: 925558
2015-10-25 15:50:16,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742037_1213, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742038_1214 src: /192.168.54.130:54671 dest: /192.168.54.130:50010
2015-10-25 15:50:16,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54671, dest: /192.168.54.130:50010, bytes: 13016, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742038_1214, duration: 1630958
2015-10-25 15:50:16,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742038_1214, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742039_1215 src: /192.168.54.130:54672 dest: /192.168.54.130:50010
2015-10-25 15:50:16,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54672, dest: /192.168.54.130:50010, bytes: 11168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742039_1215, duration: 1570714
2015-10-25 15:50:16,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742039_1215, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742040_1216 src: /192.168.54.130:54673 dest: /192.168.54.130:50010
2015-10-25 15:50:16,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54673, dest: /192.168.54.130:50010, bytes: 10990, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742040_1216, duration: 1806379
2015-10-25 15:50:16,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742040_1216, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742041_1217 src: /192.168.54.130:54674 dest: /192.168.54.130:50010
2015-10-25 15:50:16,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54674, dest: /192.168.54.130:50010, bytes: 9742, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742041_1217, duration: 1130177
2015-10-25 15:50:16,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742041_1217, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742042_1218 src: /192.168.54.130:54675 dest: /192.168.54.130:50010
2015-10-25 15:50:16,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54675, dest: /192.168.54.130:50010, bytes: 12483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742042_1218, duration: 1100667
2015-10-25 15:50:16,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742042_1218, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742043_1219 src: /192.168.54.130:54676 dest: /192.168.54.130:50010
2015-10-25 15:50:16,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54676, dest: /192.168.54.130:50010, bytes: 8938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742043_1219, duration: 955304
2015-10-25 15:50:16,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742043_1219, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742044_1220 src: /192.168.54.130:54677 dest: /192.168.54.130:50010
2015-10-25 15:50:16,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54677, dest: /192.168.54.130:50010, bytes: 3948, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742044_1220, duration: 1994035
2015-10-25 15:50:16,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742044_1220, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742045_1221 src: /192.168.54.130:54678 dest: /192.168.54.130:50010
2015-10-25 15:50:16,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54678, dest: /192.168.54.130:50010, bytes: 9222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742045_1221, duration: 1149724
2015-10-25 15:50:16,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742045_1221, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742046_1222 src: /192.168.54.130:54679 dest: /192.168.54.130:50010
2015-10-25 15:50:16,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54679, dest: /192.168.54.130:50010, bytes: 6459, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742046_1222, duration: 704920
2015-10-25 15:50:16,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742046_1222, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742047_1223 src: /192.168.54.130:54680 dest: /192.168.54.130:50010
2015-10-25 15:50:16,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54680, dest: /192.168.54.130:50010, bytes: 9811, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742047_1223, duration: 560607
2015-10-25 15:50:16,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742047_1223, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742048_1224 src: /192.168.54.130:54681 dest: /192.168.54.130:50010
2015-10-25 15:50:16,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54681, dest: /192.168.54.130:50010, bytes: 19000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742048_1224, duration: 2718880
2015-10-25 15:50:16,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742048_1224, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742049_1225 src: /192.168.54.130:54682 dest: /192.168.54.130:50010
2015-10-25 15:50:16,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54682, dest: /192.168.54.130:50010, bytes: 7279, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742049_1225, duration: 682307
2015-10-25 15:50:16,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742049_1225, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742050_1226 src: /192.168.54.130:54683 dest: /192.168.54.130:50010
2015-10-25 15:50:16,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54683, dest: /192.168.54.130:50010, bytes: 29112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742050_1226, duration: 5955011
2015-10-25 15:50:16,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742050_1226, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742051_1227 src: /192.168.54.130:54684 dest: /192.168.54.130:50010
2015-10-25 15:50:16,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54684, dest: /192.168.54.130:50010, bytes: 10673, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742051_1227, duration: 5171123
2015-10-25 15:50:16,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742051_1227, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742052_1228 src: /192.168.54.130:54685 dest: /192.168.54.130:50010
2015-10-25 15:50:16,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54685, dest: /192.168.54.130:50010, bytes: 9176, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742052_1228, duration: 459625
2015-10-25 15:50:16,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742052_1228, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742053_1229 src: /192.168.54.130:54686 dest: /192.168.54.130:50010
2015-10-25 15:50:16,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54686, dest: /192.168.54.130:50010, bytes: 11230, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742053_1229, duration: 399522
2015-10-25 15:50:16,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742053_1229, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742054_1230 src: /192.168.54.130:54687 dest: /192.168.54.130:50010
2015-10-25 15:50:16,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54687, dest: /192.168.54.130:50010, bytes: 9954, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742054_1230, duration: 491820
2015-10-25 15:50:16,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742054_1230, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742055_1231 src: /192.168.54.130:54688 dest: /192.168.54.130:50010
2015-10-25 15:50:16,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54688, dest: /192.168.54.130:50010, bytes: 9328, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742055_1231, duration: 389886
2015-10-25 15:50:16,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742055_1231, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742056_1232 src: /192.168.54.130:54689 dest: /192.168.54.130:50010
2015-10-25 15:50:16,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54689, dest: /192.168.54.130:50010, bytes: 10827, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742056_1232, duration: 994246
2015-10-25 15:50:16,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742056_1232, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742057_1233 src: /192.168.54.130:54690 dest: /192.168.54.130:50010
2015-10-25 15:50:16,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54690, dest: /192.168.54.130:50010, bytes: 8573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742057_1233, duration: 373592
2015-10-25 15:50:16,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742057_1233, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742058_1234 src: /192.168.54.130:54691 dest: /192.168.54.130:50010
2015-10-25 15:50:16,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54691, dest: /192.168.54.130:50010, bytes: 12751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742058_1234, duration: 420167
2015-10-25 15:50:16,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742058_1234, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742059_1235 src: /192.168.54.130:54692 dest: /192.168.54.130:50010
2015-10-25 15:50:16,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54692, dest: /192.168.54.130:50010, bytes: 7216, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742059_1235, duration: 373221
2015-10-25 15:50:16,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742059_1235, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742060_1236 src: /192.168.54.130:54693 dest: /192.168.54.130:50010
2015-10-25 15:50:16,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54693, dest: /192.168.54.130:50010, bytes: 8245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742060_1236, duration: 412180
2015-10-25 15:50:16,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742060_1236, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742061_1237 src: /192.168.54.130:54694 dest: /192.168.54.130:50010
2015-10-25 15:50:16,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54694, dest: /192.168.54.130:50010, bytes: 13713, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742061_1237, duration: 401656
2015-10-25 15:50:16,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742061_1237, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742062_1238 src: /192.168.54.130:54695 dest: /192.168.54.130:50010
2015-10-25 15:50:16,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54695, dest: /192.168.54.130:50010, bytes: 7074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742062_1238, duration: 394321
2015-10-25 15:50:16,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742062_1238, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:16,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742063_1239 src: /192.168.54.130:54696 dest: /192.168.54.130:50010
2015-10-25 15:50:16,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54696, dest: /192.168.54.130:50010, bytes: 8670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742063_1239, duration: 528334
2015-10-25 15:50:16,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742063_1239, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742064_1240 src: /192.168.54.130:54697 dest: /192.168.54.130:50010
2015-10-25 15:50:17,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54697, dest: /192.168.54.130:50010, bytes: 8483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742064_1240, duration: 409513
2015-10-25 15:50:17,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742064_1240, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742065_1241 src: /192.168.54.130:54698 dest: /192.168.54.130:50010
2015-10-25 15:50:17,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54698, dest: /192.168.54.130:50010, bytes: 10347, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742065_1241, duration: 978666
2015-10-25 15:50:17,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742065_1241, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742066_1242 src: /192.168.54.130:54699 dest: /192.168.54.130:50010
2015-10-25 15:50:17,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54699, dest: /192.168.54.130:50010, bytes: 12018, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742066_1242, duration: 1359788
2015-10-25 15:50:17,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742066_1242, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742067_1243 src: /192.168.54.130:54700 dest: /192.168.54.130:50010
2015-10-25 15:50:17,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54700, dest: /192.168.54.130:50010, bytes: 16999, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742067_1243, duration: 1210457
2015-10-25 15:50:17,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742067_1243, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742068_1244 src: /192.168.54.130:54701 dest: /192.168.54.130:50010
2015-10-25 15:50:17,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54701, dest: /192.168.54.130:50010, bytes: 11799, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742068_1244, duration: 398851
2015-10-25 15:50:17,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742068_1244, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742069_1245 src: /192.168.54.130:54702 dest: /192.168.54.130:50010
2015-10-25 15:50:17,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54702, dest: /192.168.54.130:50010, bytes: 7797, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742069_1245, duration: 367935
2015-10-25 15:50:17,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742069_1245, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742070_1246 src: /192.168.54.130:54703 dest: /192.168.54.130:50010
2015-10-25 15:50:17,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54703, dest: /192.168.54.130:50010, bytes: 5937, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742070_1246, duration: 367142
2015-10-25 15:50:17,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742070_1246, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742071_1247 src: /192.168.54.130:54704 dest: /192.168.54.130:50010
2015-10-25 15:50:17,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54704, dest: /192.168.54.130:50010, bytes: 8751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742071_1247, duration: 387732
2015-10-25 15:50:17,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742071_1247, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742072_1248 src: /192.168.54.130:54705 dest: /192.168.54.130:50010
2015-10-25 15:50:17,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54705, dest: /192.168.54.130:50010, bytes: 9168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742072_1248, duration: 374406
2015-10-25 15:50:17,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742072_1248, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742073_1249 src: /192.168.54.130:54706 dest: /192.168.54.130:50010
2015-10-25 15:50:17,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54706, dest: /192.168.54.130:50010, bytes: 13006, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742073_1249, duration: 414813
2015-10-25 15:50:17,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742073_1249, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742074_1250 src: /192.168.54.130:54707 dest: /192.168.54.130:50010
2015-10-25 15:50:17,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54707, dest: /192.168.54.130:50010, bytes: 10662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742074_1250, duration: 1127183
2015-10-25 15:50:17,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742074_1250, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742075_1251 src: /192.168.54.130:54708 dest: /192.168.54.130:50010
2015-10-25 15:50:17,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54708, dest: /192.168.54.130:50010, bytes: 27491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742075_1251, duration: 1040699
2015-10-25 15:50:17,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742075_1251, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742076_1252 src: /192.168.54.130:54709 dest: /192.168.54.130:50010
2015-10-25 15:50:17,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54709, dest: /192.168.54.130:50010, bytes: 13387, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742076_1252, duration: 389695
2015-10-25 15:50:17,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742076_1252, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742077_1253 src: /192.168.54.130:54710 dest: /192.168.54.130:50010
2015-10-25 15:50:17,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54710, dest: /192.168.54.130:50010, bytes: 10094, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742077_1253, duration: 1013788
2015-10-25 15:50:17,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742077_1253, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742078_1254 src: /192.168.54.130:54711 dest: /192.168.54.130:50010
2015-10-25 15:50:17,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54711, dest: /192.168.54.130:50010, bytes: 16889, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742078_1254, duration: 427719
2015-10-25 15:50:17,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742078_1254, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742079_1255 src: /192.168.54.130:54712 dest: /192.168.54.130:50010
2015-10-25 15:50:17,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54712, dest: /192.168.54.130:50010, bytes: 22332, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742079_1255, duration: 747298
2015-10-25 15:50:17,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742079_1255, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742080_1256 src: /192.168.54.130:54713 dest: /192.168.54.130:50010
2015-10-25 15:50:17,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54713, dest: /192.168.54.130:50010, bytes: 12552, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742080_1256, duration: 1043120
2015-10-25 15:50:17,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742080_1256, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742081_1257 src: /192.168.54.130:54714 dest: /192.168.54.130:50010
2015-10-25 15:50:17,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54714, dest: /192.168.54.130:50010, bytes: 12248, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742081_1257, duration: 375144
2015-10-25 15:50:17,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742081_1257, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742082_1258 src: /192.168.54.130:54715 dest: /192.168.54.130:50010
2015-10-25 15:50:17,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54715, dest: /192.168.54.130:50010, bytes: 12497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742082_1258, duration: 616291
2015-10-25 15:50:17,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742082_1258, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742083_1259 src: /192.168.54.130:54716 dest: /192.168.54.130:50010
2015-10-25 15:50:17,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54716, dest: /192.168.54.130:50010, bytes: 11243, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742083_1259, duration: 538309
2015-10-25 15:50:17,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742083_1259, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742084_1260 src: /192.168.54.130:54717 dest: /192.168.54.130:50010
2015-10-25 15:50:17,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54717, dest: /192.168.54.130:50010, bytes: 16578, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742084_1260, duration: 960971
2015-10-25 15:50:17,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742084_1260, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742085_1261 src: /192.168.54.130:54718 dest: /192.168.54.130:50010
2015-10-25 15:50:17,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54718, dest: /192.168.54.130:50010, bytes: 16110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742085_1261, duration: 1671781
2015-10-25 15:50:17,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742085_1261, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742086_1262 src: /192.168.54.130:54719 dest: /192.168.54.130:50010
2015-10-25 15:50:17,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54719, dest: /192.168.54.130:50010, bytes: 8866, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742086_1262, duration: 407254
2015-10-25 15:50:17,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742086_1262, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742087_1263 src: /192.168.54.130:54720 dest: /192.168.54.130:50010
2015-10-25 15:50:17,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54720, dest: /192.168.54.130:50010, bytes: 12994, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742087_1263, duration: 673948
2015-10-25 15:50:17,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742087_1263, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742088_1264 src: /192.168.54.130:54721 dest: /192.168.54.130:50010
2015-10-25 15:50:17,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54721, dest: /192.168.54.130:50010, bytes: 15319, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742088_1264, duration: 1723066
2015-10-25 15:50:17,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742088_1264, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742089_1265 src: /192.168.54.130:54722 dest: /192.168.54.130:50010
2015-10-25 15:50:17,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54722, dest: /192.168.54.130:50010, bytes: 9408, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742089_1265, duration: 993227
2015-10-25 15:50:17,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742089_1265, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742090_1266 src: /192.168.54.130:54723 dest: /192.168.54.130:50010
2015-10-25 15:50:17,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54723, dest: /192.168.54.130:50010, bytes: 13657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742090_1266, duration: 1877282
2015-10-25 15:50:17,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742090_1266, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742091_1267 src: /192.168.54.130:54724 dest: /192.168.54.130:50010
2015-10-25 15:50:17,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54724, dest: /192.168.54.130:50010, bytes: 13575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742091_1267, duration: 843168
2015-10-25 15:50:17,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742091_1267, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742092_1268 src: /192.168.54.130:54725 dest: /192.168.54.130:50010
2015-10-25 15:50:17,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54725, dest: /192.168.54.130:50010, bytes: 8722, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742092_1268, duration: 544634
2015-10-25 15:50:17,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742092_1268, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:17,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742093_1269 src: /192.168.54.130:54726 dest: /192.168.54.130:50010
2015-10-25 15:50:17,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54726, dest: /192.168.54.130:50010, bytes: 25882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152294830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742093_1269, duration: 5147517
2015-10-25 15:50:17,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742093_1269, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:50,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742094_1270 src: /192.168.54.130:54731 dest: /192.168.54.130:50010
2015-10-25 15:50:50,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54731, dest: /192.168.54.130:50010, bytes: 4708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2086753340_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742094_1270, duration: 28119938
2015-10-25 15:50:50,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742094_1270, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:50,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742095_1271 src: /192.168.54.130:54732 dest: /192.168.54.130:50010
2015-10-25 15:50:50,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54732, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2086753340_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742095_1271, duration: 5696522
2015-10-25 15:50:50,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742095_1271, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 15:50:50,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742096_1272 src: /192.168.54.130:54734 dest: /192.168.54.130:50010
2015-10-25 15:50:50,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54734, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2086753340_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742096_1272, duration: 357445
2015-10-25 15:50:50,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742096_1272, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:50,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742097_1273 src: /192.168.54.130:54735 dest: /192.168.54.130:50010
2015-10-25 15:50:50,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54735, dest: /192.168.54.130:50010, bytes: 95860, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2086753340_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742097_1273, duration: 6172199
2015-10-25 15:50:50,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742097_1273, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:50:53,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742094_1270 to 192.168.54.131:50010 
2015-10-25 15:50:53,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742094_1270 (numBytes=4708) to /192.168.54.131:50010
2015-10-25 15:51:23,343 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5744ms
GC pool 'Copy' had collection(s): count=1 time=4972ms
2015-10-25 15:51:32,591 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5303ms
No GCs detected
2015-10-25 15:52:15,820 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): Scheduling suspect block BP-1892222280-192.168.54.130-1445296806381:blk_1073742087_1263 for rescanning.
2015-10-25 15:52:17,005 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-10-25 15:52:19,067 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd) exiting.
2015-10-25 15:53:44,902 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 8637ms
GC pool 'Copy' had collection(s): count=1 time=8589ms
2015-10-25 15:53:46,457 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): Scheduling suspect block BP-1892222280-192.168.54.130-1445296806381:blk_1073742057_1233 for rescanning.
2015-10-25 15:54:26,315 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1007ms
No GCs detected
2015-10-25 15:54:58,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742100_1276 src: /192.168.54.130:54943 dest: /192.168.54.130:50010
2015-10-25 15:55:35,133 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-25 15:55:39,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:40,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:41,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:42,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:43,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:44,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:45,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:46,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:47,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:48,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:48,106 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 15:55:49,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:50,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:51,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:52,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:53,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:54,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:55,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:56,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:57,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:58,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:55:58,118 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 15:55:59,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:00,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:01,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:02,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:03,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:04,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:05,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:06,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:07,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:08,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:08,132 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 15:56:09,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:10,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:11,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:12,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:13,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:14,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:15,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:16,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:17,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:18,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:18,148 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 15:56:19,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:20,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:21,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:22,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:23,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:23,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54943, dest: /192.168.54.130:50010, bytes: 29487821, op: HDFS_WRITE, cliID: DFSClient_attempt_1445715765354_0013_r_000000_2_-899760031_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742100_1276, duration: 85181085096
2015-10-25 15:56:23,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742100_1276, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 15:56:24,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:25,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:26,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:27,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:28,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:28,165 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 15:56:29,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:30,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:31,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 15:56:31,292 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-25 15:56:31,323 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-10-25 15:59:02,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-25 15:59:02,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-25 15:59:02,710 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-10-25 15:59:03,141 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-25 15:59:03,274 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-25 15:59:03,274 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-25 15:59:03,284 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-25 15:59:03,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-10-25 15:59:03,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-25 15:59:03,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-25 15:59:03,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-25 15:59:03,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-25 15:59:03,532 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-25 15:59:03,557 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-25 15:59:03,576 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-25 15:59:03,581 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-25 15:59:03,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-25 15:59:03,585 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-25 15:59:03,585 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-25 15:59:03,597 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49587
2015-10-25 15:59:03,597 INFO org.mortbay.log: jetty-6.1.26
2015-10-25 15:59:03,863 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49587
2015-10-25 15:59:04,020 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-25 15:59:04,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-10-25 15:59:04,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-25 15:59:04,109 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-25 15:59:04,146 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-25 15:59:04,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-25 15:59:04,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-25 15:59:04,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-25 15:59:04,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-10-25 15:59:04,290 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-25 15:59:04,291 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-25 15:59:04,627 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 25299@hadoopmaster
2015-10-25 15:59:04,742 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1892222280-192.168.54.130-1445296806381
2015-10-25 15:59:04,742 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381
2015-10-25 15:59:04,743 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-25 15:59:04,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=693764022;bpid=BP-1892222280-192.168.54.130-1445296806381;lv=-56;nsInfo=lv=-63;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0;bpid=BP-1892222280-192.168.54.130-1445296806381;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-10-25 15:59:04,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-10-25 15:59:04,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-10-25 15:59:04,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-25 15:59:04,861 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 15:59:04,861 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-25 15:59:04,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current: 32523964
2015-10-25 15:59:04,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1892222280-192.168.54.130-1445296806381 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 6ms
2015-10-25 15:59:04,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1892222280-192.168.54.130-1445296806381: 7ms
2015-10-25 15:59:04,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-25 15:59:04,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 12ms
2015-10-25 15:59:04,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2015-10-25 15:59:05,135 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1298174370 ms.
2015-10-25 15:59:05,137 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445824429137 with interval 21600000
2015-10-25 15:59:05,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-25 15:59:05,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-25 15:59:05,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-25 15:59:05,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=3512
2015-10-25 15:59:05,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-10-25 15:59:05,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1a5740dad5d5,  containing 1 storage report(s), of which we sent 1. The reports had 85 total blocks and used 1 RPC(s). This took 3 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 15:59:05,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 16:01:22,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742100_1276 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742100 for deletion
2015-10-25 16:01:22,295 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742100_1276 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742100
2015-10-25 16:01:40,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742101_1277 src: /192.168.54.130:55053 dest: /192.168.54.130:50010
2015-10-25 16:01:40,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55053, dest: /192.168.54.130:50010, bytes: 4708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2122577798_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742101_1277, duration: 63287657
2015-10-25 16:01:40,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742101_1277, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:01:41,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742102_1278 src: /192.168.54.130:55054 dest: /192.168.54.130:50010
2015-10-25 16:01:41,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55054, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2122577798_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742102_1278, duration: 42013626
2015-10-25 16:01:41,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742102_1278, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 16:01:41,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742103_1279 src: /192.168.54.130:55056 dest: /192.168.54.130:50010
2015-10-25 16:01:41,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55056, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2122577798_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742103_1279, duration: 707239
2015-10-25 16:01:41,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742103_1279, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:01:41,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742104_1280 src: /192.168.54.130:55057 dest: /192.168.54.130:50010
2015-10-25 16:01:41,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55057, dest: /192.168.54.130:50010, bytes: 95860, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2122577798_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742104_1280, duration: 9130154
2015-10-25 16:01:41,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742104_1280, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:01:43,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742101_1277 to 192.168.54.131:50010 
2015-10-25 16:01:43,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742101_1277 (numBytes=4708) to /192.168.54.131:50010
2015-10-25 16:01:48,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742105_1281 src: /192.168.54.130:55068 dest: /192.168.54.130:50010
2015-10-25 16:01:48,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55068, dest: /192.168.54.130:50010, bytes: 114019, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_707091003_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742105_1281, duration: 29991749
2015-10-25 16:01:48,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742105_1281, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:02:26,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742106_1282 src: /192.168.54.130:55113 dest: /192.168.54.130:50010
2015-10-25 16:03:08,149 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 8050ms
No GCs detected
2015-10-25 16:04:05,811 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2061ms
No GCs detected
2015-10-25 16:04:16,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55113, dest: /192.168.54.130:50010, bytes: 291767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_707091003_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742106_1282, duration: 108482275175
2015-10-25 16:04:16,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742106_1282, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:04:16,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742107_1283 src: /192.168.54.130:55294 dest: /192.168.54.130:50010
2015-10-25 16:04:16,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55294, dest: /192.168.54.130:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_707091003_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742107_1283, duration: 11848633
2015-10-25 16:04:16,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742107_1283, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:04:17,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742108_1284 src: /192.168.54.130:55296 dest: /192.168.54.130:50010
2015-10-25 16:04:17,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55296, dest: /192.168.54.130:50010, bytes: 291767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_707091003_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742108_1284, duration: 44446894
2015-10-25 16:04:17,323 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742108_1284, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:04:17,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742109_1285 src: /192.168.54.130:55297 dest: /192.168.54.130:50010
2015-10-25 16:04:17,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55297, dest: /192.168.54.130:50010, bytes: 114019, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_707091003_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742109_1285, duration: 21516744
2015-10-25 16:04:17,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742109_1285, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:04:22,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742101_1277 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742101 for deletion
2015-10-25 16:04:22,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742102_1278 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742102 for deletion
2015-10-25 16:04:22,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742103_1279 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742103 for deletion
2015-10-25 16:04:22,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742104_1280 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742104 for deletion
2015-10-25 16:04:22,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742105_1281 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742105 for deletion
2015-10-25 16:04:22,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742106_1282 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742106 for deletion
2015-10-25 16:04:22,274 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742101_1277 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742101
2015-10-25 16:04:22,274 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742102_1278 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742102
2015-10-25 16:04:22,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742103_1279 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742103
2015-10-25 16:04:22,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742104_1280 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742104
2015-10-25 16:04:22,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742105_1281 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742105
2015-10-25 16:04:22,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742106_1282 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742106
2015-10-25 16:12:27,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742110_1286 src: /192.168.54.130:55311 dest: /192.168.54.130:50010
2015-10-25 16:12:28,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55311, dest: /192.168.54.130:50010, bytes: 4708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_418946127_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742110_1286, duration: 31774353
2015-10-25 16:12:28,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742110_1286, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:12:28,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742111_1287 src: /192.168.54.130:55312 dest: /192.168.54.130:50010
2015-10-25 16:12:28,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55312, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_418946127_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742111_1287, duration: 6244788
2015-10-25 16:12:28,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742111_1287, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 16:12:28,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742112_1288 src: /192.168.54.130:55314 dest: /192.168.54.130:50010
2015-10-25 16:12:28,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55314, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_418946127_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742112_1288, duration: 946439
2015-10-25 16:12:28,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742112_1288, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:12:28,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742113_1289 src: /192.168.54.130:55315 dest: /192.168.54.130:50010
2015-10-25 16:12:28,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55315, dest: /192.168.54.130:50010, bytes: 95860, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_418946127_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742113_1289, duration: 1268872
2015-10-25 16:12:28,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742113_1289, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:12:31,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742110_1286 to 192.168.54.131:50010 
2015-10-25 16:12:31,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742110_1286 (numBytes=4708) to /192.168.54.131:50010
2015-10-25 16:13:21,117 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 11321ms
No GCs detected
2015-10-25 16:14:03,940 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1035ms
No GCs detected
2015-10-25 16:14:49,225 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3128ms
GC pool 'Copy' had collection(s): count=1 time=2946ms
2015-10-25 16:14:56,195 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3381ms
No GCs detected
2015-10-25 16:15:55,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:15:56,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:15:57,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:15:58,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:15:59,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:00,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:01,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:02,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:03,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:04,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:05,146 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:16:06,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:07,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:08,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:09,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:10,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:11,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:12,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:13,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:14,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:15,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:15,723 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:16:16,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:18,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:19,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:20,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:21,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:22,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:23,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:24,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:25,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:26,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:26,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:16:27,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:28,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:29,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:30,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:31,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:32,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:33,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:34,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:35,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:36,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:36,498 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:16:37,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:38,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:39,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:40,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:41,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:42,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:43,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:44,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:45,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:46,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:46,525 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:16:47,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:48,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:49,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:50,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:51,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:52,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:53,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:54,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:55,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:56,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:56,546 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:16:57,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:58,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:16:59,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:00,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:01,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:02,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:03,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:04,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:05,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:06,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:06,568 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:17:07,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:08,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:09,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:10,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:11,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:12,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:13,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:14,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:15,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:16,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:16,590 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:17:17,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:18,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:19,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:20,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:21,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:22,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:23,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:24,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:25,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:26,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:26,609 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:17:27,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:28,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:29,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:30,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:31,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:32,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:33,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:34,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:35,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:36,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:36,632 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:17:37,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:38,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:39,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:40,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:41,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:42,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:43,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:44,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:45,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:46,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:46,663 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:17:47,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:48,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:49,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:50,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:51,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:52,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:53,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:54,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:55,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:56,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:56,689 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:17:57,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:58,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:17:59,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:00,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:01,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:02,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:03,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:04,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:05,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:06,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:06,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:18:07,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:08,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:09,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:10,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:11,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:12,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:13,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:14,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:15,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:16,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:16,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:18:17,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:18,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:19,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:20,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:21,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:22,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:23,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:24,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:25,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:26,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:26,769 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:18:27,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:28,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:29,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:30,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:31,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:32,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:33,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:34,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:35,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:36,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:36,790 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:18:37,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:38,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:39,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:40,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:41,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:42,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:43,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:44,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:45,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:46,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:46,807 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:18:47,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:48,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:49,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:50,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:51,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:52,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:53,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:54,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:55,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:56,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:56,826 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:18:57,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:58,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:18:59,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:00,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:01,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:02,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:03,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:04,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:05,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:06,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:06,840 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:19:07,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:08,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:09,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:10,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:11,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:12,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:13,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:14,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:15,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:16,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:16,860 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:19:17,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:18,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:19,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:20,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:21,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:22,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:23,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:24,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:25,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:26,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:26,872 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:19:27,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:28,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:29,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:30,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:31,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:32,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:33,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:34,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:35,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:36,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:36,889 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:19:37,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:38,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:39,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:40,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:41,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:42,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:43,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:44,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:45,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:46,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:46,910 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:19:47,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:48,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:49,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:50,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:51,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:52,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:53,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:54,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:55,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:56,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:56,928 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:19:57,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:58,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:19:59,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:00,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:01,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:02,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:03,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:04,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:05,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:06,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:06,947 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:20:07,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:08,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:09,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:10,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:11,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:12,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:13,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:14,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:15,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:16,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:16,964 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:20:17,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:18,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:19,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:20,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:21,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:22,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:23,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:24,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:25,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:26,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:26,979 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:20:27,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:28,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:29,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:30,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:31,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:32,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:33,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:34,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:35,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:36,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:36,996 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:20:37,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:39,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:40,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:41,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:42,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:43,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:44,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:45,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:46,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:47,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:47,013 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:20:48,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:49,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:50,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:51,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:52,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:53,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:54,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:55,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:56,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:57,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:57,031 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:20:58,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:20:59,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:00,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:01,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:02,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:03,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:04,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:05,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:06,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:07,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:07,048 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:21:08,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:09,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:10,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:11,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:12,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:13,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:14,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:15,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:16,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:17,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:17,063 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:21:18,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:19,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:20,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:21,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:22,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:23,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:24,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:25,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:26,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:27,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:27,077 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:21:28,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:29,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:30,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:31,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:32,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:33,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:34,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:35,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:36,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:37,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:37,093 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:21:38,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:39,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:40,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:41,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:42,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:43,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:44,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:45,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:46,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:47,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:47,110 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:21:48,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:49,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:50,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:51,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:52,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:53,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:54,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:55,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:56,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:57,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:57,127 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:21:58,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:21:59,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:00,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:01,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:02,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:03,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:04,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:05,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:06,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:07,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:07,143 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:22:08,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:09,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:10,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:11,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:12,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:13,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:14,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:15,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:16,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:17,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:17,158 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:22:18,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:19,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:20,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:21,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:22,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:23,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:24,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:25,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:26,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:27,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:27,174 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:22:28,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:29,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:30,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:31,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:32,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:33,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:34,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:35,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:36,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:37,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:37,193 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:22:38,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:39,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:40,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:41,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:42,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:43,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:44,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:45,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:46,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:47,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:47,208 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:22:48,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:49,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:50,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:51,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:52,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:53,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:54,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:55,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:56,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:57,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:57,229 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:22:58,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:22:59,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:00,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:01,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:02,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:03,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:04,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:05,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:06,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:07,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:07,257 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:23:08,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:09,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:10,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:11,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:12,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:13,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:14,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:15,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:16,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:17,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:17,283 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:23:18,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:19,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:20,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:21,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:22,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:23,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:24,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:25,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:26,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:27,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:27,311 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:23:28,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:29,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:30,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:31,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:32,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:33,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:34,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:35,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:36,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:37,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:37,338 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:23:38,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:39,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:40,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:41,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:42,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:43,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:44,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:45,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:46,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:47,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:47,363 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:23:48,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:49,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:50,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:51,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:52,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:53,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:54,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:55,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:56,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:57,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:57,390 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:23:58,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:23:59,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:00,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:01,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:02,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:03,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:04,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:05,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:06,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:07,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:07,413 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:24:08,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:09,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:10,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:11,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:12,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:13,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:14,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:15,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:16,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:17,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:17,439 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:24:18,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:19,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:20,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:21,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:22,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:23,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:24,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:25,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:26,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:27,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:27,459 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:24:28,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:29,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:30,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:31,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:32,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:33,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:34,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:35,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:36,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:37,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:37,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:24:38,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:39,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:40,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:41,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:42,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:43,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:44,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:45,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:46,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:47,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:47,499 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:24:48,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:49,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:50,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:51,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:52,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:53,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:54,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:55,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:56,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:57,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:57,515 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:24:58,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:24:59,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:00,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:01,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:02,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:03,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:04,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:05,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:06,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:07,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:07,526 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:25:08,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:09,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:10,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:11,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:12,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:13,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:14,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:15,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:16,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:17,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:17,539 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:25:18,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:19,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:20,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:21,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:22,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:23,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:24,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:25,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:26,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:27,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:27,552 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:25:28,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:29,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:30,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:31,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:32,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:33,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:34,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:35,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:36,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:37,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:37,567 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:25:38,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:39,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:40,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:41,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:42,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:43,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:44,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:45,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:46,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:47,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:47,584 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:25:48,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:49,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:50,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:51,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:52,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:53,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:54,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:55,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:56,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:57,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:57,595 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:25:58,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:25:59,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:00,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:01,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:02,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:03,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:04,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:05,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:06,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:07,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:07,611 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:26:08,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:09,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:10,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:11,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:12,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:13,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:14,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:15,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:16,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:17,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:17,630 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:26:18,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:19,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:20,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:21,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:22,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:23,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:24,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:25,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:26,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:27,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:27,644 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:26:28,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:29,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:30,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:31,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:32,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:33,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:34,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:35,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:36,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:37,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:37,662 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:26:38,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:39,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:40,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:41,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:42,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:43,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:44,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:45,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:46,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:47,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:47,676 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:26:48,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:49,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:50,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:51,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:52,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:53,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:54,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:55,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:56,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:57,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:57,694 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:26:58,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:26:59,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:00,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:01,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:02,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:03,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:04,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:05,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:06,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:07,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:07,714 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:27:08,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:09,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:10,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:11,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:12,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:13,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:14,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:15,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:16,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:17,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:17,731 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:27:18,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:19,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:20,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:21,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:22,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:23,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:24,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:25,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:26,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:27,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:27,749 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:27:28,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:29,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:30,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:31,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:32,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:33,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:34,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:35,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:36,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:37,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:37,767 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:27:38,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:39,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:40,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:41,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:42,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:43,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:44,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:45,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:46,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:47,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:47,792 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:27:48,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:49,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:50,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:51,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:52,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:53,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:54,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:55,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:56,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:57,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:57,806 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:27:58,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:27:59,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:00,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:01,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:02,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:03,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:04,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:05,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:06,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:07,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:07,821 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:28:08,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:09,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:10,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:11,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:12,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:13,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:14,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:15,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:16,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:17,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:17,836 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:28:18,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:19,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:20,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:21,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:22,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:23,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:24,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:25,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:26,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:27,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:27,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:28:28,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:29,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:30,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:31,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:32,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:33,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:34,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:35,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:36,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:37,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:37,875 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:28:38,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:39,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:40,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:41,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:42,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:43,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:44,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:45,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:46,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:47,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:47,888 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:28:48,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:49,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:50,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:51,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:52,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:53,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:54,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:55,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:56,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:57,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:57,906 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:28:58,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:28:59,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:00,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:01,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:02,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:03,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:04,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:05,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:06,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:07,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:07,924 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:29:08,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:09,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:10,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:11,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:12,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:13,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:14,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:15,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:16,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:17,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:17,946 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:29:18,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:19,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:20,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:21,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:22,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:23,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:24,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:25,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:26,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:27,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:27,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:29:28,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:29,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:30,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:31,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:32,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:33,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:34,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:35,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:36,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:37,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:37,984 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:29:38,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:39,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:40,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:41,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:42,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:43,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:44,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:45,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:47,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:48,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:48,002 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:29:49,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:50,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:51,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:52,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:53,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:54,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:55,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:56,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:57,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:58,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:29:58,018 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:29:59,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:00,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:01,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:02,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:03,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:04,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:05,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:06,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:07,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:08,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:08,035 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:30:09,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:10,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:11,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:12,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:13,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:14,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:15,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:16,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:17,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:18,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:18,054 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:30:19,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:20,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:21,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:22,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:23,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:24,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:25,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:26,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:27,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:28,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:28,069 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:30:29,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:30,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:31,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:32,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:33,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:34,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:35,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:36,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:37,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:38,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:38,083 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:30:39,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:40,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:41,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:42,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:43,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:44,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:45,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:46,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:47,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:48,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:48,100 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:30:49,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:50,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:51,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:52,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:53,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:54,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:55,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:56,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:57,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:58,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:30:58,119 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:30:59,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:00,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:01,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:02,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:03,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:04,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:05,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:06,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:07,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:08,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:08,132 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:31:09,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:10,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:11,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:12,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:13,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:14,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:15,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:16,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:17,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:18,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:18,151 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:31:19,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:20,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:21,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:22,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:23,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:24,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:25,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:26,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:27,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:28,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:28,170 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:31:29,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:30,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:31,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:32,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:33,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:34,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:35,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:36,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:37,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:38,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:38,188 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:31:39,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:40,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:41,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:42,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:43,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:44,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:45,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:46,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:47,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:48,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:48,206 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:31:49,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:50,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:51,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:52,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:53,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:54,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:55,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:56,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:57,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:58,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:31:58,221 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:31:59,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:00,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:01,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:02,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:03,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:04,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:05,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:06,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:07,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:08,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:08,239 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:32:09,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:10,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:11,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:12,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:13,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:14,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:15,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:16,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:17,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:18,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:18,255 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:32:19,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:20,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:21,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:22,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:23,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:24,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:25,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:26,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:27,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:28,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:28,274 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:32:29,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:30,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:31,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:32,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:33,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:34,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:35,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:36,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:37,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:38,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:38,290 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:32:39,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:40,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:41,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:42,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:43,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:44,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:45,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:46,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:47,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:48,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:48,309 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:32:49,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:50,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:51,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:52,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:53,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:54,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:55,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:56,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:57,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:58,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:32:58,335 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:32:59,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:00,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:01,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:02,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:03,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:04,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:05,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:06,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:07,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:08,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:08,350 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:33:09,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:10,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:11,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:12,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:13,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:14,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:15,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:16,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:17,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:18,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:18,369 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:33:19,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:20,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:21,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:22,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:23,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:24,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:25,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:26,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:27,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:28,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:28,385 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:33:29,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:30,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:31,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:32,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:33,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:34,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:35,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:36,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:37,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:38,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:38,405 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:33:39,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:40,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:41,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:42,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:43,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:44,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:45,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:46,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:47,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:48,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:48,428 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:33:49,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:50,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:51,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:52,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:53,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:54,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:55,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:56,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:57,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:58,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:33:58,449 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:33:59,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:00,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:01,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:02,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:03,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:04,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:05,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:06,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:07,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:08,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:08,461 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:34:09,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:10,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:11,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:12,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:13,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:14,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:15,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:16,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:17,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:18,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:18,476 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:34:19,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:20,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:21,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:22,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:23,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:24,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:25,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:26,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:27,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:28,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:28,493 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:34:29,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:30,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:31,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:32,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:33,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:34,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:35,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:36,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:37,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:38,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:38,507 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:34:39,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:40,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:41,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:42,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:43,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:44,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:45,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:46,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:47,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:48,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:48,522 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:34:49,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:50,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:51,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:52,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:53,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:54,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:55,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:56,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:57,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:58,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:34:58,542 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:34:59,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:00,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:01,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:02,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:03,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:04,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:05,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:06,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:07,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:08,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:08,560 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:35:09,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:10,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:11,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:12,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:13,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:14,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:15,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:16,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:17,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:18,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:18,574 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:35:19,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:20,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:21,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:22,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:23,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:24,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:25,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:26,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:27,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:28,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:28,590 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:35:29,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:30,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:31,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:32,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:33,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:34,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:35,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:36,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:37,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:38,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:38,606 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:35:39,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:40,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:41,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:42,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:43,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:44,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:45,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:46,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:47,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:48,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:48,625 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:35:49,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:50,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:51,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:52,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:53,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:54,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:55,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:56,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:57,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:58,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:35:58,644 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:35:59,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:00,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:01,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:02,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:03,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:04,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:05,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:06,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:07,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:08,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:08,662 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:36:09,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:10,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:11,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:12,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:13,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:14,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:15,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:16,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:17,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:18,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:18,680 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:36:19,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:20,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:21,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:22,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:23,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:24,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:25,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:26,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:27,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:28,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:28,703 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:36:29,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:30,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:31,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:32,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:33,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:34,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:35,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:36,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:37,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:38,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:38,723 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:36:39,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:40,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:41,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:42,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:43,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:44,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:45,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:46,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:47,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:48,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:48,739 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:36:49,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:50,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:51,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:52,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:53,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:54,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:55,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:56,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:57,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:58,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:36:58,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:36:59,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:00,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:01,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:02,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:03,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:04,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:05,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:06,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:07,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:08,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:08,776 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:37:09,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:10,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:11,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:12,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:13,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:14,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:15,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:16,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:17,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:18,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:18,800 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:37:19,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:20,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:21,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:22,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:23,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:24,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:25,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:26,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:27,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:28,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:28,815 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:37:29,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:30,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:31,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:32,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:33,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:34,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:35,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:36,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:37,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:38,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:38,831 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:37:39,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:40,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:41,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:42,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:43,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:44,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:45,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:46,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:47,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:48,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:48,843 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:37:49,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:50,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:51,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:52,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:53,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:54,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:55,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:56,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:57,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:58,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:37:58,857 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:37:59,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:00,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:01,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:02,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:03,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:04,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:05,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:06,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:07,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:08,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:08,873 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:38:09,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:10,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:11,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:12,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:13,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:14,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:15,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:16,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:17,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:18,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:18,893 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:38:19,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:20,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:21,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:22,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:23,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:24,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:25,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:26,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:27,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:28,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:28,918 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:38:29,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:30,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:31,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:32,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:33,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:34,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:35,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:36,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:37,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:38,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:38,935 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:38:39,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:40,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:41,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:42,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:43,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:44,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:45,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:46,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:47,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:48,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:48,950 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:38:49,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:50,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:51,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:52,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:53,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:54,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:55,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:56,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:57,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:58,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:38:58,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:38:59,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:00,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:01,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:02,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:03,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:04,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:05,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:06,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:07,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:08,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:08,978 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:39:09,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:10,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:11,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:12,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:13,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:14,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:15,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:16,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:17,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:18,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:18,997 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:39:19,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:21,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:22,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:23,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:24,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:25,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:26,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:27,007 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:28,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:29,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:29,011 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:39:30,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:31,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:32,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:33,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:34,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:35,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:36,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:37,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:38,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:39,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:39,029 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:39:40,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:41,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:42,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:43,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:44,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:45,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:46,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:47,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:48,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:49,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:49,047 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:39:50,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:51,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:52,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:53,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:54,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:55,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:56,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:57,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:58,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:59,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:39:59,065 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:40:00,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:01,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:02,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:03,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:04,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:05,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:06,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:07,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:08,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:09,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:09,084 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:40:10,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:11,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:12,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:13,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:14,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:15,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:16,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:17,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:18,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:19,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:19,103 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:40:20,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:21,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:22,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:23,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:24,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:25,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:26,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:27,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:28,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:29,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:29,120 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:40:30,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:31,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:32,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:33,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:34,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:35,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:36,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:37,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:38,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:39,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:39,141 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:40:40,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:41,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:42,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:43,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:44,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:45,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:46,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:47,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:48,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:49,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:49,156 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:40:50,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:51,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:52,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:53,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:54,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:55,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:56,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:57,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:58,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:59,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:40:59,187 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:41:00,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:01,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:02,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:03,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:04,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:05,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:06,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:07,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:08,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:09,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:09,204 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:41:10,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:11,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:12,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:13,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:14,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:15,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:16,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:17,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:18,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:19,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:19,225 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:41:20,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:21,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:22,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:23,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:24,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:25,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:26,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:27,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:28,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:29,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:29,243 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:41:30,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:31,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:32,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:33,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:34,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:35,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:36,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:37,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:38,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:39,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:39,257 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:41:40,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:41,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:42,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:43,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:44,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:45,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:46,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:47,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:48,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:49,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:49,271 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:41:50,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:51,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:52,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:53,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:54,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:55,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:56,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:57,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:58,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:59,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:41:59,287 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:42:00,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:01,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:02,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:03,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:04,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:05,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:06,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:07,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:08,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:09,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:09,311 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:42:10,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:11,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:12,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:13,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:14,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:15,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:16,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:17,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:18,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:19,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:19,324 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:42:20,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:21,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:22,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:23,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:24,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:25,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:26,333 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:27,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:28,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:29,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:29,339 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:42:30,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:31,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:32,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:33,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:34,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:35,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:36,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:37,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:38,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:39,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:39,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:42:40,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:41,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:42,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:43,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:44,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:45,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:46,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:47,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:48,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:49,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:49,371 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:42:50,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:51,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:52,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:53,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:54,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:55,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:56,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:57,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:58,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:59,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:42:59,384 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:43:00,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:01,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:02,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:03,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:04,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:05,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:06,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:07,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:08,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:09,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:09,398 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:43:10,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:11,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:12,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:13,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:14,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:15,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:16,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:17,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:18,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:19,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:19,410 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:43:20,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:21,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:22,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:23,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:24,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:25,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:26,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:27,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:28,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:29,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:29,424 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:43:30,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:31,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:32,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:33,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:34,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:35,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:36,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:37,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:38,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:39,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:39,440 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:43:40,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:41,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:42,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:43,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:44,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:45,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:46,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:47,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:48,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:49,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:49,454 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:43:50,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:51,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:52,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:53,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:54,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:55,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:56,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:57,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:58,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:59,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:43:59,469 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:44:00,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:01,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:02,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:03,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:04,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:05,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:06,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:07,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:08,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:09,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:09,493 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:44:10,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:11,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:12,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:13,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:14,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:15,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:16,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:17,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:18,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:19,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:19,509 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:44:20,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:21,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:22,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:23,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:24,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:25,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:26,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:27,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:28,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:29,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:29,525 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:44:30,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:31,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:32,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:33,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:34,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:35,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:36,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:37,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:38,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:39,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:39,543 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:44:40,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:41,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:42,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:43,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:44,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:45,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:46,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:47,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:48,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:49,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:49,556 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:44:50,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:51,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:52,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:53,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:54,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:55,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:56,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:57,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:58,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:59,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:44:59,573 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:45:00,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:01,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:02,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:03,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:04,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:05,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:06,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:07,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:08,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:09,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:09,587 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:45:10,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:11,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:12,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:13,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:14,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:15,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:16,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:17,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:18,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:19,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:19,604 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:45:20,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:21,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:22,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:23,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:24,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:25,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:26,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:27,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:28,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:29,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:29,623 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:45:30,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:31,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:32,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:33,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:34,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:35,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:36,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:37,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:38,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:39,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:39,638 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:45:40,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:41,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:42,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:43,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:44,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:45,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:46,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:47,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:48,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:49,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:49,655 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:45:50,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:51,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:52,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:53,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:54,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:55,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:56,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:57,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:58,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:59,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:45:59,671 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:46:00,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:01,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:02,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:03,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:04,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:05,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:06,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:07,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:08,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:09,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:09,688 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:46:10,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:11,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:12,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:13,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:14,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:15,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:16,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:17,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:18,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:19,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:19,705 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:46:20,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:21,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:22,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:23,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:24,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:25,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:26,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:27,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:28,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:29,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:29,721 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:46:30,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:31,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:32,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:33,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:34,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:35,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:36,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:37,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:38,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:39,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:39,738 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:46:40,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:41,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:42,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:43,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:44,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:45,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:46,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:47,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:48,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:49,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:49,754 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:46:50,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:51,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:52,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:53,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:54,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:55,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:56,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:57,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:58,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:59,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:46:59,772 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:47:00,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:01,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:02,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:03,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:04,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:05,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:06,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:07,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:08,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:09,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:09,786 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:47:10,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:11,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:12,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:13,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:14,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:15,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:16,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:17,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:18,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:19,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:19,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:47:20,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:21,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:22,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:23,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:24,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:25,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:26,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:27,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:28,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:29,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:29,814 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:47:30,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:31,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:32,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:33,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:34,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:35,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:36,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:37,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:38,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:39,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:39,828 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:47:40,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:41,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:42,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:43,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:47:43,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoopmaster/192.168.54.130:9000 with active state
2015-10-25 16:47:43,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-25 16:47:43,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-25 16:47:43,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1cfecacdf497,  containing 1 storage report(s), of which we sent 1. The reports had 91 total blocks and used 1 RPC(s). This took 0 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 16:47:43,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 16:51:04,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742116_1292 src: /192.168.54.130:58007 dest: /192.168.54.130:50010
2015-10-25 16:51:04,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58007, dest: /192.168.54.130:50010, bytes: 4708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_864021944_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742116_1292, duration: 38133622
2015-10-25 16:51:04,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742116_1292, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:51:04,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742117_1293 src: /192.168.54.130:58008 dest: /192.168.54.130:50010
2015-10-25 16:51:04,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58008, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_864021944_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742117_1293, duration: 6185714
2015-10-25 16:51:04,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742117_1293, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 16:51:04,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742118_1294 src: /192.168.54.130:58010 dest: /192.168.54.130:50010
2015-10-25 16:51:04,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58010, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_864021944_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742118_1294, duration: 1069512
2015-10-25 16:51:04,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742118_1294, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:51:04,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742119_1295 src: /192.168.54.130:58011 dest: /192.168.54.130:50010
2015-10-25 16:51:04,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58011, dest: /192.168.54.130:50010, bytes: 95860, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_864021944_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742119_1295, duration: 10286189
2015-10-25 16:51:04,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742119_1295, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:51:07,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742116_1292 to 192.168.54.131:50010 
2015-10-25 16:51:07,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742116_1292 (numBytes=4708) to /192.168.54.131:50010
2015-10-25 16:51:43,361 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1085ms
No GCs detected
2015-10-25 16:51:48,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:50,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:51,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:52,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:53,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:54,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:56,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:57,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:58,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:59,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:51:59,848 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:52:03,751 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3395ms
GC pool 'Copy' had collection(s): count=1 time=3412ms
2015-10-25 16:52:05,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:06,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:07,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:08,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:09,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:10,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:11,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:12,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:13,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:14,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:14,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:52:16,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:17,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:18,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:19,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:20,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:21,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:22,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:23,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:24,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:25,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:25,113 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:52:26,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:27,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:28,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:29,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:30,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:31,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:32,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:33,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:34,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:35,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:35,221 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:52:36,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:37,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:38,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:39,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:40,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:41,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:42,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:43,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:44,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:45,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:45,264 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:52:46,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:47,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:48,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:49,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:50,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:51,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:52,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:53,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:54,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:55,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:55,280 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:52:56,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:57,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:58,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:52:59,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:00,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:01,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:02,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:03,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:04,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:05,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:05,297 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:53:06,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:07,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:08,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:09,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:10,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:11,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:12,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:13,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:14,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:15,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:15,311 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 16:53:16,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:17,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 16:53:17,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoopmaster/192.168.54.130:9000 with active state
2015-10-25 16:53:17,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-25 16:53:17,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-25 16:53:17,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1d4c7a81c1f0,  containing 1 storage report(s), of which we sent 1. The reports had 95 total blocks and used 1 RPC(s). This took 2 msec to generate and 20 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 16:53:17,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 16:54:05,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742122_1298 src: /192.168.54.130:58253 dest: /192.168.54.130:50010
2015-10-25 16:54:05,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58253, dest: /192.168.54.130:50010, bytes: 4708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1047172859_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742122_1298, duration: 38704200
2015-10-25 16:54:05,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742122_1298, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:54:05,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742123_1299 src: /192.168.54.130:58254 dest: /192.168.54.130:50010
2015-10-25 16:54:05,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58254, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1047172859_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742123_1299, duration: 12684377
2015-10-25 16:54:05,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742123_1299, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 16:54:05,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742124_1300 src: /192.168.54.130:58256 dest: /192.168.54.130:50010
2015-10-25 16:54:05,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58256, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1047172859_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742124_1300, duration: 2327083
2015-10-25 16:54:05,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742124_1300, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:54:05,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742125_1301 src: /192.168.54.130:58257 dest: /192.168.54.130:50010
2015-10-25 16:54:05,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58257, dest: /192.168.54.130:50010, bytes: 95860, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1047172859_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742125_1301, duration: 11626623
2015-10-25 16:54:05,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742125_1301, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:54:05,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742122_1298 to 192.168.54.131:50010 
2015-10-25 16:54:05,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742122_1298 (numBytes=4708) to /192.168.54.131:50010
2015-10-25 16:54:10,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742126_1302 src: /192.168.54.130:58267 dest: /192.168.54.130:50010
2015-10-25 16:54:10,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58267, dest: /192.168.54.130:50010, bytes: 114019, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_387968776_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742126_1302, duration: 70500901
2015-10-25 16:54:10,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742126_1302, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:54:38,690 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1153ms
No GCs detected
2015-10-25 16:54:39,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742127_1303 src: /192.168.54.130:58307 dest: /192.168.54.130:50010
2015-10-25 16:55:43,934 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1303ms
No GCs detected
2015-10-25 16:57:02,164 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1219ms
No GCs detected
2015-10-25 16:59:31,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58307, dest: /192.168.54.130:50010, bytes: 459521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_387968776_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742127_1303, duration: 291763525225
2015-10-25 16:59:31,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742127_1303, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:59:31,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742129_1305 src: /192.168.54.130:58583 dest: /192.168.54.130:50010
2015-10-25 16:59:31,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58583, dest: /192.168.54.130:50010, bytes: 358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_387968776_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742129_1305, duration: 1520636
2015-10-25 16:59:31,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742129_1305, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:59:31,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742130_1306 src: /192.168.54.130:58585 dest: /192.168.54.130:50010
2015-10-25 16:59:31,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58585, dest: /192.168.54.130:50010, bytes: 459521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_387968776_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742130_1306, duration: 18649077
2015-10-25 16:59:31,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742130_1306, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:59:31,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742131_1307 src: /192.168.54.130:58586 dest: /192.168.54.130:50010
2015-10-25 16:59:31,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58586, dest: /192.168.54.130:50010, bytes: 114019, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_387968776_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742131_1307, duration: 13238011
2015-10-25 16:59:31,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742131_1307, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 16:59:35,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742122_1298 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742122 for deletion
2015-10-25 16:59:35,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742123_1299 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742123 for deletion
2015-10-25 16:59:35,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742124_1300 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742124 for deletion
2015-10-25 16:59:35,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742125_1301 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742125 for deletion
2015-10-25 16:59:35,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742126_1302 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742126 for deletion
2015-10-25 16:59:35,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742127_1303 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742127 for deletion
2015-10-25 16:59:35,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742122_1298 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742122
2015-10-25 16:59:35,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742123_1299 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742123
2015-10-25 16:59:35,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742124_1300 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742124
2015-10-25 16:59:35,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742125_1301 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742125
2015-10-25 16:59:35,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742126_1302 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742126
2015-10-25 16:59:35,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742127_1303 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742127
2015-10-25 17:11:26,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742080_1256 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742080 for deletion
2015-10-25 17:11:26,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742081_1257 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742081 for deletion
2015-10-25 17:11:26,544 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742082_1258 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742082 for deletion
2015-10-25 17:11:26,544 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742083_1259 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742083 for deletion
2015-10-25 17:11:26,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742084_1260 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742084 for deletion
2015-10-25 17:11:26,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742085_1261 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742085 for deletion
2015-10-25 17:11:26,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742086_1262 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742086 for deletion
2015-10-25 17:11:26,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742087_1263 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742087 for deletion
2015-10-25 17:11:26,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742088_1264 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742088 for deletion
2015-10-25 17:11:26,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742080_1256 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742080
2015-10-25 17:11:26,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742089_1265 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742089 for deletion
2015-10-25 17:11:26,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742090_1266 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742090 for deletion
2015-10-25 17:11:26,550 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742091_1267 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742091 for deletion
2015-10-25 17:11:26,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742092_1268 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742092 for deletion
2015-10-25 17:11:26,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742093_1269 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742093 for deletion
2015-10-25 17:11:26,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742032_1208 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742032 for deletion
2015-10-25 17:11:26,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742033_1209 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742033 for deletion
2015-10-25 17:11:26,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742081_1257 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742081
2015-10-25 17:11:26,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742034_1210 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742034 for deletion
2015-10-25 17:11:26,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742035_1211 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742035 for deletion
2015-10-25 17:11:26,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742036_1212 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742036 for deletion
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742082_1258 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742082
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742083_1259 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742083
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742084_1260 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742084
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742085_1261 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742085
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742037_1213 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742037 for deletion
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742038_1214 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742038 for deletion
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742039_1215 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742039 for deletion
2015-10-25 17:11:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742040_1216 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742040 for deletion
2015-10-25 17:11:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742086_1262 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742086
2015-10-25 17:11:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742041_1217 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742041 for deletion
2015-10-25 17:11:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742042_1218 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742042 for deletion
2015-10-25 17:11:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742087_1263 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742087
2015-10-25 17:11:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742043_1219 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742043 for deletion
2015-10-25 17:11:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742044_1220 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742044 for deletion
2015-10-25 17:11:26,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742088_1264 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742088
2015-10-25 17:11:26,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742045_1221 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742045 for deletion
2015-10-25 17:11:26,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742046_1222 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742046 for deletion
2015-10-25 17:11:26,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742047_1223 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742047 for deletion
2015-10-25 17:11:26,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742048_1224 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742048 for deletion
2015-10-25 17:11:26,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742049_1225 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742049 for deletion
2015-10-25 17:11:26,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742089_1265 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742089
2015-10-25 17:11:26,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742050_1226 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742050 for deletion
2015-10-25 17:11:26,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742051_1227 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742051 for deletion
2015-10-25 17:11:26,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742052_1228 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742052 for deletion
2015-10-25 17:11:26,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742090_1266 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742090
2015-10-25 17:11:26,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742053_1229 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742053 for deletion
2015-10-25 17:11:26,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742091_1267 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742091
2015-10-25 17:11:26,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742054_1230 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742054 for deletion
2015-10-25 17:11:26,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742055_1231 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742055 for deletion
2015-10-25 17:11:26,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742092_1268 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742092
2015-10-25 17:11:26,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742056_1232 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742056 for deletion
2015-10-25 17:11:26,558 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742093_1269 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742093
2015-10-25 17:11:26,558 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742057_1233 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742057 for deletion
2015-10-25 17:11:26,558 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742058_1234 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742058 for deletion
2015-10-25 17:11:26,558 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742059_1235 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742059 for deletion
2015-10-25 17:11:26,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742060_1236 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742060 for deletion
2015-10-25 17:11:26,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742061_1237 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742061 for deletion
2015-10-25 17:11:26,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742032_1208 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742032
2015-10-25 17:11:26,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742062_1238 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742062 for deletion
2015-10-25 17:11:26,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742063_1239 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742063 for deletion
2015-10-25 17:11:26,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742064_1240 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742064 for deletion
2015-10-25 17:11:26,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742033_1209 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742033
2015-10-25 17:11:26,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742065_1241 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742065 for deletion
2015-10-25 17:11:26,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742066_1242 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742066 for deletion
2015-10-25 17:11:26,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742067_1243 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742067 for deletion
2015-10-25 17:11:26,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742068_1244 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742068 for deletion
2015-10-25 17:11:26,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742069_1245 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742069 for deletion
2015-10-25 17:11:26,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742070_1246 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742070 for deletion
2015-10-25 17:11:26,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742034_1210 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742034
2015-10-25 17:11:26,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742071_1247 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742071 for deletion
2015-10-25 17:11:26,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742072_1248 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742072 for deletion
2015-10-25 17:11:26,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742035_1211 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742035
2015-10-25 17:11:26,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742073_1249 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742073 for deletion
2015-10-25 17:11:26,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742036_1212 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742036
2015-10-25 17:11:26,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742037_1213 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742037
2015-10-25 17:11:26,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742074_1250 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742074 for deletion
2015-10-25 17:11:26,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742038_1214 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742038
2015-10-25 17:11:26,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742039_1215 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742039
2015-10-25 17:11:26,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742075_1251 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742075 for deletion
2015-10-25 17:11:26,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742076_1252 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742076 for deletion
2015-10-25 17:11:26,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742040_1216 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742040
2015-10-25 17:11:26,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742077_1253 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742077 for deletion
2015-10-25 17:11:26,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742041_1217 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742041
2015-10-25 17:11:26,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742078_1254 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742078 for deletion
2015-10-25 17:11:26,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742042_1218 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742042
2015-10-25 17:11:26,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742043_1219 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742043
2015-10-25 17:11:26,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742079_1255 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742079 for deletion
2015-10-25 17:11:26,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742044_1220 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742044
2015-10-25 17:11:26,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742045_1221 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742045
2015-10-25 17:11:26,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742046_1222 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742046
2015-10-25 17:11:26,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742047_1223 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742047
2015-10-25 17:11:26,572 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742048_1224 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742048
2015-10-25 17:11:26,572 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742049_1225 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742049
2015-10-25 17:11:26,573 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742050_1226 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742050
2015-10-25 17:11:26,573 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742051_1227 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742051
2015-10-25 17:11:26,573 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742052_1228 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742052
2015-10-25 17:11:26,574 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742053_1229 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742053
2015-10-25 17:11:26,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742054_1230 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742054
2015-10-25 17:11:26,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742055_1231 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742055
2015-10-25 17:11:26,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742056_1232 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742056
2015-10-25 17:11:26,576 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742057_1233 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742057
2015-10-25 17:11:26,576 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742058_1234 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742058
2015-10-25 17:11:26,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742059_1235 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742059
2015-10-25 17:11:26,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742060_1236 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742060
2015-10-25 17:11:26,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742061_1237 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742061
2015-10-25 17:11:26,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742062_1238 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742062
2015-10-25 17:11:26,578 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742063_1239 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742063
2015-10-25 17:11:26,578 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742064_1240 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742064
2015-10-25 17:11:26,578 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742065_1241 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742065
2015-10-25 17:11:26,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742066_1242 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742066
2015-10-25 17:11:26,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742067_1243 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742067
2015-10-25 17:11:26,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742068_1244 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742068
2015-10-25 17:11:26,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742069_1245 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742069
2015-10-25 17:11:26,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742070_1246 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742070
2015-10-25 17:11:26,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742071_1247 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742071
2015-10-25 17:11:26,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742072_1248 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742072
2015-10-25 17:11:26,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742073_1249 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742073
2015-10-25 17:11:26,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742074_1250 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742074
2015-10-25 17:11:26,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742075_1251 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742075
2015-10-25 17:11:26,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742076_1252 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742076
2015-10-25 17:11:26,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742077_1253 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742077
2015-10-25 17:11:26,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742078_1254 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742078
2015-10-25 17:11:26,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742079_1255 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir0/blk_1073742079
2015-10-25 17:11:54,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742132_1308 src: /192.168.54.130:58606 dest: /192.168.54.130:50010
2015-10-25 17:11:54,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58606, dest: /192.168.54.130:50010, bytes: 663, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1266802132_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742132_1308, duration: 75391239
2015-10-25 17:11:54,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742132_1308, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:10,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742133_1309 src: /192.168.54.130:58609 dest: /192.168.54.130:50010
2015-10-25 17:12:10,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58609, dest: /192.168.54.130:50010, bytes: 4708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-328669240_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742133_1309, duration: 43964170
2015-10-25 17:12:10,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742133_1309, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:10,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742134_1310 src: /192.168.54.130:58610 dest: /192.168.54.130:50010
2015-10-25 17:12:10,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58610, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-328669240_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742134_1310, duration: 6349834
2015-10-25 17:12:10,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742134_1310, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 17:12:10,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742135_1311 src: /192.168.54.130:58612 dest: /192.168.54.130:50010
2015-10-25 17:12:10,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58612, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-328669240_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742135_1311, duration: 1221530
2015-10-25 17:12:10,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742135_1311, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:10,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742136_1312 src: /192.168.54.130:58613 dest: /192.168.54.130:50010
2015-10-25 17:12:10,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58613, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-328669240_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742136_1312, duration: 5733842
2015-10-25 17:12:10,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742136_1312, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:11,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742133_1309 to 192.168.54.131:50010 
2015-10-25 17:12:11,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742133_1309 (numBytes=4708) to /192.168.54.131:50010
2015-10-25 17:12:16,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742137_1313 src: /192.168.54.130:58622 dest: /192.168.54.130:50010
2015-10-25 17:12:17,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58622, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_608039392_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742137_1313, duration: 38051554
2015-10-25 17:12:17,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742137_1313, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:26,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742138_1314 src: /192.168.54.130:58635 dest: /192.168.54.130:50010
2015-10-25 17:12:31,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58635, dest: /192.168.54.130:50010, bytes: 39943, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_608039392_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742138_1314, duration: 4965502183
2015-10-25 17:12:31,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742138_1314, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:31,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742140_1316 src: /192.168.54.130:58638 dest: /192.168.54.130:50010
2015-10-25 17:12:31,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58638, dest: /192.168.54.130:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_608039392_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742140_1316, duration: 4765222
2015-10-25 17:12:31,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742140_1316, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:31,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742141_1317 src: /192.168.54.130:58640 dest: /192.168.54.130:50010
2015-10-25 17:12:31,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58640, dest: /192.168.54.130:50010, bytes: 39943, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_608039392_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742141_1317, duration: 7703450
2015-10-25 17:12:31,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742141_1317, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:31,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742142_1318 src: /192.168.54.130:58641 dest: /192.168.54.130:50010
2015-10-25 17:12:31,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58641, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_608039392_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742142_1318, duration: 6458843
2015-10-25 17:12:31,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742142_1318, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:12:38,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742133_1309 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742133 for deletion
2015-10-25 17:12:38,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742134_1310 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742134 for deletion
2015-10-25 17:12:38,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742135_1311 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742135 for deletion
2015-10-25 17:12:38,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742136_1312 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742136 for deletion
2015-10-25 17:12:38,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742137_1313 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742137 for deletion
2015-10-25 17:12:38,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742138_1314 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742138 for deletion
2015-10-25 17:12:38,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742133_1309 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742133
2015-10-25 17:12:38,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742134_1310 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742134
2015-10-25 17:12:38,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742135_1311 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742135
2015-10-25 17:12:38,531 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742136_1312 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742136
2015-10-25 17:12:38,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742137_1313 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742137
2015-10-25 17:12:38,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742138_1314 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742138
2015-10-25 17:43:40,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742143_1319 src: /192.168.54.130:58720 dest: /192.168.54.130:50010
2015-10-25 17:43:40,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58720, dest: /192.168.54.130:50010, bytes: 4496, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537605410_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742143_1319, duration: 287513905
2015-10-25 17:43:40,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742143_1319, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:43:41,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742144_1320 src: /192.168.54.130:58721 dest: /192.168.54.130:50010
2015-10-25 17:43:41,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58721, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537605410_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742144_1320, duration: 103378217
2015-10-25 17:43:41,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742144_1320, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 17:43:41,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742145_1321 src: /192.168.54.130:58723 dest: /192.168.54.130:50010
2015-10-25 17:43:42,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58723, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537605410_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742145_1321, duration: 36812128
2015-10-25 17:43:42,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742145_1321, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:43:43,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742146_1322 src: /192.168.54.130:58724 dest: /192.168.54.130:50010
2015-10-25 17:43:43,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58724, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-537605410_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742146_1322, duration: 147897020
2015-10-25 17:43:43,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742146_1322, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:43:44,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742143_1319 to 192.168.54.131:50010 
2015-10-25 17:43:44,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742143_1319 (numBytes=4496) to /192.168.54.131:50010
2015-10-25 17:45:13,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742147_1323 src: /192.168.54.130:58730 dest: /192.168.54.130:50010
2015-10-25 17:45:13,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58730, dest: /192.168.54.130:50010, bytes: 4496, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2136613388_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742147_1323, duration: 44580873
2015-10-25 17:45:13,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742147_1323, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:45:13,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742148_1324 src: /192.168.54.130:58731 dest: /192.168.54.130:50010
2015-10-25 17:45:13,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58731, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2136613388_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742148_1324, duration: 17738042
2015-10-25 17:45:13,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742148_1324, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 17:45:13,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742149_1325 src: /192.168.54.130:58733 dest: /192.168.54.130:50010
2015-10-25 17:45:13,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58733, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2136613388_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742149_1325, duration: 3603252
2015-10-25 17:45:13,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742149_1325, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:45:13,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742150_1326 src: /192.168.54.130:58734 dest: /192.168.54.130:50010
2015-10-25 17:45:13,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58734, dest: /192.168.54.130:50010, bytes: 95858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2136613388_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742150_1326, duration: 18918224
2015-10-25 17:45:13,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742150_1326, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:45:17,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742147_1323 to 192.168.54.131:50010 
2015-10-25 17:45:17,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742147_1323 (numBytes=4496) to /192.168.54.131:50010
2015-10-25 17:45:24,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742151_1327 src: /192.168.54.130:58744 dest: /192.168.54.130:50010
2015-10-25 17:45:24,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58744, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_394429164_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742151_1327, duration: 78782366
2015-10-25 17:45:24,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742151_1327, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:45:37,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742152_1328 src: /192.168.54.130:58751 dest: /192.168.54.130:50010
2015-10-25 17:46:01,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58751, dest: /192.168.54.130:50010, bytes: 27038, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_394429164_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742152_1328, duration: 23944492259
2015-10-25 17:46:01,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742152_1328, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:46:02,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742153_1329 src: /192.168.54.130:58762 dest: /192.168.54.130:50010
2015-10-25 17:46:02,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58762, dest: /192.168.54.130:50010, bytes: 338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_394429164_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742153_1329, duration: 112344648
2015-10-25 17:46:02,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742153_1329, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:46:03,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742154_1330 src: /192.168.54.130:58764 dest: /192.168.54.130:50010
2015-10-25 17:46:03,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58764, dest: /192.168.54.130:50010, bytes: 27038, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_394429164_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742154_1330, duration: 79091913
2015-10-25 17:46:03,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742154_1330, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:46:03,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742155_1331 src: /192.168.54.130:58765 dest: /192.168.54.130:50010
2015-10-25 17:46:03,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58765, dest: /192.168.54.130:50010, bytes: 114017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_394429164_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742155_1331, duration: 32883429
2015-10-25 17:46:03,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742155_1331, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:46:11,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742147_1323 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742147 for deletion
2015-10-25 17:46:11,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742148_1324 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742148 for deletion
2015-10-25 17:46:11,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742149_1325 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742149 for deletion
2015-10-25 17:46:11,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742150_1326 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742150 for deletion
2015-10-25 17:46:11,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742151_1327 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742151 for deletion
2015-10-25 17:46:11,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742152_1328 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742152 for deletion
2015-10-25 17:46:11,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742147_1323 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742147
2015-10-25 17:46:11,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742148_1324 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742148
2015-10-25 17:46:11,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742149_1325 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742149
2015-10-25 17:46:11,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742150_1326 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742150
2015-10-25 17:46:11,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742151_1327 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742151
2015-10-25 17:46:11,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742152_1328 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742152
2015-10-25 17:47:41,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742156_1332 src: /192.168.54.130:58773 dest: /192.168.54.130:50010
2015-10-25 17:47:41,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58773, dest: /192.168.54.130:50010, bytes: 4492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_432273749_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742156_1332, duration: 32487567
2015-10-25 17:47:41,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742156_1332, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:47:41,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742157_1333 src: /192.168.54.130:58774 dest: /192.168.54.130:50010
2015-10-25 17:47:41,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58774, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_432273749_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742157_1333, duration: 5154747
2015-10-25 17:47:41,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742157_1333, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 17:47:41,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742158_1334 src: /192.168.54.130:58776 dest: /192.168.54.130:50010
2015-10-25 17:47:41,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58776, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_432273749_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742158_1334, duration: 1428144
2015-10-25 17:47:41,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742158_1334, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:47:41,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742159_1335 src: /192.168.54.130:58777 dest: /192.168.54.130:50010
2015-10-25 17:47:41,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58777, dest: /192.168.54.130:50010, bytes: 95861, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_432273749_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742159_1335, duration: 5818915
2015-10-25 17:47:41,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742159_1335, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:47:44,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742156_1332 to 192.168.54.131:50010 
2015-10-25 17:47:44,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742156_1332 (numBytes=4492) to /192.168.54.131:50010
2015-10-25 17:48:11,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742162_1338 src: /192.168.54.130:58800 dest: /192.168.54.130:50010
2015-10-25 17:48:11,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58800, dest: /192.168.54.130:50010, bytes: 7859, op: HDFS_WRITE, cliID: DFSClient_attempt_1445813912906_0008_r_000000_0_1286165613_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742162_1338, duration: 39494573
2015-10-25 17:48:11,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742162_1338, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:48:20,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742156_1332 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742156 for deletion
2015-10-25 17:48:20,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742157_1333 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742157 for deletion
2015-10-25 17:48:20,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742158_1334 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742158 for deletion
2015-10-25 17:48:20,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742159_1335 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742159 for deletion
2015-10-25 17:48:20,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742156_1332 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742156
2015-10-25 17:48:20,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742157_1333 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742157
2015-10-25 17:48:20,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742158_1334 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742158
2015-10-25 17:48:20,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742159_1335 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742159
2015-10-25 17:52:02,533 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742132_1308 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742132 for deletion
2015-10-25 17:52:02,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742132_1308 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742132
2015-10-25 17:54:00,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742166_1345 src: /192.168.54.130:58814 dest: /192.168.54.130:50010
2015-10-25 17:54:00,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58814, dest: /192.168.54.130:50010, bytes: 663, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742166_1345, duration: 46316158
2015-10-25 17:54:00,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742166_1345, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742167_1346 src: /192.168.54.130:58815 dest: /192.168.54.130:50010
2015-10-25 17:54:00,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58815, dest: /192.168.54.130:50010, bytes: 4491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742167_1346, duration: 1448786
2015-10-25 17:54:00,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742167_1346, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742168_1347 src: /192.168.54.130:58816 dest: /192.168.54.130:50010
2015-10-25 17:54:00,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58816, dest: /192.168.54.130:50010, bytes: 4299, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742168_1347, duration: 1804208
2015-10-25 17:54:00,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742168_1347, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742169_1348 src: /192.168.54.130:58817 dest: /192.168.54.130:50010
2015-10-25 17:54:00,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58817, dest: /192.168.54.130:50010, bytes: 9525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742169_1348, duration: 2011173
2015-10-25 17:54:00,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742169_1348, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742170_1349 src: /192.168.54.130:58818 dest: /192.168.54.130:50010
2015-10-25 17:54:00,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58818, dest: /192.168.54.130:50010, bytes: 5957, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742170_1349, duration: 2987682
2015-10-25 17:54:00,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742170_1349, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742171_1350 src: /192.168.54.130:58819 dest: /192.168.54.130:50010
2015-10-25 17:54:00,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58819, dest: /192.168.54.130:50010, bytes: 5268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742171_1350, duration: 2366663
2015-10-25 17:54:00,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742171_1350, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742172_1351 src: /192.168.54.130:58820 dest: /192.168.54.130:50010
2015-10-25 17:54:00,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58820, dest: /192.168.54.130:50010, bytes: 13016, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742172_1351, duration: 3027464
2015-10-25 17:54:00,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742172_1351, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742173_1352 src: /192.168.54.130:58821 dest: /192.168.54.130:50010
2015-10-25 17:54:00,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58821, dest: /192.168.54.130:50010, bytes: 11168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742173_1352, duration: 1663292
2015-10-25 17:54:00,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742173_1352, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742174_1353 src: /192.168.54.130:58822 dest: /192.168.54.130:50010
2015-10-25 17:54:00,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58822, dest: /192.168.54.130:50010, bytes: 10990, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742174_1353, duration: 1565569
2015-10-25 17:54:00,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742174_1353, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742175_1354 src: /192.168.54.130:58823 dest: /192.168.54.130:50010
2015-10-25 17:54:00,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58823, dest: /192.168.54.130:50010, bytes: 9742, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742175_1354, duration: 1595374
2015-10-25 17:54:00,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742175_1354, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742176_1355 src: /192.168.54.130:58824 dest: /192.168.54.130:50010
2015-10-25 17:54:00,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58824, dest: /192.168.54.130:50010, bytes: 12483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742176_1355, duration: 1780475
2015-10-25 17:54:00,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742176_1355, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742177_1356 src: /192.168.54.130:58825 dest: /192.168.54.130:50010
2015-10-25 17:54:00,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58825, dest: /192.168.54.130:50010, bytes: 8938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742177_1356, duration: 3963165
2015-10-25 17:54:00,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742177_1356, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742178_1357 src: /192.168.54.130:58826 dest: /192.168.54.130:50010
2015-10-25 17:54:00,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58826, dest: /192.168.54.130:50010, bytes: 3948, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742178_1357, duration: 1840155
2015-10-25 17:54:00,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742178_1357, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742179_1358 src: /192.168.54.130:58827 dest: /192.168.54.130:50010
2015-10-25 17:54:00,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58827, dest: /192.168.54.130:50010, bytes: 9222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742179_1358, duration: 5159075
2015-10-25 17:54:00,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742179_1358, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742180_1359 src: /192.168.54.130:58828 dest: /192.168.54.130:50010
2015-10-25 17:54:00,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58828, dest: /192.168.54.130:50010, bytes: 6459, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742180_1359, duration: 1692015
2015-10-25 17:54:00,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742180_1359, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742181_1360 src: /192.168.54.130:58829 dest: /192.168.54.130:50010
2015-10-25 17:54:00,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58829, dest: /192.168.54.130:50010, bytes: 9811, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742181_1360, duration: 1516207
2015-10-25 17:54:00,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742181_1360, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742182_1361 src: /192.168.54.130:58830 dest: /192.168.54.130:50010
2015-10-25 17:54:00,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58830, dest: /192.168.54.130:50010, bytes: 19000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742182_1361, duration: 2210700
2015-10-25 17:54:00,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742182_1361, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742183_1362 src: /192.168.54.130:58831 dest: /192.168.54.130:50010
2015-10-25 17:54:00,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58831, dest: /192.168.54.130:50010, bytes: 7279, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742183_1362, duration: 4985085
2015-10-25 17:54:00,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742183_1362, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742184_1363 src: /192.168.54.130:58832 dest: /192.168.54.130:50010
2015-10-25 17:54:00,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58832, dest: /192.168.54.130:50010, bytes: 29112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742184_1363, duration: 2097800
2015-10-25 17:54:00,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742184_1363, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742185_1364 src: /192.168.54.130:58833 dest: /192.168.54.130:50010
2015-10-25 17:54:00,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58833, dest: /192.168.54.130:50010, bytes: 10673, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742185_1364, duration: 1410625
2015-10-25 17:54:00,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742185_1364, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742186_1365 src: /192.168.54.130:58834 dest: /192.168.54.130:50010
2015-10-25 17:54:00,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58834, dest: /192.168.54.130:50010, bytes: 9176, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742186_1365, duration: 1246744
2015-10-25 17:54:00,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742186_1365, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742187_1366 src: /192.168.54.130:58835 dest: /192.168.54.130:50010
2015-10-25 17:54:00,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58835, dest: /192.168.54.130:50010, bytes: 11230, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742187_1366, duration: 1861794
2015-10-25 17:54:00,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742187_1366, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742188_1367 src: /192.168.54.130:58836 dest: /192.168.54.130:50010
2015-10-25 17:54:00,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58836, dest: /192.168.54.130:50010, bytes: 9954, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742188_1367, duration: 1575292
2015-10-25 17:54:00,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742188_1367, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742189_1368 src: /192.168.54.130:58837 dest: /192.168.54.130:50010
2015-10-25 17:54:00,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58837, dest: /192.168.54.130:50010, bytes: 9328, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742189_1368, duration: 1189258
2015-10-25 17:54:00,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742189_1368, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742190_1369 src: /192.168.54.130:58838 dest: /192.168.54.130:50010
2015-10-25 17:54:00,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58838, dest: /192.168.54.130:50010, bytes: 10827, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742190_1369, duration: 1482116
2015-10-25 17:54:00,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742190_1369, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742191_1370 src: /192.168.54.130:58839 dest: /192.168.54.130:50010
2015-10-25 17:54:00,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58839, dest: /192.168.54.130:50010, bytes: 8573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742191_1370, duration: 1510432
2015-10-25 17:54:00,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742191_1370, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742192_1371 src: /192.168.54.130:58840 dest: /192.168.54.130:50010
2015-10-25 17:54:00,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58840, dest: /192.168.54.130:50010, bytes: 12751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742192_1371, duration: 1241012
2015-10-25 17:54:00,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742192_1371, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742193_1372 src: /192.168.54.130:58841 dest: /192.168.54.130:50010
2015-10-25 17:54:00,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58841, dest: /192.168.54.130:50010, bytes: 7216, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742193_1372, duration: 1049486
2015-10-25 17:54:00,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742193_1372, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742194_1373 src: /192.168.54.130:58842 dest: /192.168.54.130:50010
2015-10-25 17:54:00,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58842, dest: /192.168.54.130:50010, bytes: 8245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742194_1373, duration: 1049611
2015-10-25 17:54:00,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742194_1373, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742195_1374 src: /192.168.54.130:58843 dest: /192.168.54.130:50010
2015-10-25 17:54:00,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58843, dest: /192.168.54.130:50010, bytes: 13713, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742195_1374, duration: 1494281
2015-10-25 17:54:00,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742195_1374, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742196_1375 src: /192.168.54.130:58844 dest: /192.168.54.130:50010
2015-10-25 17:54:00,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58844, dest: /192.168.54.130:50010, bytes: 7074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742196_1375, duration: 1230417
2015-10-25 17:54:00,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742196_1375, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742197_1376 src: /192.168.54.130:58845 dest: /192.168.54.130:50010
2015-10-25 17:54:00,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58845, dest: /192.168.54.130:50010, bytes: 8670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742197_1376, duration: 1325152
2015-10-25 17:54:00,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742197_1376, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742198_1377 src: /192.168.54.130:58846 dest: /192.168.54.130:50010
2015-10-25 17:54:00,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58846, dest: /192.168.54.130:50010, bytes: 8483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742198_1377, duration: 1014637
2015-10-25 17:54:00,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742198_1377, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742199_1378 src: /192.168.54.130:58847 dest: /192.168.54.130:50010
2015-10-25 17:54:00,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58847, dest: /192.168.54.130:50010, bytes: 10347, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742199_1378, duration: 1211903
2015-10-25 17:54:00,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742199_1378, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742200_1379 src: /192.168.54.130:58848 dest: /192.168.54.130:50010
2015-10-25 17:54:00,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58848, dest: /192.168.54.130:50010, bytes: 12018, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742200_1379, duration: 1459928
2015-10-25 17:54:00,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742200_1379, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:00,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742201_1380 src: /192.168.54.130:58849 dest: /192.168.54.130:50010
2015-10-25 17:54:00,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58849, dest: /192.168.54.130:50010, bytes: 16999, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742201_1380, duration: 1522560
2015-10-25 17:54:00,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742201_1380, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742202_1381 src: /192.168.54.130:58850 dest: /192.168.54.130:50010
2015-10-25 17:54:01,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58850, dest: /192.168.54.130:50010, bytes: 11799, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742202_1381, duration: 1161086
2015-10-25 17:54:01,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742202_1381, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742203_1382 src: /192.168.54.130:58851 dest: /192.168.54.130:50010
2015-10-25 17:54:01,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58851, dest: /192.168.54.130:50010, bytes: 7797, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742203_1382, duration: 1066718
2015-10-25 17:54:01,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742203_1382, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742204_1383 src: /192.168.54.130:58852 dest: /192.168.54.130:50010
2015-10-25 17:54:01,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58852, dest: /192.168.54.130:50010, bytes: 5937, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742204_1383, duration: 1313391
2015-10-25 17:54:01,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742204_1383, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742205_1384 src: /192.168.54.130:58853 dest: /192.168.54.130:50010
2015-10-25 17:54:01,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58853, dest: /192.168.54.130:50010, bytes: 8751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742205_1384, duration: 1472865
2015-10-25 17:54:01,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742205_1384, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742206_1385 src: /192.168.54.130:58854 dest: /192.168.54.130:50010
2015-10-25 17:54:01,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58854, dest: /192.168.54.130:50010, bytes: 9168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742206_1385, duration: 1077975
2015-10-25 17:54:01,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742206_1385, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742207_1386 src: /192.168.54.130:58855 dest: /192.168.54.130:50010
2015-10-25 17:54:01,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58855, dest: /192.168.54.130:50010, bytes: 13006, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742207_1386, duration: 1069151
2015-10-25 17:54:01,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742207_1386, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742208_1387 src: /192.168.54.130:58856 dest: /192.168.54.130:50010
2015-10-25 17:54:01,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58856, dest: /192.168.54.130:50010, bytes: 10662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742208_1387, duration: 1233790
2015-10-25 17:54:01,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742208_1387, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742209_1388 src: /192.168.54.130:58857 dest: /192.168.54.130:50010
2015-10-25 17:54:01,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58857, dest: /192.168.54.130:50010, bytes: 27491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742209_1388, duration: 461423
2015-10-25 17:54:01,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742209_1388, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742210_1389 src: /192.168.54.130:58858 dest: /192.168.54.130:50010
2015-10-25 17:54:01,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58858, dest: /192.168.54.130:50010, bytes: 13387, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742210_1389, duration: 1139543
2015-10-25 17:54:01,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742210_1389, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742211_1390 src: /192.168.54.130:58859 dest: /192.168.54.130:50010
2015-10-25 17:54:01,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58859, dest: /192.168.54.130:50010, bytes: 10094, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742211_1390, duration: 1099166
2015-10-25 17:54:01,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742211_1390, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742212_1391 src: /192.168.54.130:58860 dest: /192.168.54.130:50010
2015-10-25 17:54:01,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58860, dest: /192.168.54.130:50010, bytes: 16889, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742212_1391, duration: 1085295
2015-10-25 17:54:01,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742212_1391, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742213_1392 src: /192.168.54.130:58861 dest: /192.168.54.130:50010
2015-10-25 17:54:01,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58861, dest: /192.168.54.130:50010, bytes: 22332, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742213_1392, duration: 1348966
2015-10-25 17:54:01,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742213_1392, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742214_1393 src: /192.168.54.130:58862 dest: /192.168.54.130:50010
2015-10-25 17:54:01,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58862, dest: /192.168.54.130:50010, bytes: 12552, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742214_1393, duration: 1851820
2015-10-25 17:54:01,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742214_1393, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742215_1394 src: /192.168.54.130:58863 dest: /192.168.54.130:50010
2015-10-25 17:54:01,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58863, dest: /192.168.54.130:50010, bytes: 12248, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742215_1394, duration: 1031028
2015-10-25 17:54:01,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742215_1394, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742216_1395 src: /192.168.54.130:58864 dest: /192.168.54.130:50010
2015-10-25 17:54:01,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58864, dest: /192.168.54.130:50010, bytes: 12497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742216_1395, duration: 1163054
2015-10-25 17:54:01,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742216_1395, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742217_1396 src: /192.168.54.130:58865 dest: /192.168.54.130:50010
2015-10-25 17:54:01,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58865, dest: /192.168.54.130:50010, bytes: 11243, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742217_1396, duration: 846173
2015-10-25 17:54:01,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742217_1396, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742218_1397 src: /192.168.54.130:58866 dest: /192.168.54.130:50010
2015-10-25 17:54:01,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58866, dest: /192.168.54.130:50010, bytes: 16578, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742218_1397, duration: 1031965
2015-10-25 17:54:01,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742218_1397, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742219_1398 src: /192.168.54.130:58867 dest: /192.168.54.130:50010
2015-10-25 17:54:01,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58867, dest: /192.168.54.130:50010, bytes: 16110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742219_1398, duration: 1006193
2015-10-25 17:54:01,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742219_1398, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742220_1399 src: /192.168.54.130:58868 dest: /192.168.54.130:50010
2015-10-25 17:54:01,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58868, dest: /192.168.54.130:50010, bytes: 8866, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742220_1399, duration: 955903
2015-10-25 17:54:01,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742220_1399, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742221_1400 src: /192.168.54.130:58869 dest: /192.168.54.130:50010
2015-10-25 17:54:01,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58869, dest: /192.168.54.130:50010, bytes: 12994, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742221_1400, duration: 1070100
2015-10-25 17:54:01,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742221_1400, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742222_1401 src: /192.168.54.130:58870 dest: /192.168.54.130:50010
2015-10-25 17:54:01,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58870, dest: /192.168.54.130:50010, bytes: 15319, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742222_1401, duration: 5028274
2015-10-25 17:54:01,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742222_1401, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742223_1402 src: /192.168.54.130:58871 dest: /192.168.54.130:50010
2015-10-25 17:54:01,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58871, dest: /192.168.54.130:50010, bytes: 9408, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742223_1402, duration: 1233809
2015-10-25 17:54:01,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742223_1402, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742224_1403 src: /192.168.54.130:58872 dest: /192.168.54.130:50010
2015-10-25 17:54:01,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58872, dest: /192.168.54.130:50010, bytes: 13657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742224_1403, duration: 1341677
2015-10-25 17:54:01,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742224_1403, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742225_1404 src: /192.168.54.130:58873 dest: /192.168.54.130:50010
2015-10-25 17:54:01,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58873, dest: /192.168.54.130:50010, bytes: 13575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742225_1404, duration: 1353259
2015-10-25 17:54:01,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742225_1404, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742226_1405 src: /192.168.54.130:58874 dest: /192.168.54.130:50010
2015-10-25 17:54:01,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58874, dest: /192.168.54.130:50010, bytes: 8722, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742226_1405, duration: 1196618
2015-10-25 17:54:01,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742226_1405, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:54:01,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742227_1406 src: /192.168.54.130:58875 dest: /192.168.54.130:50010
2015-10-25 17:54:01,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58875, dest: /192.168.54.130:50010, bytes: 25882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1347125287_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742227_1406, duration: 1126211
2015-10-25 17:54:01,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742227_1406, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:55:11,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742162_1338 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742162 for deletion
2015-10-25 17:55:11,544 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742162_1338 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742162
2015-10-25 17:55:32,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742228_1407 src: /192.168.54.130:58881 dest: /192.168.54.130:50010
2015-10-25 17:55:32,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58881, dest: /192.168.54.130:50010, bytes: 4492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1706583607_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742228_1407, duration: 67085703
2015-10-25 17:55:32,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742228_1407, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:55:32,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742229_1408 src: /192.168.54.130:58882 dest: /192.168.54.130:50010
2015-10-25 17:55:32,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58882, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1706583607_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742229_1408, duration: 7155325
2015-10-25 17:55:32,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742229_1408, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 17:55:32,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742230_1409 src: /192.168.54.130:58884 dest: /192.168.54.130:50010
2015-10-25 17:55:32,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58884, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1706583607_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742230_1409, duration: 1000403
2015-10-25 17:55:32,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742230_1409, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:55:32,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742231_1410 src: /192.168.54.130:58885 dest: /192.168.54.130:50010
2015-10-25 17:55:32,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58885, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1706583607_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742231_1410, duration: 13519499
2015-10-25 17:55:32,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742231_1410, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:55:35,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742228_1407 to 192.168.54.131:50010 
2015-10-25 17:55:35,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742228_1407 (numBytes=4492) to /192.168.54.131:50010
2015-10-25 17:55:41,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742232_1411 src: /192.168.54.130:58894 dest: /192.168.54.130:50010
2015-10-25 17:55:41,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:58894, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-610659069_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742232_1411, duration: 61362472
2015-10-25 17:55:41,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742232_1411, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:56:03,349 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1538ms
No GCs detected
2015-10-25 17:56:03,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742233_1412 src: /192.168.54.130:58909 dest: /192.168.54.130:50010
2015-10-25 17:59:26,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-25 17:59:26,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-25 17:59:27,618 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-10-25 17:59:28,223 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-25 17:59:28,412 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-25 17:59:28,412 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-25 17:59:28,424 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-25 17:59:28,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-10-25 17:59:28,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-25 17:59:28,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-25 17:59:28,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-25 17:59:28,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-25 17:59:28,630 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-25 17:59:28,646 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-25 17:59:28,664 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-25 17:59:28,678 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-25 17:59:28,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-25 17:59:28,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-25 17:59:28,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-25 17:59:28,706 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41207
2015-10-25 17:59:28,706 INFO org.mortbay.log: jetty-6.1.26
2015-10-25 17:59:29,024 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41207
2015-10-25 17:59:29,198 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-25 17:59:29,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-10-25 17:59:29,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-25 17:59:29,306 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-25 17:59:29,346 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-25 17:59:29,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-25 17:59:29,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-25 17:59:29,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-25 17:59:29,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-10-25 17:59:29,578 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-25 17:59:29,580 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-25 17:59:29,899 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 11508@hadoopmaster
2015-10-25 17:59:30,020 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1892222280-192.168.54.130-1445296806381
2015-10-25 17:59:30,020 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381
2015-10-25 17:59:30,021 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-25 17:59:30,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=693764022;bpid=BP-1892222280-192.168.54.130-1445296806381;lv=-56;nsInfo=lv=-63;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0;bpid=BP-1892222280-192.168.54.130-1445296806381;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-10-25 17:59:30,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-10-25 17:59:30,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-10-25 17:59:30,163 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-25 17:59:30,164 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 17:59:30,164 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-25 17:59:30,190 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1892222280-192.168.54.130-1445296806381 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 26ms
2015-10-25 17:59:30,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1892222280-192.168.54.130-1445296806381: 26ms
2015-10-25 17:59:30,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-25 17:59:30,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 33ms
2015-10-25 17:59:30,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-10-25 17:59:30,483 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1290949022 ms.
2015-10-25 17:59:30,486 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445841748486 with interval 21600000
2015-10-25 17:59:30,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-25 17:59:30,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-25 17:59:30,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-25 17:59:30,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=4554
2015-10-25 17:59:30,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-10-25 17:59:30,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742228_1407 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742228 for deletion
2015-10-25 17:59:30,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742229_1408 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742229 for deletion
2015-10-25 17:59:30,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742230_1409 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742230 for deletion
2015-10-25 17:59:30,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742231_1410 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742231 for deletion
2015-10-25 17:59:30,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742232_1411 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742232 for deletion
2015-10-25 17:59:30,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742228_1407 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742228
2015-10-25 17:59:30,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742229_1408 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742229
2015-10-25 17:59:30,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742230_1409 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742230
2015-10-25 17:59:30,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742231_1410 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742231
2015-10-25 17:59:30,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742232_1411 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742232
2015-10-25 17:59:30,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x20e988ee88bd,  containing 1 storage report(s), of which we sent 1. The reports had 109 total blocks and used 1 RPC(s). This took 8 msec to generate and 68 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 17:59:30,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 17:59:49,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742244_1423 src: /192.168.54.130:59078 dest: /192.168.54.130:50010
2015-10-25 17:59:49,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59078, dest: /192.168.54.130:50010, bytes: 4492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-772081911_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742244_1423, duration: 78313276
2015-10-25 17:59:49,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742244_1423, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:59:49,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742245_1424 src: /192.168.54.130:59079 dest: /192.168.54.130:50010
2015-10-25 17:59:49,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59079, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-772081911_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742245_1424, duration: 7393329
2015-10-25 17:59:49,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742245_1424, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 17:59:49,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742246_1425 src: /192.168.54.130:59081 dest: /192.168.54.130:50010
2015-10-25 17:59:49,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59081, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-772081911_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742246_1425, duration: 634875
2015-10-25 17:59:49,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742246_1425, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:59:49,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742247_1426 src: /192.168.54.130:59082 dest: /192.168.54.130:50010
2015-10-25 17:59:49,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59082, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-772081911_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742247_1426, duration: 6744513
2015-10-25 17:59:49,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742247_1426, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 17:59:50,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742244_1423 to 192.168.54.131:50010 
2015-10-25 17:59:50,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742244_1423 (numBytes=4492) to /192.168.54.131:50010
2015-10-25 18:13:35,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742244_1423 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742244 for deletion
2015-10-25 18:13:35,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742245_1424 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742245 for deletion
2015-10-25 18:13:35,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742246_1425 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742246 for deletion
2015-10-25 18:13:35,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742247_1426 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742247 for deletion
2015-10-25 18:13:35,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742244_1423 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742244
2015-10-25 18:13:35,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742245_1424 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742245
2015-10-25 18:13:35,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742246_1425 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742246
2015-10-25 18:13:35,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742247_1426 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742247
2015-10-25 20:48:41,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2a24fc65c1dc,  containing 1 storage report(s), of which we sent 1. The reports had 109 total blocks and used 1 RPC(s). This took 2 msec to generate and 14 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 20:48:41,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 20:48:44,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742233_1412 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/rbw/blk_1073742233 for deletion
2015-10-25 20:48:44,574 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742233_1412 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/rbw/blk_1073742233
2015-10-25 21:16:10,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742254_1433 src: /192.168.54.130:59359 dest: /192.168.54.130:50010
2015-10-25 21:16:10,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59359, dest: /192.168.54.130:50010, bytes: 4542, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1465711818_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742254_1433, duration: 32323466
2015-10-25 21:16:10,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742254_1433, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:16:10,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742255_1434 src: /192.168.54.130:59360 dest: /192.168.54.130:50010
2015-10-25 21:16:10,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59360, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1465711818_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742255_1434, duration: 6281821
2015-10-25 21:16:10,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742255_1434, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 21:16:10,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742256_1435 src: /192.168.54.130:59362 dest: /192.168.54.130:50010
2015-10-25 21:16:10,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59362, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1465711818_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742256_1435, duration: 701985
2015-10-25 21:16:10,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742256_1435, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:16:10,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742257_1436 src: /192.168.54.130:59363 dest: /192.168.54.130:50010
2015-10-25 21:16:10,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59363, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1465711818_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742257_1436, duration: 10639793
2015-10-25 21:16:10,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742257_1436, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:16:11,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742254_1433 to 192.168.54.131:50010 
2015-10-25 21:16:11,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742254_1433 (numBytes=4542) to /192.168.54.131:50010
2015-10-25 21:16:16,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742258_1437 src: /192.168.54.130:59372 dest: /192.168.54.130:50010
2015-10-25 21:16:16,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59372, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-368258117_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742258_1437, duration: 32535614
2015-10-25 21:16:16,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742258_1437, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:16:26,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742259_1438 src: /192.168.54.130:59384 dest: /192.168.54.130:50010
2015-10-25 21:18:17,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742260_1439 src: /192.168.54.130:59636 dest: /192.168.54.130:50010
2015-10-25 21:18:18,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59636, dest: /192.168.54.130:50010, bytes: 555700, op: HDFS_WRITE, cliID: DFSClient_attempt_1445813912906_0011_r_000000_0_-112072038_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742260_1439, duration: 924072833
2015-10-25 21:18:18,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742260_1439, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:18:19,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59384, dest: /192.168.54.130:50010, bytes: 433519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-368258117_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742259_1438, duration: 112783269215
2015-10-25 21:18:19,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742259_1438, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:18:19,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742261_1440 src: /192.168.54.130:59639 dest: /192.168.54.130:50010
2015-10-25 21:18:19,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59639, dest: /192.168.54.130:50010, bytes: 357, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-368258117_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742261_1440, duration: 1373041
2015-10-25 21:18:19,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742261_1440, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:18:19,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742262_1441 src: /192.168.54.130:59641 dest: /192.168.54.130:50010
2015-10-25 21:18:19,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59641, dest: /192.168.54.130:50010, bytes: 433519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-368258117_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742262_1441, duration: 22974390
2015-10-25 21:18:19,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742262_1441, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:18:19,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742263_1442 src: /192.168.54.130:59642 dest: /192.168.54.130:50010
2015-10-25 21:18:19,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59642, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-368258117_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742263_1442, duration: 5219941
2015-10-25 21:18:19,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742263_1442, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:18:23,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742256_1435 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742256 for deletion
2015-10-25 21:18:23,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742257_1436 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742257 for deletion
2015-10-25 21:18:23,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742258_1437 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742258 for deletion
2015-10-25 21:18:23,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742259_1438 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742259 for deletion
2015-10-25 21:18:23,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742254_1433 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742254 for deletion
2015-10-25 21:18:23,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742255_1434 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742255 for deletion
2015-10-25 21:18:23,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742256_1435 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742256
2015-10-25 21:18:23,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742257_1436 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742257
2015-10-25 21:18:23,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742258_1437 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742258
2015-10-25 21:18:23,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742259_1438 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742259
2015-10-25 21:18:23,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742254_1433 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742254
2015-10-25 21:18:23,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742255_1434 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742255
2015-10-25 21:23:59,558 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742260_1439 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742260 for deletion
2015-10-25 21:23:59,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742260_1439 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742260
2015-10-25 21:24:19,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742264_1443 src: /192.168.54.130:59657 dest: /192.168.54.130:50010
2015-10-25 21:24:19,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59657, dest: /192.168.54.130:50010, bytes: 4564, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1145575553_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742264_1443, duration: 41522311
2015-10-25 21:24:19,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742264_1443, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:24:19,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742265_1444 src: /192.168.54.130:59658 dest: /192.168.54.130:50010
2015-10-25 21:24:19,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59658, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1145575553_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742265_1444, duration: 8336654
2015-10-25 21:24:19,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742265_1444, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 21:24:19,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742266_1445 src: /192.168.54.130:59660 dest: /192.168.54.130:50010
2015-10-25 21:24:19,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59660, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1145575553_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742266_1445, duration: 983140
2015-10-25 21:24:19,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742266_1445, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:24:19,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742267_1446 src: /192.168.54.130:59661 dest: /192.168.54.130:50010
2015-10-25 21:24:19,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59661, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1145575553_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742267_1446, duration: 18144141
2015-10-25 21:24:19,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742267_1446, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:24:23,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742268_1447 src: /192.168.54.130:59669 dest: /192.168.54.130:50010
2015-10-25 21:24:23,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59669, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1613284368_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742268_1447, duration: 29740632
2015-10-25 21:24:23,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742268_1447, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:24:23,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742264_1443 to 192.168.54.131:50010 
2015-10-25 21:24:23,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742264_1443 (numBytes=4564) to /192.168.54.131:50010
2015-10-25 21:24:35,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742269_1448 src: /192.168.54.130:59690 dest: /192.168.54.130:50010
2015-10-25 21:27:14,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742270_1449 src: /192.168.54.130:59949 dest: /192.168.54.130:50010
2015-10-25 21:28:27,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59949, dest: /192.168.54.130:50010, bytes: 29430593, op: HDFS_WRITE, cliID: DFSClient_attempt_1445813912906_0012_r_000000_1_1075829794_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742270_1449, duration: 73493294753
2015-10-25 21:28:27,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742270_1449, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:28:27,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59690, dest: /192.168.54.130:50010, bytes: 442904, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1613284368_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742269_1448, duration: 231860163950
2015-10-25 21:28:27,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742269_1448, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:28:27,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742271_1450 src: /192.168.54.130:59958 dest: /192.168.54.130:50010
2015-10-25 21:28:27,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59958, dest: /192.168.54.130:50010, bytes: 358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1613284368_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742271_1450, duration: 1674696
2015-10-25 21:28:27,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742271_1450, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:28:27,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742272_1451 src: /192.168.54.130:59960 dest: /192.168.54.130:50010
2015-10-25 21:28:27,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59960, dest: /192.168.54.130:50010, bytes: 442904, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1613284368_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742272_1451, duration: 1815108
2015-10-25 21:28:27,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742272_1451, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:28:27,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742273_1452 src: /192.168.54.130:59961 dest: /192.168.54.130:50010
2015-10-25 21:28:27,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59961, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1613284368_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742273_1452, duration: 3432297
2015-10-25 21:28:27,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742273_1452, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:28:35,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742264_1443 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742264 for deletion
2015-10-25 21:28:35,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742265_1444 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742265 for deletion
2015-10-25 21:28:35,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742266_1445 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742266 for deletion
2015-10-25 21:28:35,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742267_1446 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742267 for deletion
2015-10-25 21:28:35,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742268_1447 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742268 for deletion
2015-10-25 21:28:35,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742269_1448 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742269 for deletion
2015-10-25 21:28:35,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742264_1443 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742264
2015-10-25 21:28:35,565 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742265_1444 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742265
2015-10-25 21:28:35,565 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742266_1445 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742266
2015-10-25 21:28:35,565 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742267_1446 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742267
2015-10-25 21:28:35,565 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742268_1447 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742268
2015-10-25 21:28:35,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742269_1448 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742269
2015-10-25 21:45:47,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742270_1449 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742270 for deletion
2015-10-25 21:45:47,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742270_1449 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742270
2015-10-25 21:47:15,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742274_1453 src: /192.168.54.130:59987 dest: /192.168.54.130:50010
2015-10-25 21:47:15,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59987, dest: /192.168.54.130:50010, bytes: 4605, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1450362601_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742274_1453, duration: 28433239
2015-10-25 21:47:15,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742274_1453, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:47:15,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742275_1454 src: /192.168.54.130:59988 dest: /192.168.54.130:50010
2015-10-25 21:47:15,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59988, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1450362601_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742275_1454, duration: 11633916
2015-10-25 21:47:15,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742275_1454, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 21:47:15,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742276_1455 src: /192.168.54.130:59990 dest: /192.168.54.130:50010
2015-10-25 21:47:15,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59990, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1450362601_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742276_1455, duration: 2766408
2015-10-25 21:47:15,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742276_1455, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:47:15,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742277_1456 src: /192.168.54.130:59991 dest: /192.168.54.130:50010
2015-10-25 21:47:15,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:59991, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1450362601_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742277_1456, duration: 9843965
2015-10-25 21:47:15,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742277_1456, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:47:17,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742274_1453 to 192.168.54.131:50010 
2015-10-25 21:47:17,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742274_1453 (numBytes=4605) to /192.168.54.131:50010
2015-10-25 21:47:20,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742278_1457 src: /192.168.54.130:60000 dest: /192.168.54.130:50010
2015-10-25 21:47:20,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60000, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1114699909_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742278_1457, duration: 39263472
2015-10-25 21:47:20,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742278_1457, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:47:30,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742279_1458 src: /192.168.54.130:60012 dest: /192.168.54.130:50010
2015-10-25 21:47:54,995 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-25 21:47:57,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:47:58,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:47:59,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:00,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:01,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:02,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:03,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:04,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:05,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:06,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:06,310 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:48:07,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:08,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:09,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:10,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:11,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:12,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:13,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:14,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:15,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:16,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:16,372 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:48:17,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:18,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:19,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:20,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:21,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:22,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:23,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:24,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:25,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:26,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:26,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:48:27,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:28,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:29,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:30,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:31,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:32,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:33,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:34,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:35,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:36,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:36,497 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:48:37,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:38,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:39,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:40,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:41,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:42,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:43,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:44,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:45,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:46,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:46,511 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:48:47,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:47,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60012, dest: /192.168.54.130:50010, bytes: 356754, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1114699909_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742279_1458, duration: 76847791891
2015-10-25 21:48:47,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742279_1458, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:48:48,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:49,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:50,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:51,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:52,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:53,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:54,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:55,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:56,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:56,531 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:48:57,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:58,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:48:59,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:00,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:01,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:02,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:03,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:04,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:05,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:06,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:06,549 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:49:07,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:08,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:09,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:10,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:11,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:12,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:13,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:14,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:15,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:16,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:16,570 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:49:17,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:18,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:19,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:20,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:21,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:22,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:23,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:24,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:25,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:26,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:26,590 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:49:27,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:28,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:29,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:30,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:31,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:32,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:33,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:34,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:35,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:36,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:36,610 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:49:37,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:38,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:39,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:40,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:41,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:42,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:43,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:44,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:45,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:46,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:46,628 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:49:47,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:48,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:49,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:50,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:51,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:52,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:53,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:54,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:55,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:56,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:56,646 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:49:57,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:58,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:49:59,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:00,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:01,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:02,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:03,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:04,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:05,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:06,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:06,667 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:50:07,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:08,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:09,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:10,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:11,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:50:11,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoopmaster/192.168.54.130:9000 with active state
2015-10-25 21:50:11,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-25 21:50:11,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-25 21:50:11,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2d80352be7f2,  containing 1 storage report(s), of which we sent 1. The reports had 120 total blocks and used 1 RPC(s). This took 2 msec to generate and 10 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 21:50:11,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 21:51:48,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742280_1459 src: /192.168.54.130:60526 dest: /192.168.54.130:50010
2015-10-25 21:51:48,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60526, dest: /192.168.54.130:50010, bytes: 4605, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-5503169_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742280_1459, duration: 35412564
2015-10-25 21:51:48,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742280_1459, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:51:49,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742281_1460 src: /192.168.54.130:60527 dest: /192.168.54.130:50010
2015-10-25 21:51:49,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60527, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-5503169_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742281_1460, duration: 7396692
2015-10-25 21:51:49,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742281_1460, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 21:51:49,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742282_1461 src: /192.168.54.130:60529 dest: /192.168.54.130:50010
2015-10-25 21:51:49,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60529, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-5503169_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742282_1461, duration: 1399197
2015-10-25 21:51:49,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742282_1461, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:51:49,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742283_1462 src: /192.168.54.130:60530 dest: /192.168.54.130:50010
2015-10-25 21:51:49,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60530, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-5503169_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742283_1462, duration: 10901607
2015-10-25 21:51:49,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742283_1462, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:51:50,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742280_1459 to 192.168.54.131:50010 
2015-10-25 21:51:50,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742280_1459 (numBytes=4605) to /192.168.54.131:50010
2015-10-25 21:51:54,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742284_1463 src: /192.168.54.130:60539 dest: /192.168.54.130:50010
2015-10-25 21:51:54,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60539, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_348807475_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742284_1463, duration: 28459167
2015-10-25 21:51:54,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742284_1463, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:52:08,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742285_1464 src: /192.168.54.130:60561 dest: /192.168.54.130:50010
2015-10-25 21:53:33,372 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-25 21:53:34,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:35,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:36,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:37,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:38,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:39,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:40,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:41,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:42,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:43,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:43,529 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:53:44,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:45,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:46,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:47,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:48,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:49,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:50,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:51,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:52,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:53,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:53,565 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:53:54,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:54,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:60561, dest: /192.168.54.130:50010, bytes: 352047, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_348807475_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742285_1464, duration: 106409165005
2015-10-25 21:53:54,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742285_1464, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:53:55,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:56,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:57,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:58,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:53:59,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:00,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:01,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:02,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:03,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:03,592 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:54:04,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:05,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:06,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:07,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:08,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:09,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:10,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:11,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:12,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:13,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:13,607 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:54:14,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:15,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:16,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:17,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:18,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:19,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:20,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:21,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:22,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:23,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:23,625 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:54:24,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:25,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:26,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:27,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:28,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:29,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:30,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:31,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:32,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:33,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:33,651 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:54:34,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:35,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:36,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:37,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:38,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:39,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:40,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:41,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:42,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:43,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:43,665 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:54:44,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:45,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:46,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:47,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:48,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:49,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:50,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:51,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:52,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:53,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:53,680 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:54:54,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:55,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:56,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:57,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:58,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:54:59,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:00,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:01,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:02,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:03,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:03,697 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:55:04,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:05,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:06,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:07,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:08,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:09,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:10,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:11,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:12,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:13,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:13,715 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:55:14,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:15,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:16,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:17,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:18,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:19,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:20,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:21,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:22,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:55:22,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from hadoopmaster/192.168.54.130:9000 with active state
2015-10-25 21:55:22,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-25 21:55:22,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-25 21:55:22,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2dc89736febd,  containing 1 storage report(s), of which we sent 1. The reports had 126 total blocks and used 1 RPC(s). This took 3 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 21:55:22,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 21:56:01,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742286_1465 src: /192.168.54.130:32772 dest: /192.168.54.130:50010
2015-10-25 21:56:01,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:32772, dest: /192.168.54.130:50010, bytes: 4605, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1184800377_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742286_1465, duration: 33436352
2015-10-25 21:56:01,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742286_1465, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:56:01,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742287_1466 src: /192.168.54.130:32773 dest: /192.168.54.130:50010
2015-10-25 21:56:01,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:32773, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1184800377_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742287_1466, duration: 12865922
2015-10-25 21:56:01,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742287_1466, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 21:56:01,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742288_1467 src: /192.168.54.130:32775 dest: /192.168.54.130:50010
2015-10-25 21:56:01,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:32775, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1184800377_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742288_1467, duration: 1298017
2015-10-25 21:56:01,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742288_1467, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:56:01,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742289_1468 src: /192.168.54.130:32776 dest: /192.168.54.130:50010
2015-10-25 21:56:01,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:32776, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1184800377_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742289_1468, duration: 11719647
2015-10-25 21:56:01,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742289_1468, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 21:56:04,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742286_1465 to 192.168.54.131:50010 
2015-10-25 21:56:04,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742286_1465 (numBytes=4605) to /192.168.54.131:50010
2015-10-25 21:56:40,439 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-25 21:56:44,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:45,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:46,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:47,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:48,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:49,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:50,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:51,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:52,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:53,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:53,256 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:56:54,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:55,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:56,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:57,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:58,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:56:59,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:00,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:01,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:02,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:03,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:03,291 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:57:04,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:05,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:06,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:07,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:08,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:09,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:10,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:11,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:12,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:13,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:13,374 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:57:14,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:15,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:16,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:17,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:18,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:19,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:20,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:21,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:22,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:23,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:23,426 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:57:24,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:25,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:26,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:27,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:28,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:29,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:30,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:31,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:32,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:33,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:33,438 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:57:34,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:35,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:36,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:37,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:38,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:39,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:40,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:41,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:42,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:43,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:43,454 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:57:44,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:45,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:46,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:47,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:48,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:49,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:50,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:52,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:53,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:53,471 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:57:54,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:55,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:56,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:57,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:58,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:57:59,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:00,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:01,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:02,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:03,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:03,487 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:58:04,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:05,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:06,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:07,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:08,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:09,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:10,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:11,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:12,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:13,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:13,507 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:58:14,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:15,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:16,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:17,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:18,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:19,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:20,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:21,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:22,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:23,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:23,521 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:58:24,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:25,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:26,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:27,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:28,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:29,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:30,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:31,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:32,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:33,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:33,546 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:58:34,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:35,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:36,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:37,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:38,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:39,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:40,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:41,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:42,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:43,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:43,561 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:58:44,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:45,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:46,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:47,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:48,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:49,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:50,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:51,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:52,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:53,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:53,578 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:58:54,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:55,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:56,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:57,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:58,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:58:59,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:00,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:01,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:02,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:03,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:03,596 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:59:04,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:05,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:06,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:07,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:08,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:09,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:10,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:11,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:12,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:13,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:13,610 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:59:14,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:15,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:16,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:17,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:18,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:19,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:20,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:21,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:22,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:23,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:23,629 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:59:24,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:25,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:26,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:27,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:28,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:29,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:30,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:31,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:32,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:33,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:33,651 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:59:34,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:35,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:36,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:37,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:38,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:39,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:40,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:41,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:42,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:43,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:43,666 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:59:44,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:45,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:46,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:47,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:48,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:49,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:50,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:51,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:52,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:53,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:53,682 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 21:59:54,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:55,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:56,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:57,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:58,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 21:59:59,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:00,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:01,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:02,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:03,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:03,698 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:00:04,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:05,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:06,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:07,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:08,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:09,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:10,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:11,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:12,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:13,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:13,717 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:00:14,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:15,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:16,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:17,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:18,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:19,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:20,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:21,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:22,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:23,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:23,736 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:00:24,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:25,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:26,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:27,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:28,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:29,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:30,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:31,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:32,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:33,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:33,765 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:00:34,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:35,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:36,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:37,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:38,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:39,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:40,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:41,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:42,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:43,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:43,782 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:00:44,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:45,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:46,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:47,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:48,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:49,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:50,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:51,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:52,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:53,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:53,808 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:00:54,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:55,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:56,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:57,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:58,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:00:59,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:00,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:01,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:02,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:03,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:03,821 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:01:04,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:05,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:40,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:41,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:43,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:44,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:45,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:46,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:47,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:47,474 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-10-25 22:01:48,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:49,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:50,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:51,444 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-25 22:01:51,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-25 22:01:51,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-10-25 22:08:56,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-25 22:08:56,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-25 22:08:56,923 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-10-25 22:08:57,443 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-25 22:08:57,569 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-25 22:08:57,570 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-25 22:08:57,594 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-25 22:08:57,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-10-25 22:08:57,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-25 22:08:57,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-25 22:08:57,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-25 22:08:57,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-25 22:08:57,909 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-25 22:08:57,934 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-25 22:08:57,954 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-25 22:08:57,970 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-25 22:08:57,983 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-25 22:08:57,984 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-25 22:08:57,984 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-25 22:08:58,015 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49491
2015-10-25 22:08:58,015 INFO org.mortbay.log: jetty-6.1.26
2015-10-25 22:08:58,267 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49491
2015-10-25 22:08:58,426 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-25 22:08:58,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-10-25 22:08:58,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-25 22:08:58,527 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-25 22:08:58,556 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-25 22:08:58,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-25 22:08:58,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-25 22:08:58,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-25 22:08:58,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-10-25 22:08:58,688 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-25 22:08:58,688 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-25 22:08:59,028 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 2810@hadoopmaster
2015-10-25 22:08:59,135 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1892222280-192.168.54.130-1445296806381
2015-10-25 22:08:59,135 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381
2015-10-25 22:08:59,137 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-25 22:08:59,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=693764022;bpid=BP-1892222280-192.168.54.130-1445296806381;lv=-56;nsInfo=lv=-63;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0;bpid=BP-1892222280-192.168.54.130-1445296806381;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-10-25 22:08:59,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-10-25 22:08:59,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-10-25 22:08:59,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-25 22:08:59,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 22:08:59,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-25 22:08:59,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current: 7081984
2015-10-25 22:08:59,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1892222280-192.168.54.130-1445296806381 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 16ms
2015-10-25 22:08:59,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1892222280-192.168.54.130-1445296806381: 17ms
2015-10-25 22:08:59,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-10-25 22:08:59,318 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 38ms
2015-10-25 22:08:59,318 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 39ms
2015-10-25 22:08:59,546 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1275979959 ms.
2015-10-25 22:08:59,548 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445852979548 with interval 21600000
2015-10-25 22:08:59,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-10-25 22:08:59,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-10-25 22:08:59,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-25 22:08:59,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=4998
2015-10-25 22:08:59,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-10-25 22:08:59,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2c732fb132,  containing 1 storage report(s), of which we sent 1. The reports had 130 total blocks and used 1 RPC(s). This took 7 msec to generate and 60 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-25 22:08:59,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-10-25 22:11:03,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742292_1471 src: /192.168.54.130:45951 dest: /192.168.54.130:50010
2015-10-25 22:11:03,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:45951, dest: /192.168.54.130:50010, bytes: 4605, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1527362099_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742292_1471, duration: 64887813
2015-10-25 22:11:03,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742292_1471, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:11:04,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742293_1472 src: /192.168.54.130:45952 dest: /192.168.54.130:50010
2015-10-25 22:11:04,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:45952, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1527362099_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742293_1472, duration: 41637012
2015-10-25 22:11:04,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742293_1472, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 22:11:04,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742294_1473 src: /192.168.54.130:45954 dest: /192.168.54.130:50010
2015-10-25 22:11:04,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:45954, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1527362099_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742294_1473, duration: 711595
2015-10-25 22:11:04,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742294_1473, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:11:04,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742295_1474 src: /192.168.54.130:45955 dest: /192.168.54.130:50010
2015-10-25 22:11:04,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:45955, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1527362099_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742295_1474, duration: 7409167
2015-10-25 22:11:04,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742295_1474, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:11:07,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742292_1471 to 192.168.54.131:50010 
2015-10-25 22:11:07,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742292_1471 (numBytes=4605) to /192.168.54.131:50010
2015-10-25 22:11:11,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742296_1475 src: /192.168.54.130:45965 dest: /192.168.54.130:50010
2015-10-25 22:11:11,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:45965, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_984444622_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742296_1475, duration: 43941386
2015-10-25 22:11:11,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742296_1475, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:11:36,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742297_1476 src: /192.168.54.130:46007 dest: /192.168.54.130:50010
2015-10-25 22:14:19,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46007, dest: /192.168.54.130:50010, bytes: 434960, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_984444622_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742297_1476, duration: 162098849355
2015-10-25 22:14:19,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742297_1476, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:14:19,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742299_1478 src: /192.168.54.130:46214 dest: /192.168.54.130:50010
2015-10-25 22:14:19,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46214, dest: /192.168.54.130:50010, bytes: 358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_984444622_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742299_1478, duration: 1115793
2015-10-25 22:14:19,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742299_1478, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:14:19,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742300_1479 src: /192.168.54.130:46216 dest: /192.168.54.130:50010
2015-10-25 22:14:19,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46216, dest: /192.168.54.130:50010, bytes: 434960, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_984444622_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742300_1479, duration: 5721097
2015-10-25 22:14:19,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742300_1479, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:14:19,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742301_1480 src: /192.168.54.130:46217 dest: /192.168.54.130:50010
2015-10-25 22:14:19,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46217, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_984444622_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742301_1480, duration: 920925
2015-10-25 22:14:19,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742301_1480, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 22:14:25,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742292_1471 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742292 for deletion
2015-10-25 22:14:25,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742293_1472 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742293 for deletion
2015-10-25 22:14:25,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742294_1473 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742294 for deletion
2015-10-25 22:14:25,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742295_1474 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742295 for deletion
2015-10-25 22:14:25,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742296_1475 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742296 for deletion
2015-10-25 22:14:25,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742297_1476 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742297 for deletion
2015-10-25 22:14:25,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742292_1471 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742292
2015-10-25 22:14:25,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742293_1472 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742293
2015-10-25 22:14:25,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742294_1473 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742294
2015-10-25 22:14:25,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742295_1474 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742295
2015-10-25 22:14:25,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742296_1475 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742296
2015-10-25 22:14:25,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742297_1476 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742297
2015-10-25 23:08:16,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: NameNode at hadoopmaster/192.168.54.130:9000 calls recoverBlock(BP-1892222280-192.168.54.130-1445296806381:blk_1073742279_1458, targets=[DatanodeInfoWithStorage[192.168.54.130:50010,null,null]], newGenerationStamp=1481)
2015-10-25 23:08:16,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: blk_1073742279_1458, recoveryId=1481, replica=FinalizedReplica, blk_1073742279_1458, FINALIZED
  getNumBytes()     = 356754
  getBytesOnDisk()  = 356754
  getVisibleLength()= 356754
  getVolume()       = /usr/local/hadoop-2.7.1/hdfs/datanode/current
  getBlockFile()    = /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742279
  unlinked          =false
2015-10-25 23:08:16,683 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for blk_1073742279_1458 from FINALIZED to RUR
2015-10-25 23:08:16,683 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: updateReplica: BP-1892222280-192.168.54.130-1445296806381:blk_1073742279_1458, recoveryId=1481, length=356754, replica=ReplicaUnderRecovery, blk_1073742279_1458, RUR
  getNumBytes()     = 356754
  getBytesOnDisk()  = 356754
  getVisibleLength()= 356754
  getVolume()       = /usr/local/hadoop-2.7.1/hdfs/datanode/current
  getBlockFile()    = /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742279
  recoveryId=1481
  original=FinalizedReplica, blk_1073742279_1458, FINALIZED
  getNumBytes()     = 356754
  getBytesOnDisk()  = 356754
  getVisibleLength()= 356754
  getVolume()       = /usr/local/hadoop-2.7.1/hdfs/datanode/current
  getBlockFile()    = /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742279
  unlinked          =false
2015-10-25 23:08:16,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: NameNode at hadoopmaster/192.168.54.130:9000 calls recoverBlock(BP-1892222280-192.168.54.130-1445296806381:blk_1073742285_1464, targets=[DatanodeInfoWithStorage[192.168.54.130:50010,null,null]], newGenerationStamp=1482)
2015-10-25 23:08:16,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: blk_1073742285_1464, recoveryId=1482, replica=FinalizedReplica, blk_1073742285_1464, FINALIZED
  getNumBytes()     = 352047
  getBytesOnDisk()  = 352047
  getVisibleLength()= 352047
  getVolume()       = /usr/local/hadoop-2.7.1/hdfs/datanode/current
  getBlockFile()    = /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742285
  unlinked          =false
2015-10-25 23:08:16,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for blk_1073742285_1464 from FINALIZED to RUR
2015-10-25 23:08:16,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: updateReplica: BP-1892222280-192.168.54.130-1445296806381:blk_1073742285_1464, recoveryId=1482, length=352047, replica=ReplicaUnderRecovery, blk_1073742285_1464, RUR
  getNumBytes()     = 352047
  getBytesOnDisk()  = 352047
  getVisibleLength()= 352047
  getVolume()       = /usr/local/hadoop-2.7.1/hdfs/datanode/current
  getBlockFile()    = /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742285
  recoveryId=1482
  original=FinalizedReplica, blk_1073742285_1464, FINALIZED
  getNumBytes()     = 352047
  getBytesOnDisk()  = 352047
  getVisibleLength()= 352047
  getVolume()       = /usr/local/hadoop-2.7.1/hdfs/datanode/current
  getBlockFile()    = /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742285
  unlinked          =false
2015-10-25 23:13:47,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742302_1484 src: /192.168.54.130:46287 dest: /192.168.54.130:50010
2015-10-25 23:13:47,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46287, dest: /192.168.54.130:50010, bytes: 4605, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541093736_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742302_1484, duration: 37206746
2015-10-25 23:13:47,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742302_1484, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:13:47,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742303_1485 src: /192.168.54.130:46288 dest: /192.168.54.130:50010
2015-10-25 23:13:47,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46288, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541093736_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742303_1485, duration: 9089820
2015-10-25 23:13:47,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742303_1485, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 23:13:47,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742304_1486 src: /192.168.54.130:46290 dest: /192.168.54.130:50010
2015-10-25 23:13:47,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46290, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541093736_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742304_1486, duration: 575246
2015-10-25 23:13:47,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742304_1486, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:13:48,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742305_1487 src: /192.168.54.130:46291 dest: /192.168.54.130:50010
2015-10-25 23:13:48,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46291, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541093736_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742305_1487, duration: 996864
2015-10-25 23:13:48,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742305_1487, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:13:49,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742302_1484 to 192.168.54.131:50010 
2015-10-25 23:13:49,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742302_1484 (numBytes=4605) to /192.168.54.131:50010
2015-10-25 23:13:51,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742306_1488 src: /192.168.54.130:46300 dest: /192.168.54.130:50010
2015-10-25 23:13:52,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46300, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_427046889_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742306_1488, duration: 80741828
2015-10-25 23:13:52,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742306_1488, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:14:02,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742307_1489 src: /192.168.54.130:46319 dest: /192.168.54.130:50010
2015-10-25 23:16:51,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46319, dest: /192.168.54.130:50010, bytes: 433394, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_427046889_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742307_1489, duration: 168582313506
2015-10-25 23:16:51,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742307_1489, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:16:51,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742309_1491 src: /192.168.54.130:46529 dest: /192.168.54.130:50010
2015-10-25 23:16:51,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46529, dest: /192.168.54.130:50010, bytes: 358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_427046889_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742309_1491, duration: 1722412
2015-10-25 23:16:51,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742309_1491, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:16:51,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742310_1492 src: /192.168.54.130:46531 dest: /192.168.54.130:50010
2015-10-25 23:16:51,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46531, dest: /192.168.54.130:50010, bytes: 433394, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_427046889_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742310_1492, duration: 9946350
2015-10-25 23:16:51,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742310_1492, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:16:51,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742311_1493 src: /192.168.54.130:46532 dest: /192.168.54.130:50010
2015-10-25 23:16:51,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46532, dest: /192.168.54.130:50010, bytes: 114022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_427046889_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742311_1493, duration: 2196785
2015-10-25 23:16:51,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742311_1493, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:16:55,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742304_1486 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742304 for deletion
2015-10-25 23:16:55,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742305_1487 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742305 for deletion
2015-10-25 23:16:55,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742306_1488 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742306 for deletion
2015-10-25 23:16:55,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742307_1489 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742307 for deletion
2015-10-25 23:16:55,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742302_1484 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742302 for deletion
2015-10-25 23:16:55,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742303_1485 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742303 for deletion
2015-10-25 23:16:55,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742304_1486 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742304
2015-10-25 23:16:55,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742305_1487 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742305
2015-10-25 23:16:55,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742306_1488 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742306
2015-10-25 23:16:55,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742307_1489 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742307
2015-10-25 23:16:55,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742302_1484 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742302
2015-10-25 23:16:55,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742303_1485 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742303
2015-10-25 23:24:49,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742208_1387 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742208 for deletion
2015-10-25 23:24:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742209_1388 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742209 for deletion
2015-10-25 23:24:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742210_1389 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742210 for deletion
2015-10-25 23:24:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742211_1390 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742211 for deletion
2015-10-25 23:24:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742212_1391 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742212 for deletion
2015-10-25 23:24:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742213_1392 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742213 for deletion
2015-10-25 23:24:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742214_1393 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742214 for deletion
2015-10-25 23:24:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742215_1394 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742215 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742216_1395 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742216 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742217_1396 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742217 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742218_1397 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742218 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742219_1398 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742219 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742220_1399 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742220 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742221_1400 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742221 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742222_1401 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742222 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742223_1402 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742223 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742224_1403 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742224 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742225_1404 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742225 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742226_1405 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742226 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742227_1406 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742227 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742166_1345 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742166 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742167_1346 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742167 for deletion
2015-10-25 23:24:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742168_1347 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742168 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742169_1348 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742169 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742170_1349 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742170 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742171_1350 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742171 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742172_1351 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742172 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742173_1352 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742173 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742174_1353 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742174 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742175_1354 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742175 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742176_1355 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742176 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742177_1356 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742177 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742178_1357 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742178 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742179_1358 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742179 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742180_1359 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742180 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742181_1360 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742181 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742182_1361 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742182 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742183_1362 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742183 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742184_1363 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742184 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742185_1364 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742185 for deletion
2015-10-25 23:24:49,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742186_1365 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742186 for deletion
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742187_1366 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742187 for deletion
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742188_1367 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742188 for deletion
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742189_1368 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742189 for deletion
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742190_1369 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742190 for deletion
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742191_1370 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742191 for deletion
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742192_1371 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742192 for deletion
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742208_1387 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742208
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742209_1388 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742209
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742210_1389 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742210
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742211_1390 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742211
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742212_1391 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742212
2015-10-25 23:24:49,671 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742213_1392 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742213
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742214_1393 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742214
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742215_1394 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742215
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742216_1395 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742216
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742217_1396 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742217
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742218_1397 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742218
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742219_1398 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742219
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742220_1399 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742220
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742221_1400 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742221
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742222_1401 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742222
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742223_1402 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742223
2015-10-25 23:24:49,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742224_1403 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742224
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742225_1404 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742225
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742226_1405 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742226
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742227_1406 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742227
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742166_1345 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742166
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742167_1346 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742167
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742168_1347 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742168
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742169_1348 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742169
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742170_1349 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742170
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742171_1350 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742171
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742172_1351 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742172
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742173_1352 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742173
2015-10-25 23:24:49,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742174_1353 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742174
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742175_1354 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742175
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742176_1355 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742176
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742177_1356 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742177
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742178_1357 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742178
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742179_1358 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742179
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742180_1359 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742180
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742181_1360 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742181
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742182_1361 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742182
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742183_1362 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742183
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742184_1363 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742184
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742185_1364 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742185
2015-10-25 23:24:49,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742186_1365 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742186
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742187_1366 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742187
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742188_1367 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742188
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742189_1368 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742189
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742190_1369 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742190
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742193_1372 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742193 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742194_1373 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742194 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742195_1374 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742195 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742196_1375 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742196 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742197_1376 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742197 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742198_1377 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742198 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742199_1378 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742199 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742200_1379 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742200 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742201_1380 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742201 for deletion
2015-10-25 23:24:49,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742202_1381 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742202 for deletion
2015-10-25 23:24:49,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742203_1382 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742203 for deletion
2015-10-25 23:24:49,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742204_1383 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742204 for deletion
2015-10-25 23:24:49,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742205_1384 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742205 for deletion
2015-10-25 23:24:49,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742206_1385 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742206 for deletion
2015-10-25 23:24:49,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742207_1386 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742207 for deletion
2015-10-25 23:24:49,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742191_1370 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742191
2015-10-25 23:24:49,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742192_1371 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742192
2015-10-25 23:24:49,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742193_1372 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742193
2015-10-25 23:24:49,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742194_1373 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742194
2015-10-25 23:24:49,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742195_1374 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742195
2015-10-25 23:24:49,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742196_1375 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742196
2015-10-25 23:24:49,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742197_1376 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742197
2015-10-25 23:24:49,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742198_1377 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742198
2015-10-25 23:24:49,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742199_1378 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742199
2015-10-25 23:24:49,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742200_1379 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742200
2015-10-25 23:24:49,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742201_1380 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742201
2015-10-25 23:24:49,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742202_1381 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742202
2015-10-25 23:24:49,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742203_1382 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742203
2015-10-25 23:24:49,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742204_1383 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742204
2015-10-25 23:24:49,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742205_1384 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742205
2015-10-25 23:24:49,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742206_1385 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742206
2015-10-25 23:24:49,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742207_1386 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742207
2015-10-25 23:25:15,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742312_1494 src: /192.168.54.130:46547 dest: /192.168.54.130:50010
2015-10-25 23:25:15,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46547, dest: /192.168.54.130:50010, bytes: 663, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1208732945_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742312_1494, duration: 23958427
2015-10-25 23:25:15,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742312_1494, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:25:53,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742313_1495 src: /192.168.54.130:46552 dest: /192.168.54.130:50010
2015-10-25 23:25:53,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46552, dest: /192.168.54.130:50010, bytes: 4605, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1971006563_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742313_1495, duration: 36097974
2015-10-25 23:25:53,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742313_1495, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:25:53,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742314_1496 src: /192.168.54.130:46553 dest: /192.168.54.130:50010
2015-10-25 23:25:53,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46553, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1971006563_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742314_1496, duration: 5122384
2015-10-25 23:25:53,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742314_1496, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 23:25:53,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742315_1497 src: /192.168.54.130:46555 dest: /192.168.54.130:50010
2015-10-25 23:25:53,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46555, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1971006563_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742315_1497, duration: 572348
2015-10-25 23:25:53,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742315_1497, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:25:53,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742316_1498 src: /192.168.54.130:46556 dest: /192.168.54.130:50010
2015-10-25 23:25:53,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46556, dest: /192.168.54.130:50010, bytes: 95861, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1971006563_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742316_1498, duration: 1365485
2015-10-25 23:25:53,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742316_1498, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:25:55,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742313_1495 to 192.168.54.131:50010 
2015-10-25 23:25:55,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742313_1495 (numBytes=4605) to /192.168.54.131:50010
2015-10-25 23:25:57,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742317_1499 src: /192.168.54.130:46565 dest: /192.168.54.130:50010
2015-10-25 23:25:57,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46565, dest: /192.168.54.130:50010, bytes: 114020, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_959789123_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742317_1499, duration: 25056390
2015-10-25 23:25:57,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742317_1499, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:26:03,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742318_1500 src: /192.168.54.130:46577 dest: /192.168.54.130:50010
2015-10-25 23:26:08,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742319_1501 src: /192.168.54.130:46582 dest: /192.168.54.130:50010
2015-10-25 23:26:08,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46582, dest: /192.168.54.130:50010, bytes: 7842, op: HDFS_WRITE, cliID: DFSClient_attempt_1445836126082_0003_r_000000_0_-236484936_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742319_1501, duration: 29736130
2015-10-25 23:26:08,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742319_1501, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:26:09,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46577, dest: /192.168.54.130:50010, bytes: 39935, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_959789123_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742318_1500, duration: 5141965808
2015-10-25 23:26:09,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742318_1500, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:26:09,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742320_1502 src: /192.168.54.130:46584 dest: /192.168.54.130:50010
2015-10-25 23:26:09,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46584, dest: /192.168.54.130:50010, bytes: 352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_959789123_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742320_1502, duration: 3187627
2015-10-25 23:26:09,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742320_1502, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:26:09,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742321_1503 src: /192.168.54.130:46586 dest: /192.168.54.130:50010
2015-10-25 23:26:09,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46586, dest: /192.168.54.130:50010, bytes: 39935, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_959789123_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742321_1503, duration: 3326944
2015-10-25 23:26:09,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742321_1503, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:26:09,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742322_1504 src: /192.168.54.130:46587 dest: /192.168.54.130:50010
2015-10-25 23:26:09,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46587, dest: /192.168.54.130:50010, bytes: 114020, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_959789123_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742322_1504, duration: 3811488
2015-10-25 23:26:09,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742322_1504, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:26:16,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742313_1495 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742313 for deletion
2015-10-25 23:26:16,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742314_1496 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742314 for deletion
2015-10-25 23:26:16,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742315_1497 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742315 for deletion
2015-10-25 23:26:16,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742316_1498 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742316 for deletion
2015-10-25 23:26:16,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742317_1499 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742317 for deletion
2015-10-25 23:26:16,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742318_1500 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742318 for deletion
2015-10-25 23:26:16,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742313_1495 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742313
2015-10-25 23:26:16,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742314_1496 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742314
2015-10-25 23:26:16,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742315_1497 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742315
2015-10-25 23:26:16,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742316_1498 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742316
2015-10-25 23:26:16,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742317_1499 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742317
2015-10-25 23:26:16,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742318_1500 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742318
2015-10-25 23:55:49,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742319_1501 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742319 for deletion
2015-10-25 23:55:49,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742319_1501 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742319
2015-10-25 23:56:13,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742323_1505 src: /192.168.54.130:46624 dest: /192.168.54.130:50010
2015-10-25 23:56:13,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46624, dest: /192.168.54.130:50010, bytes: 4575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1360481502_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742323_1505, duration: 29130699
2015-10-25 23:56:13,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742323_1505, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:13,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742324_1506 src: /192.168.54.130:46625 dest: /192.168.54.130:50010
2015-10-25 23:56:13,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46625, dest: /192.168.54.130:50010, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1360481502_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742324_1506, duration: 6421630
2015-10-25 23:56:13,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742324_1506, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 23:56:13,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742325_1507 src: /192.168.54.130:46627 dest: /192.168.54.130:50010
2015-10-25 23:56:13,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46627, dest: /192.168.54.130:50010, bytes: 46, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1360481502_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742325_1507, duration: 646573
2015-10-25 23:56:13,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742325_1507, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:13,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742326_1508 src: /192.168.54.130:46628 dest: /192.168.54.130:50010
2015-10-25 23:56:13,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46628, dest: /192.168.54.130:50010, bytes: 95861, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1360481502_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742326_1508, duration: 1096611
2015-10-25 23:56:13,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742326_1508, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:16,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742323_1505 to 192.168.54.131:50010 
2015-10-25 23:56:16,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742323_1505 (numBytes=4575) to /192.168.54.131:50010
2015-10-25 23:56:17,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742327_1509 src: /192.168.54.130:46637 dest: /192.168.54.130:50010
2015-10-25 23:56:17,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46637, dest: /192.168.54.130:50010, bytes: 114020, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1776416677_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742327_1509, duration: 24951122
2015-10-25 23:56:17,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742327_1509, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:23,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742328_1510 src: /192.168.54.130:46649 dest: /192.168.54.130:50010
2015-10-25 23:56:28,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742329_1511 src: /192.168.54.130:46654 dest: /192.168.54.130:50010
2015-10-25 23:56:28,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46654, dest: /192.168.54.130:50010, bytes: 7859, op: HDFS_WRITE, cliID: DFSClient_attempt_1445836126082_0004_r_000000_0_-241721269_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742329_1511, duration: 32643793
2015-10-25 23:56:28,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742329_1511, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:28,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46649, dest: /192.168.54.130:50010, bytes: 39934, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1776416677_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742328_1510, duration: 4842496931
2015-10-25 23:56:28,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742328_1510, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:28,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742330_1512 src: /192.168.54.130:46656 dest: /192.168.54.130:50010
2015-10-25 23:56:28,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46656, dest: /192.168.54.130:50010, bytes: 352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1776416677_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742330_1512, duration: 3143578
2015-10-25 23:56:28,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742330_1512, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:28,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742331_1513 src: /192.168.54.130:46658 dest: /192.168.54.130:50010
2015-10-25 23:56:28,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46658, dest: /192.168.54.130:50010, bytes: 39934, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1776416677_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742331_1513, duration: 3329647
2015-10-25 23:56:28,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742331_1513, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:28,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742332_1514 src: /192.168.54.130:46659 dest: /192.168.54.130:50010
2015-10-25 23:56:28,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46659, dest: /192.168.54.130:50010, bytes: 114020, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1776416677_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742332_1514, duration: 4167831
2015-10-25 23:56:28,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742332_1514, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:56:34,666 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742323_1505 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742323 for deletion
2015-10-25 23:56:34,666 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742324_1506 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742324 for deletion
2015-10-25 23:56:34,666 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742325_1507 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742325 for deletion
2015-10-25 23:56:34,666 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742326_1508 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742326 for deletion
2015-10-25 23:56:34,666 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742323_1505 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742323
2015-10-25 23:56:34,666 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742324_1506 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742324
2015-10-25 23:56:34,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742325_1507 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742325
2015-10-25 23:56:34,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742326_1508 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742326
2015-10-25 23:56:34,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742327_1509 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742327 for deletion
2015-10-25 23:56:34,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742328_1510 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742328 for deletion
2015-10-25 23:56:34,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742327_1509 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742327
2015-10-25 23:56:34,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742328_1510 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742328
2015-10-25 23:57:31,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742329_1511 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742329 for deletion
2015-10-25 23:57:31,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742329_1511 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742329
2015-10-25 23:57:52,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742312_1494 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742312 for deletion
2015-10-25 23:57:52,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742312_1494 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir1/blk_1073742312
2015-10-25 23:58:26,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742333_1515 src: /192.168.54.130:46668 dest: /192.168.54.130:50010
2015-10-25 23:58:27,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46668, dest: /192.168.54.130:50010, bytes: 663, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742333_1515, duration: 29942900
2015-10-25 23:58:27,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742333_1515, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742334_1516 src: /192.168.54.130:46669 dest: /192.168.54.130:50010
2015-10-25 23:58:27,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46669, dest: /192.168.54.130:50010, bytes: 4491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742334_1516, duration: 701200
2015-10-25 23:58:27,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742334_1516, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742335_1517 src: /192.168.54.130:46670 dest: /192.168.54.130:50010
2015-10-25 23:58:27,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46670, dest: /192.168.54.130:50010, bytes: 4299, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742335_1517, duration: 703840
2015-10-25 23:58:27,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742335_1517, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742336_1518 src: /192.168.54.130:46671 dest: /192.168.54.130:50010
2015-10-25 23:58:27,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46671, dest: /192.168.54.130:50010, bytes: 9525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742336_1518, duration: 777235
2015-10-25 23:58:27,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742336_1518, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742337_1519 src: /192.168.54.130:46672 dest: /192.168.54.130:50010
2015-10-25 23:58:27,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46672, dest: /192.168.54.130:50010, bytes: 5957, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742337_1519, duration: 720348
2015-10-25 23:58:27,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742337_1519, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742338_1520 src: /192.168.54.130:46673 dest: /192.168.54.130:50010
2015-10-25 23:58:27,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46673, dest: /192.168.54.130:50010, bytes: 5268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742338_1520, duration: 595483
2015-10-25 23:58:27,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742338_1520, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742339_1521 src: /192.168.54.130:46674 dest: /192.168.54.130:50010
2015-10-25 23:58:27,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46674, dest: /192.168.54.130:50010, bytes: 13016, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742339_1521, duration: 610642
2015-10-25 23:58:27,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742339_1521, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742340_1522 src: /192.168.54.130:46675 dest: /192.168.54.130:50010
2015-10-25 23:58:27,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46675, dest: /192.168.54.130:50010, bytes: 11168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742340_1522, duration: 694502
2015-10-25 23:58:27,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742340_1522, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742341_1523 src: /192.168.54.130:46676 dest: /192.168.54.130:50010
2015-10-25 23:58:27,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46676, dest: /192.168.54.130:50010, bytes: 10990, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742341_1523, duration: 745419
2015-10-25 23:58:27,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742341_1523, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742342_1524 src: /192.168.54.130:46677 dest: /192.168.54.130:50010
2015-10-25 23:58:27,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46677, dest: /192.168.54.130:50010, bytes: 9742, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742342_1524, duration: 989169
2015-10-25 23:58:27,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742342_1524, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742343_1525 src: /192.168.54.130:46678 dest: /192.168.54.130:50010
2015-10-25 23:58:27,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46678, dest: /192.168.54.130:50010, bytes: 12483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742343_1525, duration: 678987
2015-10-25 23:58:27,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742343_1525, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742344_1526 src: /192.168.54.130:46679 dest: /192.168.54.130:50010
2015-10-25 23:58:27,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46679, dest: /192.168.54.130:50010, bytes: 8938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742344_1526, duration: 721756
2015-10-25 23:58:27,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742344_1526, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742345_1527 src: /192.168.54.130:46680 dest: /192.168.54.130:50010
2015-10-25 23:58:27,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46680, dest: /192.168.54.130:50010, bytes: 3948, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742345_1527, duration: 787959
2015-10-25 23:58:27,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742345_1527, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742346_1528 src: /192.168.54.130:46681 dest: /192.168.54.130:50010
2015-10-25 23:58:27,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46681, dest: /192.168.54.130:50010, bytes: 9222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742346_1528, duration: 791998
2015-10-25 23:58:27,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742346_1528, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742347_1529 src: /192.168.54.130:46682 dest: /192.168.54.130:50010
2015-10-25 23:58:27,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46682, dest: /192.168.54.130:50010, bytes: 6459, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742347_1529, duration: 1122029
2015-10-25 23:58:27,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742347_1529, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742348_1530 src: /192.168.54.130:46683 dest: /192.168.54.130:50010
2015-10-25 23:58:27,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46683, dest: /192.168.54.130:50010, bytes: 9811, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742348_1530, duration: 1020154
2015-10-25 23:58:27,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742348_1530, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742349_1531 src: /192.168.54.130:46684 dest: /192.168.54.130:50010
2015-10-25 23:58:27,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46684, dest: /192.168.54.130:50010, bytes: 19000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742349_1531, duration: 919890
2015-10-25 23:58:27,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742349_1531, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742350_1532 src: /192.168.54.130:46685 dest: /192.168.54.130:50010
2015-10-25 23:58:27,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46685, dest: /192.168.54.130:50010, bytes: 7279, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742350_1532, duration: 774176
2015-10-25 23:58:27,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742350_1532, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742351_1533 src: /192.168.54.130:46686 dest: /192.168.54.130:50010
2015-10-25 23:58:27,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46686, dest: /192.168.54.130:50010, bytes: 29112, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742351_1533, duration: 669977
2015-10-25 23:58:27,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742351_1533, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742352_1534 src: /192.168.54.130:46687 dest: /192.168.54.130:50010
2015-10-25 23:58:27,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46687, dest: /192.168.54.130:50010, bytes: 10673, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742352_1534, duration: 849698
2015-10-25 23:58:27,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742352_1534, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742353_1535 src: /192.168.54.130:46688 dest: /192.168.54.130:50010
2015-10-25 23:58:27,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46688, dest: /192.168.54.130:50010, bytes: 9176, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742353_1535, duration: 740215
2015-10-25 23:58:27,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742353_1535, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742354_1536 src: /192.168.54.130:46689 dest: /192.168.54.130:50010
2015-10-25 23:58:27,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46689, dest: /192.168.54.130:50010, bytes: 11230, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742354_1536, duration: 744387
2015-10-25 23:58:27,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742354_1536, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742355_1537 src: /192.168.54.130:46690 dest: /192.168.54.130:50010
2015-10-25 23:58:27,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46690, dest: /192.168.54.130:50010, bytes: 9954, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742355_1537, duration: 2630230
2015-10-25 23:58:27,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742355_1537, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742356_1538 src: /192.168.54.130:46691 dest: /192.168.54.130:50010
2015-10-25 23:58:27,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46691, dest: /192.168.54.130:50010, bytes: 9328, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742356_1538, duration: 654166
2015-10-25 23:58:27,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742356_1538, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742357_1539 src: /192.168.54.130:46692 dest: /192.168.54.130:50010
2015-10-25 23:58:27,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46692, dest: /192.168.54.130:50010, bytes: 10827, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742357_1539, duration: 531197
2015-10-25 23:58:27,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742357_1539, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742358_1540 src: /192.168.54.130:46693 dest: /192.168.54.130:50010
2015-10-25 23:58:27,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46693, dest: /192.168.54.130:50010, bytes: 8573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742358_1540, duration: 621698
2015-10-25 23:58:27,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742358_1540, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742359_1541 src: /192.168.54.130:46694 dest: /192.168.54.130:50010
2015-10-25 23:58:27,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46694, dest: /192.168.54.130:50010, bytes: 12751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742359_1541, duration: 640275
2015-10-25 23:58:27,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742359_1541, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742360_1542 src: /192.168.54.130:46695 dest: /192.168.54.130:50010
2015-10-25 23:58:27,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46695, dest: /192.168.54.130:50010, bytes: 7216, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742360_1542, duration: 572154
2015-10-25 23:58:27,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742360_1542, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742361_1543 src: /192.168.54.130:46696 dest: /192.168.54.130:50010
2015-10-25 23:58:27,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46696, dest: /192.168.54.130:50010, bytes: 8245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742361_1543, duration: 515958
2015-10-25 23:58:27,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742361_1543, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742362_1544 src: /192.168.54.130:46697 dest: /192.168.54.130:50010
2015-10-25 23:58:27,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46697, dest: /192.168.54.130:50010, bytes: 13713, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742362_1544, duration: 673873
2015-10-25 23:58:27,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742362_1544, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742363_1545 src: /192.168.54.130:46698 dest: /192.168.54.130:50010
2015-10-25 23:58:27,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46698, dest: /192.168.54.130:50010, bytes: 7074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742363_1545, duration: 437299
2015-10-25 23:58:27,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742363_1545, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742364_1546 src: /192.168.54.130:46699 dest: /192.168.54.130:50010
2015-10-25 23:58:27,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46699, dest: /192.168.54.130:50010, bytes: 8670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742364_1546, duration: 404483
2015-10-25 23:58:27,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742364_1546, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742365_1547 src: /192.168.54.130:46700 dest: /192.168.54.130:50010
2015-10-25 23:58:27,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46700, dest: /192.168.54.130:50010, bytes: 8483, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742365_1547, duration: 408629
2015-10-25 23:58:27,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742365_1547, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742366_1548 src: /192.168.54.130:46701 dest: /192.168.54.130:50010
2015-10-25 23:58:27,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46701, dest: /192.168.54.130:50010, bytes: 10347, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742366_1548, duration: 383128
2015-10-25 23:58:27,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742366_1548, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742367_1549 src: /192.168.54.130:46702 dest: /192.168.54.130:50010
2015-10-25 23:58:27,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46702, dest: /192.168.54.130:50010, bytes: 12018, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742367_1549, duration: 374446
2015-10-25 23:58:27,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742367_1549, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742368_1550 src: /192.168.54.130:46703 dest: /192.168.54.130:50010
2015-10-25 23:58:27,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46703, dest: /192.168.54.130:50010, bytes: 16999, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742368_1550, duration: 390343
2015-10-25 23:58:27,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742368_1550, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742369_1551 src: /192.168.54.130:46704 dest: /192.168.54.130:50010
2015-10-25 23:58:27,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46704, dest: /192.168.54.130:50010, bytes: 11799, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742369_1551, duration: 617596
2015-10-25 23:58:27,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742369_1551, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742370_1552 src: /192.168.54.130:46705 dest: /192.168.54.130:50010
2015-10-25 23:58:27,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46705, dest: /192.168.54.130:50010, bytes: 7797, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742370_1552, duration: 387575
2015-10-25 23:58:27,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742370_1552, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742371_1553 src: /192.168.54.130:46706 dest: /192.168.54.130:50010
2015-10-25 23:58:27,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46706, dest: /192.168.54.130:50010, bytes: 5937, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742371_1553, duration: 527322
2015-10-25 23:58:27,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742371_1553, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742372_1554 src: /192.168.54.130:46707 dest: /192.168.54.130:50010
2015-10-25 23:58:27,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46707, dest: /192.168.54.130:50010, bytes: 8751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742372_1554, duration: 445456
2015-10-25 23:58:27,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742372_1554, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742373_1555 src: /192.168.54.130:46708 dest: /192.168.54.130:50010
2015-10-25 23:58:27,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46708, dest: /192.168.54.130:50010, bytes: 9168, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742373_1555, duration: 382730
2015-10-25 23:58:27,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742373_1555, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742374_1556 src: /192.168.54.130:46709 dest: /192.168.54.130:50010
2015-10-25 23:58:27,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46709, dest: /192.168.54.130:50010, bytes: 13006, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742374_1556, duration: 386665
2015-10-25 23:58:27,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742374_1556, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742375_1557 src: /192.168.54.130:46710 dest: /192.168.54.130:50010
2015-10-25 23:58:27,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46710, dest: /192.168.54.130:50010, bytes: 10662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742375_1557, duration: 398027
2015-10-25 23:58:27,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742375_1557, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742376_1558 src: /192.168.54.130:46711 dest: /192.168.54.130:50010
2015-10-25 23:58:27,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46711, dest: /192.168.54.130:50010, bytes: 27491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742376_1558, duration: 532018
2015-10-25 23:58:27,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742376_1558, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742377_1559 src: /192.168.54.130:46712 dest: /192.168.54.130:50010
2015-10-25 23:58:27,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46712, dest: /192.168.54.130:50010, bytes: 13387, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742377_1559, duration: 1347868
2015-10-25 23:58:27,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742377_1559, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742378_1560 src: /192.168.54.130:46713 dest: /192.168.54.130:50010
2015-10-25 23:58:27,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46713, dest: /192.168.54.130:50010, bytes: 10094, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742378_1560, duration: 1077296
2015-10-25 23:58:27,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742378_1560, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742379_1561 src: /192.168.54.130:46714 dest: /192.168.54.130:50010
2015-10-25 23:58:27,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46714, dest: /192.168.54.130:50010, bytes: 16889, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742379_1561, duration: 346900
2015-10-25 23:58:27,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742379_1561, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742380_1562 src: /192.168.54.130:46715 dest: /192.168.54.130:50010
2015-10-25 23:58:27,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46715, dest: /192.168.54.130:50010, bytes: 22332, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742380_1562, duration: 1709414
2015-10-25 23:58:27,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742380_1562, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742381_1563 src: /192.168.54.130:46716 dest: /192.168.54.130:50010
2015-10-25 23:58:27,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46716, dest: /192.168.54.130:50010, bytes: 12552, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742381_1563, duration: 370781
2015-10-25 23:58:27,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742381_1563, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742382_1564 src: /192.168.54.130:46717 dest: /192.168.54.130:50010
2015-10-25 23:58:27,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46717, dest: /192.168.54.130:50010, bytes: 12248, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742382_1564, duration: 402744
2015-10-25 23:58:27,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742382_1564, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742383_1565 src: /192.168.54.130:46718 dest: /192.168.54.130:50010
2015-10-25 23:58:27,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46718, dest: /192.168.54.130:50010, bytes: 12497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742383_1565, duration: 461827
2015-10-25 23:58:27,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742383_1565, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742384_1566 src: /192.168.54.130:46719 dest: /192.168.54.130:50010
2015-10-25 23:58:27,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46719, dest: /192.168.54.130:50010, bytes: 11243, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742384_1566, duration: 373089
2015-10-25 23:58:27,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742384_1566, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742385_1567 src: /192.168.54.130:46720 dest: /192.168.54.130:50010
2015-10-25 23:58:27,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46720, dest: /192.168.54.130:50010, bytes: 16578, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742385_1567, duration: 364288
2015-10-25 23:58:27,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742385_1567, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742386_1568 src: /192.168.54.130:46721 dest: /192.168.54.130:50010
2015-10-25 23:58:27,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46721, dest: /192.168.54.130:50010, bytes: 16110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742386_1568, duration: 368114
2015-10-25 23:58:27,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742386_1568, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742387_1569 src: /192.168.54.130:46722 dest: /192.168.54.130:50010
2015-10-25 23:58:27,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46722, dest: /192.168.54.130:50010, bytes: 8866, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742387_1569, duration: 462649
2015-10-25 23:58:27,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742387_1569, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742388_1570 src: /192.168.54.130:46723 dest: /192.168.54.130:50010
2015-10-25 23:58:27,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46723, dest: /192.168.54.130:50010, bytes: 12994, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742388_1570, duration: 362587
2015-10-25 23:58:27,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742388_1570, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742389_1571 src: /192.168.54.130:46724 dest: /192.168.54.130:50010
2015-10-25 23:58:27,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46724, dest: /192.168.54.130:50010, bytes: 15319, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742389_1571, duration: 408075
2015-10-25 23:58:27,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742389_1571, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742390_1572 src: /192.168.54.130:46725 dest: /192.168.54.130:50010
2015-10-25 23:58:27,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46725, dest: /192.168.54.130:50010, bytes: 9408, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742390_1572, duration: 408348
2015-10-25 23:58:27,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742390_1572, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742391_1573 src: /192.168.54.130:46726 dest: /192.168.54.130:50010
2015-10-25 23:58:27,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46726, dest: /192.168.54.130:50010, bytes: 13657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742391_1573, duration: 413617
2015-10-25 23:58:27,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742391_1573, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742392_1574 src: /192.168.54.130:46727 dest: /192.168.54.130:50010
2015-10-25 23:58:27,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46727, dest: /192.168.54.130:50010, bytes: 13575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742392_1574, duration: 388053
2015-10-25 23:58:27,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742392_1574, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742393_1575 src: /192.168.54.130:46728 dest: /192.168.54.130:50010
2015-10-25 23:58:27,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46728, dest: /192.168.54.130:50010, bytes: 8722, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742393_1575, duration: 396512
2015-10-25 23:58:27,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742393_1575, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:27,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742394_1576 src: /192.168.54.130:46729 dest: /192.168.54.130:50010
2015-10-25 23:58:27,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46729, dest: /192.168.54.130:50010, bytes: 25882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_441969200_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742394_1576, duration: 951375
2015-10-25 23:58:27,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742394_1576, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:55,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742395_1577 src: /192.168.54.130:46734 dest: /192.168.54.130:50010
2015-10-25 23:58:55,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46734, dest: /192.168.54.130:50010, bytes: 4575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1794903518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742395_1577, duration: 26150770
2015-10-25 23:58:55,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742395_1577, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:55,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742396_1578 src: /192.168.54.130:46735 dest: /192.168.54.130:50010
2015-10-25 23:58:55,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46735, dest: /192.168.54.130:50010, bytes: 5525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1794903518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742396_1578, duration: 4276092
2015-10-25 23:58:55,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742396_1578, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-25 23:58:55,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742397_1579 src: /192.168.54.130:46737 dest: /192.168.54.130:50010
2015-10-25 23:58:55,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46737, dest: /192.168.54.130:50010, bytes: 1245, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1794903518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742397_1579, duration: 359725
2015-10-25 23:58:55,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742397_1579, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:56,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742398_1580 src: /192.168.54.130:46738 dest: /192.168.54.130:50010
2015-10-25 23:58:56,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:46738, dest: /192.168.54.130:50010, bytes: 95863, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1794903518_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742398_1580, duration: 730474
2015-10-25 23:58:56,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742398_1580, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-25 23:58:58,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742395_1577 to 192.168.54.131:50010 
2015-10-25 23:58:58,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1892222280-192.168.54.130-1445296806381:blk_1073742395_1577 (numBytes=4575) to /192.168.54.131:50010
2015-10-26 00:02:19,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742395_1577 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742395 for deletion
2015-10-26 00:02:19,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742396_1578 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742396 for deletion
2015-10-26 00:02:19,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742397_1579 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742397 for deletion
2015-10-26 00:02:19,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742398_1580 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742398 for deletion
2015-10-26 00:02:19,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742395_1577 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742395
2015-10-26 00:02:19,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742396_1578 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742396
2015-10-26 00:02:19,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742397_1579 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742397
2015-10-26 00:02:19,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742398_1580 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742398
2015-10-26 08:02:20,767 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1064ms
No GCs detected
2015-11-15 10:21:47,528 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 142, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-15 11:21:36,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x12c0f114fef0,  containing 1 storage report(s), of which we sent 1. The reports had 142 total blocks and used 1 RPC(s). This took 0 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-15 11:21:36,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-15 11:53:13,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742405_1587 src: /192.168.54.130:49137 dest: /192.168.54.130:50010
2015-11-15 11:53:13,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49137, dest: /192.168.54.130:50010, bytes: 94506, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1524857597_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742405_1587, duration: 50569917
2015-11-15 11:53:13,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742405_1587, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 11:53:13,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742406_1588 src: /192.168.54.130:49142 dest: /192.168.54.130:50010
2015-11-15 11:53:13,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49142, dest: /192.168.54.130:50010, bytes: 98238, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1524857597_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742406_1588, duration: 8295840
2015-11-15 11:53:13,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742406_1588, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 11:53:27,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742405_1587 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742405 for deletion
2015-11-15 11:53:27,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742406_1588 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742406 for deletion
2015-11-15 11:53:27,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742405_1587 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742405
2015-11-15 11:53:27,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742406_1588 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742406
2015-11-15 12:44:57,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742407_1589 src: /192.168.54.130:52049 dest: /192.168.54.130:50010
2015-11-15 12:44:57,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52049, dest: /192.168.54.130:50010, bytes: 94506, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1901819255_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742407_1589, duration: 46746013
2015-11-15 12:44:57,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742407_1589, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 12:44:58,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742408_1590 src: /192.168.54.130:52052 dest: /192.168.54.130:50010
2015-11-15 12:44:58,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52052, dest: /192.168.54.130:50010, bytes: 98229, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1901819255_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742408_1590, duration: 7265135
2015-11-15 12:44:58,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742408_1590, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 12:45:12,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742407_1589 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742407 for deletion
2015-11-15 12:45:12,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742408_1590 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742408 for deletion
2015-11-15 12:45:12,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742407_1589 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742407
2015-11-15 12:45:12,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742408_1590 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742408
2015-11-15 12:53:50,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742409_1591 src: /192.168.54.130:53050 dest: /192.168.54.130:50010
2015-11-15 12:53:50,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53050, dest: /192.168.54.130:50010, bytes: 94506, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1849752049_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742409_1591, duration: 43142269
2015-11-15 12:53:50,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742409_1591, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 12:53:50,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742410_1592 src: /192.168.54.130:53052 dest: /192.168.54.130:50010
2015-11-15 12:53:50,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53052, dest: /192.168.54.130:50010, bytes: 98216, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1849752049_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742410_1592, duration: 18874496
2015-11-15 12:53:50,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742410_1592, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 12:53:57,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742411_1593 src: /192.168.54.130:53072 dest: /192.168.54.130:50010
2015-11-15 12:53:57,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53072, dest: /192.168.54.130:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1201582337_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742411_1593, duration: 33582961
2015-11-15 12:53:57,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742411_1593, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 12:54:03,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742409_1591 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742409 for deletion
2015-11-15 12:54:03,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742410_1592 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742410 for deletion
2015-11-15 12:54:03,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742411_1593 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742411 for deletion
2015-11-15 12:54:03,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742409_1591 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742409
2015-11-15 12:54:03,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742410_1592 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742410
2015-11-15 12:54:03,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742411_1593 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742411
2015-11-15 14:04:03,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742412_1594 src: /192.168.54.130:53208 dest: /192.168.54.130:50010
2015-11-15 14:04:03,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53208, dest: /192.168.54.130:50010, bytes: 94506, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_17767610_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742412_1594, duration: 40177102
2015-11-15 14:04:03,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742412_1594, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 14:04:03,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742413_1595 src: /192.168.54.130:53210 dest: /192.168.54.130:50010
2015-11-15 14:04:03,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53210, dest: /192.168.54.130:50010, bytes: 98216, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_17767610_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742413_1595, duration: 8573127
2015-11-15 14:04:03,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742413_1595, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 14:04:11,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742414_1596 src: /192.168.54.130:53231 dest: /192.168.54.130:50010
2015-11-15 14:04:11,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53231, dest: /192.168.54.130:50010, bytes: 44, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-418192105_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742414_1596, duration: 36620258
2015-11-15 14:04:11,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742414_1596, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 14:04:15,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742412_1594 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742412 for deletion
2015-11-15 14:04:15,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742413_1595 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742413 for deletion
2015-11-15 14:04:15,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742414_1596 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742414 for deletion
2015-11-15 14:04:15,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742412_1594 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742412
2015-11-15 14:04:15,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742413_1595 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742413
2015-11-15 14:04:15,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742414_1596 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742414
2015-11-15 14:21:26,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742415_1597 src: /192.168.54.130:53316 dest: /192.168.54.130:50010
2015-11-15 14:21:26,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53316, dest: /192.168.54.130:50010, bytes: 94506, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-523846121_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742415_1597, duration: 44003101
2015-11-15 14:21:26,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742415_1597, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 14:21:26,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742416_1598 src: /192.168.54.130:53318 dest: /192.168.54.130:50010
2015-11-15 14:21:26,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53318, dest: /192.168.54.130:50010, bytes: 98229, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-523846121_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742416_1598, duration: 6626857
2015-11-15 14:21:26,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742416_1598, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 14:21:34,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742417_1599 src: /192.168.54.130:53339 dest: /192.168.54.130:50010
2015-11-15 14:21:34,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53339, dest: /192.168.54.130:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2000082528_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742417_1599, duration: 34135819
2015-11-15 14:21:34,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742417_1599, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 14:21:39,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742416_1598 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742416 for deletion
2015-11-15 14:21:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742417_1599 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742417 for deletion
2015-11-15 14:21:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742415_1597 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742415 for deletion
2015-11-15 14:21:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742416_1598 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742416
2015-11-15 14:21:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742417_1599 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742417
2015-11-15 14:21:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742415_1597 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742415
2015-11-15 15:52:09,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742418_1600 src: /192.168.54.130:53502 dest: /192.168.54.130:50010
2015-11-15 15:52:09,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53502, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-69814568_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742418_1600, duration: 31842274
2015-11-15 15:52:09,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742418_1600, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 15:52:09,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742419_1601 src: /192.168.54.130:53504 dest: /192.168.54.130:50010
2015-11-15 15:52:09,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53504, dest: /192.168.54.130:50010, bytes: 3441, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-69814568_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742419_1601, duration: 3593145
2015-11-15 15:52:09,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742419_1601, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 15:52:09,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742420_1602 src: /192.168.54.130:53506 dest: /192.168.54.130:50010
2015-11-15 15:52:09,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53506, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-69814568_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742420_1602, duration: 7953474
2015-11-15 15:52:09,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742420_1602, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 15:53:54,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742418_1600 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742418 for deletion
2015-11-15 15:53:54,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742419_1601 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742419 for deletion
2015-11-15 15:53:54,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742420_1602 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742420 for deletion
2015-11-15 15:53:54,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742418_1600 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742418
2015-11-15 15:53:54,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742419_1601 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742419
2015-11-15 15:53:54,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742420_1602 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742420
2015-11-15 15:59:20,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742421_1603 src: /192.168.54.130:53524 dest: /192.168.54.130:50010
2015-11-15 15:59:20,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53524, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-288678352_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742421_1603, duration: 32473955
2015-11-15 15:59:20,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742421_1603, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 15:59:21,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742422_1604 src: /192.168.54.130:53526 dest: /192.168.54.130:50010
2015-11-15 15:59:21,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53526, dest: /192.168.54.130:50010, bytes: 3441, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-288678352_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742422_1604, duration: 4239424
2015-11-15 15:59:21,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742422_1604, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 15:59:21,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742423_1605 src: /192.168.54.130:53528 dest: /192.168.54.130:50010
2015-11-15 15:59:21,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53528, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-288678352_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742423_1605, duration: 8310857
2015-11-15 15:59:21,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742423_1605, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:01:15,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742421_1603 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742421 for deletion
2015-11-15 16:01:15,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742422_1604 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742422 for deletion
2015-11-15 16:01:15,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742423_1605 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742423 for deletion
2015-11-15 16:01:15,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742421_1603 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742421
2015-11-15 16:01:15,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742422_1604 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742422
2015-11-15 16:01:15,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742423_1605 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742423
2015-11-15 16:02:48,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742424_1606 src: /192.168.54.130:53543 dest: /192.168.54.130:50010
2015-11-15 16:02:48,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53543, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_178586514_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742424_1606, duration: 33553401
2015-11-15 16:02:48,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742424_1606, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:02:48,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742425_1607 src: /192.168.54.130:53545 dest: /192.168.54.130:50010
2015-11-15 16:02:48,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53545, dest: /192.168.54.130:50010, bytes: 3441, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_178586514_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742425_1607, duration: 3659527
2015-11-15 16:02:48,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742425_1607, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:02:49,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742426_1608 src: /192.168.54.130:53547 dest: /192.168.54.130:50010
2015-11-15 16:02:49,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53547, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_178586514_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742426_1608, duration: 23414030
2015-11-15 16:02:49,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742426_1608, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:03:57,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742424_1606 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742424 for deletion
2015-11-15 16:03:57,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742425_1607 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742425 for deletion
2015-11-15 16:03:57,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742426_1608 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742426 for deletion
2015-11-15 16:03:57,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742424_1606 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742424
2015-11-15 16:03:57,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742425_1607 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742425
2015-11-15 16:03:57,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742426_1608 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742426
2015-11-15 16:08:45,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742427_1609 src: /192.168.54.130:53563 dest: /192.168.54.130:50010
2015-11-15 16:08:45,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53563, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1752856880_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742427_1609, duration: 37755840
2015-11-15 16:08:45,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742427_1609, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:08:45,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742428_1610 src: /192.168.54.130:53565 dest: /192.168.54.130:50010
2015-11-15 16:08:45,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53565, dest: /192.168.54.130:50010, bytes: 3441, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1752856880_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742428_1610, duration: 3532706
2015-11-15 16:08:45,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742428_1610, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:08:45,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742429_1611 src: /192.168.54.130:53567 dest: /192.168.54.130:50010
2015-11-15 16:08:45,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53567, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1752856880_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742429_1611, duration: 7411867
2015-11-15 16:08:45,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742429_1611, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:08:51,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742427_1609 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742427 for deletion
2015-11-15 16:08:51,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742428_1610 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742428 for deletion
2015-11-15 16:08:51,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742429_1611 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742429 for deletion
2015-11-15 16:08:51,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742427_1609 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742427
2015-11-15 16:08:51,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742428_1610 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742428
2015-11-15 16:08:51,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742429_1611 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742429
2015-11-15 16:15:01,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742430_1612 src: /192.168.54.130:53583 dest: /192.168.54.130:50010
2015-11-15 16:15:01,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53583, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1167499209_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742430_1612, duration: 31932013
2015-11-15 16:15:01,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742430_1612, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:15:01,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742431_1613 src: /192.168.54.130:53585 dest: /192.168.54.130:50010
2015-11-15 16:15:01,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53585, dest: /192.168.54.130:50010, bytes: 3441, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1167499209_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742431_1613, duration: 4698028
2015-11-15 16:15:01,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742431_1613, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:15:01,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742432_1614 src: /192.168.54.130:53587 dest: /192.168.54.130:50010
2015-11-15 16:15:01,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53587, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1167499209_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742432_1614, duration: 10758444
2015-11-15 16:15:01,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742432_1614, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:15:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742432_1614 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742432 for deletion
2015-11-15 16:15:09,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742430_1612 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742430 for deletion
2015-11-15 16:15:09,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742431_1613 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742431 for deletion
2015-11-15 16:15:09,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742432_1614 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742432
2015-11-15 16:15:09,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742430_1612 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742430
2015-11-15 16:15:09,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742431_1613 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742431
2015-11-15 16:21:47,502 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 142, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-15 16:28:13,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742433_1615 src: /192.168.54.130:53611 dest: /192.168.54.130:50010
2015-11-15 16:28:13,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53611, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_425541100_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742433_1615, duration: 44128937
2015-11-15 16:28:13,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742433_1615, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:28:13,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742434_1616 src: /192.168.54.130:53613 dest: /192.168.54.130:50010
2015-11-15 16:28:13,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53613, dest: /192.168.54.130:50010, bytes: 3510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_425541100_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742434_1616, duration: 9153324
2015-11-15 16:28:13,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742434_1616, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:28:13,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742435_1617 src: /192.168.54.130:53615 dest: /192.168.54.130:50010
2015-11-15 16:28:13,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53615, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_425541100_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742435_1617, duration: 7797954
2015-11-15 16:28:13,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742435_1617, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 16:28:21,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742433_1615 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742433 for deletion
2015-11-15 16:28:21,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742434_1616 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742434 for deletion
2015-11-15 16:28:21,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742435_1617 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742435 for deletion
2015-11-15 16:28:21,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742433_1615 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742433
2015-11-15 16:28:21,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742434_1616 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742434
2015-11-15 16:28:21,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742435_1617 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742435
2015-11-15 17:21:36,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2666156f69c1,  containing 1 storage report(s), of which we sent 1. The reports had 142 total blocks and used 1 RPC(s). This took 1 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-15 17:21:36,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-15 18:48:33,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742436_1618 src: /192.168.54.130:53795 dest: /192.168.54.130:50010
2015-11-15 18:48:33,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53795, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-674098986_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742436_1618, duration: 34243244
2015-11-15 18:48:33,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742436_1618, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 18:48:33,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742437_1619 src: /192.168.54.130:53797 dest: /192.168.54.130:50010
2015-11-15 18:48:33,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53797, dest: /192.168.54.130:50010, bytes: 3510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-674098986_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742437_1619, duration: 3402279
2015-11-15 18:48:33,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742437_1619, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 18:48:33,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742438_1620 src: /192.168.54.130:53799 dest: /192.168.54.130:50010
2015-11-15 18:48:33,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53799, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-674098986_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742438_1620, duration: 9277501
2015-11-15 18:48:33,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742438_1620, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 18:48:39,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742436_1618 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742436 for deletion
2015-11-15 18:48:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742437_1619 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742437 for deletion
2015-11-15 18:48:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742438_1620 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742438 for deletion
2015-11-15 18:48:39,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742436_1618 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742436
2015-11-15 18:48:39,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742437_1619 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742437
2015-11-15 18:48:39,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742438_1620 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742438
2015-11-15 19:15:34,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742439_1621 src: /192.168.54.130:53839 dest: /192.168.54.130:50010
2015-11-15 19:15:34,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53839, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276549562_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742439_1621, duration: 35805258
2015-11-15 19:15:34,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742439_1621, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:15:34,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742440_1622 src: /192.168.54.130:53841 dest: /192.168.54.130:50010
2015-11-15 19:15:34,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53841, dest: /192.168.54.130:50010, bytes: 3510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276549562_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742440_1622, duration: 4667456
2015-11-15 19:15:34,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742440_1622, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:15:34,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742441_1623 src: /192.168.54.130:53843 dest: /192.168.54.130:50010
2015-11-15 19:15:34,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53843, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276549562_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742441_1623, duration: 6755899
2015-11-15 19:15:34,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742441_1623, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:15:45,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742439_1621 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742439 for deletion
2015-11-15 19:15:45,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742440_1622 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742440 for deletion
2015-11-15 19:15:45,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742441_1623 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742441 for deletion
2015-11-15 19:15:45,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742439_1621 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742439
2015-11-15 19:15:45,572 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742440_1622 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742440
2015-11-15 19:15:45,572 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742441_1623 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742441
2015-11-15 19:44:23,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742442_1624 src: /192.168.54.130:53905 dest: /192.168.54.130:50010
2015-11-15 19:44:23,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53905, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1470882025_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742442_1624, duration: 39691550
2015-11-15 19:44:23,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742442_1624, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:44:23,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742443_1625 src: /192.168.54.130:53907 dest: /192.168.54.130:50010
2015-11-15 19:44:23,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53907, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1470882025_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742443_1625, duration: 4200072
2015-11-15 19:44:23,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742443_1625, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:44:23,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742444_1626 src: /192.168.54.130:53909 dest: /192.168.54.130:50010
2015-11-15 19:44:23,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53909, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1470882025_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742444_1626, duration: 12602309
2015-11-15 19:44:23,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742444_1626, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:44:30,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742442_1624 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742442 for deletion
2015-11-15 19:44:30,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742443_1625 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742443 for deletion
2015-11-15 19:44:30,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742444_1626 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742444 for deletion
2015-11-15 19:44:30,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742442_1624 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742442
2015-11-15 19:44:30,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742443_1625 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742443
2015-11-15 19:44:30,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742444_1626 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742444
2015-11-15 19:47:28,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742445_1627 src: /192.168.54.130:53921 dest: /192.168.54.130:50010
2015-11-15 19:47:28,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53921, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1205306197_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742445_1627, duration: 34938609
2015-11-15 19:47:28,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742445_1627, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:47:28,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742446_1628 src: /192.168.54.130:53923 dest: /192.168.54.130:50010
2015-11-15 19:47:28,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53923, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1205306197_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742446_1628, duration: 4229079
2015-11-15 19:47:28,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742446_1628, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:47:28,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742447_1629 src: /192.168.54.130:53925 dest: /192.168.54.130:50010
2015-11-15 19:47:28,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53925, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1205306197_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742447_1629, duration: 11629795
2015-11-15 19:47:28,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742447_1629, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:47:36,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742445_1627 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742445 for deletion
2015-11-15 19:47:36,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742446_1628 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742446 for deletion
2015-11-15 19:47:36,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742447_1629 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742447 for deletion
2015-11-15 19:47:36,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742445_1627 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742445
2015-11-15 19:47:36,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742446_1628 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742446
2015-11-15 19:47:36,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742447_1629 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742447
2015-11-15 19:53:59,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742448_1630 src: /192.168.54.130:53943 dest: /192.168.54.130:50010
2015-11-15 19:53:59,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53943, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-810821286_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742448_1630, duration: 37815185
2015-11-15 19:53:59,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742448_1630, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:53:59,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742449_1631 src: /192.168.54.130:53945 dest: /192.168.54.130:50010
2015-11-15 19:53:59,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53945, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-810821286_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742449_1631, duration: 11789089
2015-11-15 19:53:59,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742449_1631, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:53:59,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742450_1632 src: /192.168.54.130:53947 dest: /192.168.54.130:50010
2015-11-15 19:53:59,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53947, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-810821286_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742450_1632, duration: 10134590
2015-11-15 19:53:59,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742450_1632, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:54:03,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742448_1630 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742448 for deletion
2015-11-15 19:54:03,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742449_1631 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742449 for deletion
2015-11-15 19:54:03,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742450_1632 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742450 for deletion
2015-11-15 19:54:03,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742448_1630 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742448
2015-11-15 19:54:03,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742449_1631 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742449
2015-11-15 19:54:03,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742450_1632 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742450
2015-11-15 19:56:34,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742451_1633 src: /192.168.54.130:53958 dest: /192.168.54.130:50010
2015-11-15 19:56:34,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53958, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1259725722_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742451_1633, duration: 33507598
2015-11-15 19:56:34,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742451_1633, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:56:34,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742452_1634 src: /192.168.54.130:53960 dest: /192.168.54.130:50010
2015-11-15 19:56:34,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53960, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1259725722_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742452_1634, duration: 5394366
2015-11-15 19:56:34,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742452_1634, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:56:34,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742453_1635 src: /192.168.54.130:53962 dest: /192.168.54.130:50010
2015-11-15 19:56:34,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53962, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1259725722_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742453_1635, duration: 10629128
2015-11-15 19:56:34,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742453_1635, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 19:57:48,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742451_1633 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742451 for deletion
2015-11-15 19:57:48,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742452_1634 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742452 for deletion
2015-11-15 19:57:48,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742453_1635 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742453 for deletion
2015-11-15 19:57:48,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742451_1633 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742451
2015-11-15 19:57:48,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742452_1634 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742452
2015-11-15 19:57:48,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742453_1635 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742453
2015-11-15 20:26:02,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742454_1636 src: /192.168.54.130:54009 dest: /192.168.54.130:50010
2015-11-15 20:26:02,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54009, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1107455435_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742454_1636, duration: 41727846
2015-11-15 20:26:02,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742454_1636, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:26:02,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742455_1637 src: /192.168.54.130:54011 dest: /192.168.54.130:50010
2015-11-15 20:26:02,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54011, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1107455435_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742455_1637, duration: 4857721
2015-11-15 20:26:02,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742455_1637, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:26:02,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742456_1638 src: /192.168.54.130:54013 dest: /192.168.54.130:50010
2015-11-15 20:26:02,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54013, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1107455435_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742456_1638, duration: 15315850
2015-11-15 20:26:02,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742456_1638, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:26:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742454_1636 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742454 for deletion
2015-11-15 20:26:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742455_1637 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742455 for deletion
2015-11-15 20:26:09,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742456_1638 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742456 for deletion
2015-11-15 20:26:09,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742454_1636 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742454
2015-11-15 20:26:09,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742455_1637 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742455
2015-11-15 20:26:09,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742456_1638 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742456
2015-11-15 20:29:36,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742457_1639 src: /192.168.54.130:54024 dest: /192.168.54.130:50010
2015-11-15 20:29:36,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54024, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1224452340_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742457_1639, duration: 38099453
2015-11-15 20:29:36,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742457_1639, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:29:36,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742458_1640 src: /192.168.54.130:54026 dest: /192.168.54.130:50010
2015-11-15 20:29:36,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54026, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1224452340_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742458_1640, duration: 4475010
2015-11-15 20:29:36,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742458_1640, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:29:36,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742459_1641 src: /192.168.54.130:54028 dest: /192.168.54.130:50010
2015-11-15 20:29:36,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54028, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1224452340_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742459_1641, duration: 13767897
2015-11-15 20:29:36,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742459_1641, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:29:42,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742457_1639 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742457 for deletion
2015-11-15 20:29:42,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742458_1640 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742458 for deletion
2015-11-15 20:29:42,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742459_1641 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742459 for deletion
2015-11-15 20:29:42,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742457_1639 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742457
2015-11-15 20:29:42,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742458_1640 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742458
2015-11-15 20:29:42,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742459_1641 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742459
2015-11-15 20:38:53,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742460_1642 src: /192.168.54.130:54049 dest: /192.168.54.130:50010
2015-11-15 20:38:53,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54049, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1027969966_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742460_1642, duration: 34005770
2015-11-15 20:38:53,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742460_1642, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:38:53,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742461_1643 src: /192.168.54.130:54051 dest: /192.168.54.130:50010
2015-11-15 20:38:53,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54051, dest: /192.168.54.130:50010, bytes: 3524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1027969966_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742461_1643, duration: 4678588
2015-11-15 20:38:53,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742461_1643, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:38:53,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742462_1644 src: /192.168.54.130:54053 dest: /192.168.54.130:50010
2015-11-15 20:38:53,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54053, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1027969966_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742462_1644, duration: 10170127
2015-11-15 20:38:53,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742462_1644, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:39:00,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742460_1642 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742460 for deletion
2015-11-15 20:39:00,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742461_1643 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742461 for deletion
2015-11-15 20:39:00,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742462_1644 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742462 for deletion
2015-11-15 20:39:00,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742460_1642 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742460
2015-11-15 20:39:00,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742461_1643 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742461
2015-11-15 20:39:00,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742462_1644 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742462
2015-11-15 20:40:27,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742463_1645 src: /192.168.54.130:54063 dest: /192.168.54.130:50010
2015-11-15 20:40:27,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54063, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1489769076_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742463_1645, duration: 36305220
2015-11-15 20:40:27,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742463_1645, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:40:27,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742464_1646 src: /192.168.54.130:54065 dest: /192.168.54.130:50010
2015-11-15 20:40:27,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54065, dest: /192.168.54.130:50010, bytes: 3524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1489769076_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742464_1646, duration: 4760723
2015-11-15 20:40:27,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742464_1646, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:40:27,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742465_1647 src: /192.168.54.130:54067 dest: /192.168.54.130:50010
2015-11-15 20:40:27,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54067, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1489769076_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742465_1647, duration: 9621596
2015-11-15 20:40:27,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742465_1647, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:40:33,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742464_1646 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742464 for deletion
2015-11-15 20:40:33,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742465_1647 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742465 for deletion
2015-11-15 20:40:33,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742463_1645 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742463 for deletion
2015-11-15 20:40:33,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742464_1646 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742464
2015-11-15 20:40:33,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742465_1647 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742465
2015-11-15 20:40:33,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742463_1645 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742463
2015-11-15 20:41:04,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742466_1648 src: /192.168.54.130:54076 dest: /192.168.54.130:50010
2015-11-15 20:41:04,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54076, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-621041152_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742466_1648, duration: 36225619
2015-11-15 20:41:04,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742466_1648, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:41:04,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742467_1649 src: /192.168.54.130:54078 dest: /192.168.54.130:50010
2015-11-15 20:41:04,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54078, dest: /192.168.54.130:50010, bytes: 3524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-621041152_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742467_1649, duration: 5084693
2015-11-15 20:41:04,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742467_1649, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:41:04,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742468_1650 src: /192.168.54.130:54080 dest: /192.168.54.130:50010
2015-11-15 20:41:04,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54080, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-621041152_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742468_1650, duration: 11419541
2015-11-15 20:41:04,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742468_1650, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:41:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742466_1648 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742466 for deletion
2015-11-15 20:41:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742467_1649 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742467 for deletion
2015-11-15 20:41:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742468_1650 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742468 for deletion
2015-11-15 20:41:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742466_1648 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742466
2015-11-15 20:41:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742467_1649 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742467
2015-11-15 20:41:09,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742468_1650 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742468
2015-11-15 20:47:21,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742469_1651 src: /192.168.54.130:54095 dest: /192.168.54.130:50010
2015-11-15 20:47:22,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54095, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1698045803_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742469_1651, duration: 33652432
2015-11-15 20:47:22,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742469_1651, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:47:22,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742470_1652 src: /192.168.54.130:54097 dest: /192.168.54.130:50010
2015-11-15 20:47:22,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54097, dest: /192.168.54.130:50010, bytes: 3524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1698045803_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742470_1652, duration: 4786880
2015-11-15 20:47:22,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742470_1652, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:47:22,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742471_1653 src: /192.168.54.130:54099 dest: /192.168.54.130:50010
2015-11-15 20:47:22,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54099, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1698045803_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742471_1653, duration: 15070958
2015-11-15 20:47:22,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742471_1653, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:48:36,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742469_1651 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742469 for deletion
2015-11-15 20:48:36,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742470_1652 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742470 for deletion
2015-11-15 20:48:36,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742471_1653 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742471 for deletion
2015-11-15 20:48:36,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742469_1651 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742469
2015-11-15 20:48:36,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742470_1652 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742470
2015-11-15 20:48:36,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742471_1653 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742471
2015-11-15 20:51:27,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742472_1654 src: /192.168.54.130:54114 dest: /192.168.54.130:50010
2015-11-15 20:51:27,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54114, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1932339114_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742472_1654, duration: 39876292
2015-11-15 20:51:27,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742472_1654, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:51:27,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742473_1655 src: /192.168.54.130:54116 dest: /192.168.54.130:50010
2015-11-15 20:51:27,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54116, dest: /192.168.54.130:50010, bytes: 3524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1932339114_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742473_1655, duration: 3340311
2015-11-15 20:51:27,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742473_1655, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:51:27,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742474_1656 src: /192.168.54.130:54118 dest: /192.168.54.130:50010
2015-11-15 20:51:27,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54118, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1932339114_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742474_1656, duration: 10912291
2015-11-15 20:51:27,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742474_1656, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 20:52:39,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742472_1654 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742472 for deletion
2015-11-15 20:52:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742473_1655 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742473 for deletion
2015-11-15 20:52:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742474_1656 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742474 for deletion
2015-11-15 20:52:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742472_1654 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742472
2015-11-15 20:52:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742473_1655 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742473
2015-11-15 20:52:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742474_1656 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742474
2015-11-15 22:12:07,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742475_1657 src: /192.168.54.130:54232 dest: /192.168.54.130:50010
2015-11-15 22:12:07,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54232, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1683805226_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742475_1657, duration: 48743819
2015-11-15 22:12:07,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742475_1657, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 22:12:07,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742476_1658 src: /192.168.54.130:54234 dest: /192.168.54.130:50010
2015-11-15 22:12:07,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54234, dest: /192.168.54.130:50010, bytes: 3524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1683805226_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742476_1658, duration: 4200480
2015-11-15 22:12:07,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742476_1658, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 22:12:07,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742477_1659 src: /192.168.54.130:54236 dest: /192.168.54.130:50010
2015-11-15 22:12:07,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54236, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1683805226_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742477_1659, duration: 15138391
2015-11-15 22:12:07,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742477_1659, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-15 22:13:15,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742475_1657 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742475 for deletion
2015-11-15 22:13:15,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742476_1658 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742476 for deletion
2015-11-15 22:13:15,574 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742477_1659 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742477 for deletion
2015-11-15 22:13:15,576 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742475_1657 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742475
2015-11-15 22:13:15,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742476_1658 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742476
2015-11-15 22:13:15,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742477_1659 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742477
2015-11-15 22:21:47,467 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 142, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-15 23:21:36,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3a0b39c3ed22,  containing 1 storage report(s), of which we sent 1. The reports had 142 total blocks and used 1 RPC(s). This took 1 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-15 23:21:36,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 04:21:47,471 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 142, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-16 05:21:36,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4db05e0c44b1,  containing 1 storage report(s), of which we sent 1. The reports had 142 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 05:21:36,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 08:14:24,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742478_1660 src: /192.168.54.130:55012 dest: /192.168.54.130:50010
2015-11-16 08:14:24,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55012, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_980031118_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742478_1660, duration: 37366982
2015-11-16 08:14:24,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742478_1660, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 08:14:24,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742479_1661 src: /192.168.54.130:55014 dest: /192.168.54.130:50010
2015-11-16 08:14:24,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55014, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_980031118_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742479_1661, duration: 5072386
2015-11-16 08:14:24,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742479_1661, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 08:14:25,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742480_1662 src: /192.168.54.130:55016 dest: /192.168.54.130:50010
2015-11-16 08:14:25,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55016, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_980031118_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742480_1662, duration: 13364536
2015-11-16 08:14:25,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742480_1662, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 08:14:33,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742480_1662 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742480 for deletion
2015-11-16 08:14:33,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742478_1660 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742478 for deletion
2015-11-16 08:14:33,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742479_1661 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742479 for deletion
2015-11-16 08:14:33,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742480_1662 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742480
2015-11-16 08:14:33,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742478_1660 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742478
2015-11-16 08:14:33,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742479_1661 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742479
2015-11-16 08:19:35,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742481_1663 src: /192.168.54.130:55031 dest: /192.168.54.130:50010
2015-11-16 08:19:35,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55031, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_910504804_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742481_1663, duration: 36441636
2015-11-16 08:19:35,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742481_1663, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 08:19:35,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742482_1664 src: /192.168.54.130:55033 dest: /192.168.54.130:50010
2015-11-16 08:19:35,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55033, dest: /192.168.54.130:50010, bytes: 3521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_910504804_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742482_1664, duration: 4758407
2015-11-16 08:19:35,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742482_1664, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 08:19:35,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742483_1665 src: /192.168.54.130:55035 dest: /192.168.54.130:50010
2015-11-16 08:19:35,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55035, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_910504804_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742483_1665, duration: 11871610
2015-11-16 08:19:35,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742483_1665, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 08:19:45,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742481_1663 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742481 for deletion
2015-11-16 08:19:45,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742482_1664 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742482 for deletion
2015-11-16 08:19:45,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742483_1665 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742483 for deletion
2015-11-16 08:19:45,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742481_1663 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742481
2015-11-16 08:19:45,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742482_1664 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742482
2015-11-16 08:19:45,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742483_1665 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742483
2015-11-16 09:26:42,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742484_1666 src: /192.168.54.130:55135 dest: /192.168.54.130:50010
2015-11-16 09:26:42,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55135, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1107675494_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742484_1666, duration: 54363332
2015-11-16 09:26:42,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742484_1666, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 09:26:42,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742485_1667 src: /192.168.54.130:55137 dest: /192.168.54.130:50010
2015-11-16 09:26:42,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55137, dest: /192.168.54.130:50010, bytes: 3488, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1107675494_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742485_1667, duration: 5769651
2015-11-16 09:26:42,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742485_1667, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 09:26:42,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742486_1668 src: /192.168.54.130:55139 dest: /192.168.54.130:50010
2015-11-16 09:26:42,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55139, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1107675494_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742486_1668, duration: 12538406
2015-11-16 09:26:42,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742486_1668, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 09:26:51,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742484_1666 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742484 for deletion
2015-11-16 09:26:51,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742485_1667 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742485 for deletion
2015-11-16 09:26:51,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742486_1668 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742486 for deletion
2015-11-16 09:26:51,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742484_1666 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742484
2015-11-16 09:26:51,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742485_1667 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742485
2015-11-16 09:26:51,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742486_1668 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742486
2015-11-16 09:44:31,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742487_1669 src: /192.168.54.130:55168 dest: /192.168.54.130:50010
2015-11-16 09:44:31,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55168, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-62883409_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742487_1669, duration: 34357648
2015-11-16 09:44:31,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742487_1669, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 09:44:31,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742488_1670 src: /192.168.54.130:55170 dest: /192.168.54.130:50010
2015-11-16 09:44:31,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55170, dest: /192.168.54.130:50010, bytes: 3491, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-62883409_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742488_1670, duration: 6746830
2015-11-16 09:44:31,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742488_1670, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 09:44:31,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742489_1671 src: /192.168.54.130:55172 dest: /192.168.54.130:50010
2015-11-16 09:44:32,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55172, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-62883409_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742489_1671, duration: 11822068
2015-11-16 09:44:32,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742489_1671, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 09:44:39,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742487_1669 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742487 for deletion
2015-11-16 09:44:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742488_1670 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742488 for deletion
2015-11-16 09:44:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742489_1671 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742489 for deletion
2015-11-16 09:44:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742487_1669 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742487
2015-11-16 09:44:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742488_1670 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742488
2015-11-16 09:44:39,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1892222280-192.168.54.130-1445296806381 blk_1073742489_1671 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current/finalized/subdir0/subdir2/blk_1073742489
2015-11-16 10:21:47,469 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1892222280-192.168.54.130-1445296806381 Total blocks: 142, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-16 13:29:10,654 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1042ms
No GCs detected
2015-11-16 13:53:33,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6155825d6392,  containing 1 storage report(s), of which we sent 1. The reports had 142 total blocks and used 1 RPC(s). This took 1 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 13:53:33,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 13:57:54,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742490_1672 src: /192.168.54.130:55317 dest: /192.168.54.130:50010
2015-11-16 13:57:57,110 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-1892222280-192.168.54.130-1445296806381:blk_1073742490_1672 to mirror 192.168.54.131:50010: java.net.NoRouteToHostException: No route to host
2015-11-16 13:57:57,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1892222280-192.168.54.130-1445296806381:blk_1073742490_1672 received exception java.net.NoRouteToHostException: No route to host
2015-11-16 13:57:57,307 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:55317 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:57:57,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 src: /192.168.54.130:55319 dest: /192.168.54.130:50010
2015-11-16 13:57:57,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55319, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651697982_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673, duration: 15150342
2015-11-16 13:57:57,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-16 13:57:57,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742492_1674 src: /192.168.54.130:55320 dest: /192.168.54.130:50010
2015-11-16 13:58:00,079 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-1892222280-192.168.54.130-1445296806381:blk_1073742492_1674 to mirror 192.168.54.131:50010: java.net.NoRouteToHostException: No route to host
2015-11-16 13:58:00,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1892222280-192.168.54.130-1445296806381:blk_1073742492_1674 received exception java.net.NoRouteToHostException: No route to host
2015-11-16 13:58:00,079 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:55320 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:00,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 src: /192.168.54.130:55323 dest: /192.168.54.130:50010
2015-11-16 13:58:00,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55323, dest: /192.168.54.130:50010, bytes: 3573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651697982_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675, duration: 4341141
2015-11-16 13:58:00,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-16 13:58:00,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742494_1676 src: /192.168.54.130:55325 dest: /192.168.54.130:50010
2015-11-16 13:58:00,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 13:58:03,079 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=1}:Exception transfering block BP-1892222280-192.168.54.130-1445296806381:blk_1073742494_1676 to mirror 192.168.54.131:50010: java.net.NoRouteToHostException: No route to host
2015-11-16 13:58:03,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1892222280-192.168.54.130-1445296806381:blk_1073742494_1676 received exception java.net.NoRouteToHostException: No route to host
2015-11-16 13:58:03,080 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:55325 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:03,084 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:03,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 src: /192.168.54.130:55328 dest: /192.168.54.130:50010
2015-11-16 13:58:03,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55328, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1651697982_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677, duration: 2514271
2015-11-16 13:58:03,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-16 13:58:03,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2015-11-16 13:58:03,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 13:58:03,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:58:06,079 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:06,081 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:06,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 13:58:09,079 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:18,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:58:18,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 13:58:21,079 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:21,079 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:33,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:58:36,079 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:45,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:58:48,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:58:57,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:59:00,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:59:09,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:59:12,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:59:21,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:59:24,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:59:33,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:59:36,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 13:59:45,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 13:59:48,986 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:07:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:07:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:07:30,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:07:30,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:07:30,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:07:33,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:07:36,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:07:36,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:07:39,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:07:39,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:07:42,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:07:45,989 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:07:51,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:07:54,995 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:08:03,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:08:06,363 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:08:15,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:08:18,363 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:08:27,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:08:30,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:08:39,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:08:40,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:08:51,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:08:54,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:09:03,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:09:06,801 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:17:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:17:27,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:17:30,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:17:30,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:17:33,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:17:36,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:17:42,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:17:42,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:17:45,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:17:45,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:17:48,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:17:48,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:17:57,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:18:00,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:18:09,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:18:10,728 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:18:21,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:18:22,730 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:18:33,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:18:36,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:18:45,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:18:45,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:18:57,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:19:00,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:19:09,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:19:11,160 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:27:27,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:27:27,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:27:29,632 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:27:29,634 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:27:30,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:27:32,631 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:27:36,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:27:36,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:27:39,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:27:39,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:27:42,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:27:45,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:27:51,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:27:54,989 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:28:03,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:28:06,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:28:15,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:28:18,063 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:28:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:28:30,062 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:28:39,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:28:42,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:28:51,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:28:54,990 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:29:03,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:29:06,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:37:27,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:37:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:37:27,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:37:27,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:37:30,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:37:33,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:37:42,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:37:42,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:37:45,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:37:45,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:37:45,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:37:48,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:37:57,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:38:00,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:38:09,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:38:12,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:38:21,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:38:22,431 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:38:33,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:38:34,430 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:38:45,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:38:48,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:38:57,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:39:00,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:39:09,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:39:12,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:06,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742496_1678 src: /192.168.54.130:55871 dest: /192.168.54.130:50010
2015-11-16 14:43:09,567 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-1892222280-192.168.54.130-1445296806381:blk_1073742496_1678 to mirror 192.168.54.131:50010: java.net.NoRouteToHostException: No route to host
2015-11-16 14:43:09,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1892222280-192.168.54.130-1445296806381:blk_1073742496_1678 received exception java.net.NoRouteToHostException: No route to host
2015-11-16 14:43:09,568 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:55871 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:09,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 src: /192.168.54.130:55873 dest: /192.168.54.130:50010
2015-11-16 14:43:09,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55873, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1445090877_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679, duration: 15537495
2015-11-16 14:43:09,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-16 14:43:09,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742498_1680 src: /192.168.54.130:55874 dest: /192.168.54.130:50010
2015-11-16 14:43:12,567 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-1892222280-192.168.54.130-1445296806381:blk_1073742498_1680 to mirror 192.168.54.131:50010: java.net.NoRouteToHostException: No route to host
2015-11-16 14:43:12,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1892222280-192.168.54.130-1445296806381:blk_1073742498_1680 received exception java.net.NoRouteToHostException: No route to host
2015-11-16 14:43:12,567 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:55874 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:12,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 src: /192.168.54.130:55876 dest: /192.168.54.130:50010
2015-11-16 14:43:12,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55876, dest: /192.168.54.130:50010, bytes: 3573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1445090877_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681, duration: 13966042
2015-11-16 14:43:12,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-16 14:43:12,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742500_1682 src: /192.168.54.130:55877 dest: /192.168.54.130:50010
2015-11-16 14:43:12,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 
2015-11-16 14:43:15,571 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=1}:Exception transfering block BP-1892222280-192.168.54.130-1445296806381:blk_1073742500_1682 to mirror 192.168.54.131:50010: java.net.NoRouteToHostException: No route to host
2015-11-16 14:43:15,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1892222280-192.168.54.130-1445296806381:blk_1073742500_1682 received exception java.net.NoRouteToHostException: No route to host
2015-11-16 14:43:15,571 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:55877 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:15,575 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:15,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 src: /192.168.54.130:55880 dest: /192.168.54.130:50010
2015-11-16 14:43:15,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55880, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1445090877_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683, duration: 12301580
2015-11-16 14:43:15,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-16 14:43:15,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:43:18,572 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:18,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 
2015-11-16 14:43:21,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:24,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 
2015-11-16 14:43:24,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:43:27,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:27,572 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:27,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 
2015-11-16 14:43:30,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:39,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:43:42,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:43:51,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:43:54,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:44:03,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:44:06,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:44:15,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:44:18,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:44:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:44:30,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:44:39,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:44:42,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:44:51,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:44:54,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:46:40,062 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.SocketTimeoutException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.54.130:57801 remote=hadoopmaster/192.168.54.130:9000]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.54.130:57801 remote=hadoopmaster/192.168.54.130:9000]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-16 14:46:41,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:42,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:43,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:44,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:45,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:46,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:47,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:48,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:49,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:50,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:50,084 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:46:51,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:52,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:53,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:54,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:55,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:56,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:57,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:58,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:46:59,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:00,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:00,094 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:47:01,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:02,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:03,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:04,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:05,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:06,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:07,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:08,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:09,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:10,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:10,103 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:47:11,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:12,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:13,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:14,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:15,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:16,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:17,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:18,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:19,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:20,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:20,110 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:47:21,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:22,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:23,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:24,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:25,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:26,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:27,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:28,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:29,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:30,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:30,120 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:47:31,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:32,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:33,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:34,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:35,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:36,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:37,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:38,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:39,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:40,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:40,128 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:47:41,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:42,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:43,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:44,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:47:44,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:47:44,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:47:44,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:47:47,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:47:47,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:47:47,476 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:47:48,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:47:50,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:47:53,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:47:53,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 
2015-11-16 14:47:56,613 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:47:56,615 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742495_1677 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:47:56,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 
2015-11-16 14:47:59,612 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742491_1673 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:48:12,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:48:15,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:48:24,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:48:27,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:48:36,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:48:39,984 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:48:48,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:48:51,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:49:00,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:49:03,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:49:12,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 
2015-11-16 14:49:15,985 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742493_1675 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:52:27,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 
2015-11-16 14:52:27,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:52:30,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:52:30,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:52:33,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 
2015-11-16 14:52:36,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:52:39,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 
2015-11-16 14:52:39,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:52:39,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742501_1683 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:52:39,987 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:52:42,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 
2015-11-16 14:52:45,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742497_1679 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:52:54,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:52:57,986 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:53:06,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:53:09,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:53:18,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:53:21,984 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:53:30,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:53:33,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:53:42,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:53:45,986 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:53:54,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:53:57,985 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:54:06,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0) Starting thread to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 
2015-11-16 14:54:09,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0):Failed to transfer BP-1892222280-192.168.54.130-1445296806381:blk_1073742499_1681 to 192.168.54.131:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 14:55:33,989 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-16 14:55:37,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:38,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:39,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:40,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:41,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:42,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:43,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:44,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:45,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:46,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:46,995 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:55:47,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:48,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:49,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:50,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:51,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:53,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:54,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:55,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:56,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:57,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:57,006 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 14:55:58,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:55:59,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:56:00,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:56:01,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 14:56:01,288 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-16 14:56:01,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-11-16 14:58:43,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-commons-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-core-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-graph-0.7.0.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-16 14:58:43,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-16 14:58:44,122 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-11-16 14:58:44,662 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-16 14:58:44,844 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-16 14:58:44,844 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-16 14:58:44,919 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-16 14:58:44,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-11-16 14:58:45,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-16 14:58:45,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-16 14:58:45,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-16 14:58:45,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-16 14:58:45,262 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-16 14:58:45,284 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-16 14:58:45,297 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-16 14:58:45,311 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-16 14:58:45,322 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-16 14:58:45,322 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-16 14:58:45,322 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-16 14:58:45,350 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49330
2015-11-16 14:58:45,350 INFO org.mortbay.log: jetty-6.1.26
2015-11-16 14:58:45,649 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49330
2015-11-16 14:58:45,840 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-16 14:58:45,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-11-16 14:58:45,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-16 14:58:45,960 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-16 14:58:45,997 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-16 14:58:46,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-16 14:58:46,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-16 14:58:46,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-16 14:58:46,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-11-16 14:58:46,153 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-16 14:58:46,155 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-16 14:58:46,569 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 10389@hadoopmaster
2015-11-16 14:58:46,715 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1892222280-192.168.54.130-1445296806381
2015-11-16 14:58:46,715 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381
2015-11-16 14:58:46,716 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-16 14:58:46,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=693764022;bpid=BP-1892222280-192.168.54.130-1445296806381;lv=-56;nsInfo=lv=-63;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0;bpid=BP-1892222280-192.168.54.130-1445296806381;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-11-16 14:58:46,954 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-11-16 14:58:46,954 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-11-16 14:58:46,972 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-16 14:58:46,973 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 14:58:46,973 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-16 14:58:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current: 8835072
2015-11-16 14:58:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1892222280-192.168.54.130-1445296806381 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 16ms
2015-11-16 14:58:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1892222280-192.168.54.130-1445296806381: 18ms
2015-11-16 14:58:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-16 14:58:47,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 29ms
2015-11-16 14:58:47,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 29ms
2015-11-16 14:58:47,344 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode, after more than 504 hour(s)
2015-11-16 14:58:47,351 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447736089351 with interval 21600000
2015-11-16 14:58:47,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-11-16 14:58:47,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-11-16 14:58:47,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-16 14:58:47,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=6696
2015-11-16 14:58:47,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-11-16 14:58:47,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x64e4b8ff0ec5,  containing 1 storage report(s), of which we sent 1. The reports had 154 total blocks and used 1 RPC(s). This took 10 msec to generate and 105 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 14:58:47,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 14:59:24,013 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): finished scanning block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 14:59:24,021 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1814363323 ms.
2015-11-16 15:37:13,145 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-16 15:37:17,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:18,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:19,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:20,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:21,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:22,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:23,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:24,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:25,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:26,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:26,142 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 15:37:27,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:28,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:29,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:30,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:31,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:32,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:33,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:34,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:35,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:36,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:36,155 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 15:37:37,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:38,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:39,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:40,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:41,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:42,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:43,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:44,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:45,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:46,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:46,168 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 15:37:47,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:48,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:49,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:50,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:51,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:52,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:53,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:54,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:55,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:56,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:56,182 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 15:37:57,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:58,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:37:59,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:00,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:01,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:02,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:03,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:04,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:05,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:06,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:06,194 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 15:38:07,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:08,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:09,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:10,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:11,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:12,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:13,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 15:38:13,228 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-16 15:38:13,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-11-16 15:46:02,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-commons-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-core-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-graph-0.7.0.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-16 15:46:02,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-16 15:46:03,064 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-11-16 15:46:03,530 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-16 15:46:03,663 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-16 15:46:03,663 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-16 15:46:03,678 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-16 15:46:03,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-11-16 15:46:03,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-16 15:46:03,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-16 15:46:03,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-16 15:46:03,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-16 15:46:03,954 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-16 15:46:03,967 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-16 15:46:03,982 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-16 15:46:03,991 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-16 15:46:04,001 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-16 15:46:04,001 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-16 15:46:04,001 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-16 15:46:04,024 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45285
2015-11-16 15:46:04,024 INFO org.mortbay.log: jetty-6.1.26
2015-11-16 15:46:04,280 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45285
2015-11-16 15:46:04,433 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-16 15:46:04,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-11-16 15:46:04,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-16 15:46:04,526 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-16 15:46:04,556 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-16 15:46:04,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-16 15:46:04,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-16 15:46:04,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-16 15:46:04,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-11-16 15:46:04,700 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-16 15:46:04,700 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-16 15:46:05,044 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 2850@hadoopmaster
2015-11-16 15:46:05,154 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1892222280-192.168.54.130-1445296806381
2015-11-16 15:46:05,154 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381
2015-11-16 15:46:05,155 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-16 15:46:05,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=693764022;bpid=BP-1892222280-192.168.54.130-1445296806381;lv=-56;nsInfo=lv=-63;cid=CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc;nsid=693764022;c=0;bpid=BP-1892222280-192.168.54.130-1445296806381;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-11-16 15:46:05,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-11-16 15:46:05,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-11-16 15:46:05,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-16 15:46:05,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 15:46:05,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-16 15:46:05,288 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1892222280-192.168.54.130-1445296806381/current: 8835072
2015-11-16 15:46:05,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1892222280-192.168.54.130-1445296806381 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 7ms
2015-11-16 15:46:05,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1892222280-192.168.54.130-1445296806381: 9ms
2015-11-16 15:46:05,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-16 15:46:05,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1892222280-192.168.54.130-1445296806381 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 34ms
2015-11-16 15:46:05,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 34ms
2015-11-16 15:46:05,566 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1811561778 ms.
2015-11-16 15:46:05,569 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447723890569 with interval 21600000
2015-11-16 15:46:05,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-11-16 15:46:05,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-11-16 15:46:05,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-16 15:46:05,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=6697
2015-11-16 15:46:05,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1892222280-192.168.54.130-1445296806381 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-11-16 15:46:05,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x20b53f40a0,  containing 1 storage report(s), of which we sent 1. The reports had 154 total blocks and used 1 RPC(s). This took 7 msec to generate and 56 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 15:46:05,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1892222280-192.168.54.130-1445296806381
2015-11-16 16:42:52,687 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-16 16:42:56,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:42:57,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:42:58,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:42:59,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:00,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:01,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:02,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:03,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:04,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:05,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:05,697 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 16:43:06,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:07,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:08,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:09,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:10,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:11,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:12,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:13,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:14,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:15,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:15,712 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 16:43:16,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:17,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:18,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:19,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:20,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:21,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:22,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:23,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:24,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:25,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:25,736 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 16:43:26,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:27,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:28,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:29,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:30,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:31,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:32,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:33,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:34,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:35,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:35,750 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 16:43:36,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:37,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:38,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:39,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:40,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:41,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:42,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:43,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:44,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:45,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:45,763 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-16 16:43:46,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:47,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:48,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:49,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-16 16:43:50,640 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-16 16:43:50,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-11-16 16:47:17,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-commons-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-core-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-graph-0.7.0.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-16 16:47:17,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-16 16:47:17,683 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-11-16 16:47:18,105 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-16 16:47:18,235 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-16 16:47:18,235 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-16 16:47:18,248 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-16 16:47:18,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-11-16 16:47:18,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-16 16:47:18,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-16 16:47:18,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-16 16:47:18,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-16 16:47:18,514 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-16 16:47:18,531 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-16 16:47:18,543 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-16 16:47:18,557 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-16 16:47:18,564 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-16 16:47:18,564 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-16 16:47:18,564 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-16 16:47:18,585 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41263
2015-11-16 16:47:18,585 INFO org.mortbay.log: jetty-6.1.26
2015-11-16 16:47:18,839 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41263
2015-11-16 16:47:18,947 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-16 16:47:18,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-11-16 16:47:18,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-16 16:47:19,033 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-16 16:47:19,054 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-16 16:47:19,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-16 16:47:19,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-16 16:47:19,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-16 16:47:19,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-11-16 16:47:19,211 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-16 16:47:19,212 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-16 16:47:19,568 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 5908@hadoopmaster
2015-11-16 16:47:19,569 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop-2.7.1/hdfs/datanode: namenode clusterID = CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59; datanode clusterID = CID-9a3aa7b0-b7b8-40c9-89a9-46c7d6b3d7fc
2015-11-16 16:47:19,570 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-11-16 16:47:19,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000
2015-11-16 16:47:19,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-11-16 16:47:21,581 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-11-16 16:47:21,583 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-11-16 16:47:21,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-11-16 16:52:16,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-commons-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-core-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-graph-0.7.0.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-16 16:52:16,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-16 16:52:16,824 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-11-16 16:52:17,256 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-16 16:52:17,379 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-16 16:52:17,379 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-16 16:52:17,389 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-16 16:52:17,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-11-16 16:52:17,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-16 16:52:17,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-16 16:52:17,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-16 16:52:17,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-16 16:52:17,650 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-16 16:52:17,671 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-16 16:52:17,682 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-16 16:52:17,698 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-16 16:52:17,709 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-16 16:52:17,710 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-16 16:52:17,710 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-16 16:52:17,730 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36704
2015-11-16 16:52:17,730 INFO org.mortbay.log: jetty-6.1.26
2015-11-16 16:52:18,001 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36704
2015-11-16 16:52:18,100 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-16 16:52:18,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-11-16 16:52:18,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-16 16:52:18,205 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-16 16:52:18,228 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-16 16:52:18,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-16 16:52:18,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-16 16:52:18,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-16 16:52:18,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-11-16 16:52:18,391 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-16 16:52:18,392 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-16 16:52:18,600 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 6153@hadoopmaster
2015-11-16 16:52:18,703 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-883961033-192.168.54.130-1447721127864
2015-11-16 16:52:18,704 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864
2015-11-16 16:52:18,704 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864 is not formatted for BP-883961033-192.168.54.130-1447721127864
2015-11-16 16:52:18,704 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-11-16 16:52:18,704 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-883961033-192.168.54.130-1447721127864 directory /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current
2015-11-16 16:52:18,705 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-16 16:52:18,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=739049323;bpid=BP-883961033-192.168.54.130-1447721127864;lv=-56;nsInfo=lv=-63;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0;bpid=BP-883961033-192.168.54.130-1447721127864;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-11-16 16:52:18,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-11-16 16:52:18,811 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-11-16 16:52:18,826 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-16 16:52:18,826 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-883961033-192.168.54.130-1447721127864
2015-11-16 16:52:18,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-883961033-192.168.54.130-1447721127864 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-16 16:52:18,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-883961033-192.168.54.130-1447721127864 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 25ms
2015-11-16 16:52:18,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-883961033-192.168.54.130-1447721127864: 30ms
2015-11-16 16:52:18,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-883961033-192.168.54.130-1447721127864 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-16 16:52:18,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-883961033-192.168.54.130-1447721127864 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 0ms
2015-11-16 16:52:18,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-16 16:52:18,989 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-883961033-192.168.54.130-1447721127864 on volume /usr/local/hadoop-2.7.1/hdfs/datanode
2015-11-16 16:52:18,991 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): finished scanning block pool BP-883961033-192.168.54.130-1447721127864
2015-11-16 16:52:18,997 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447727516997 with interval 21600000
2015-11-16 16:52:19,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-883961033-192.168.54.130-1447721127864 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-11-16 16:52:19,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-883961033-192.168.54.130-1447721127864 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-11-16 16:52:19,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-16 16:52:19,087 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1814399901 ms.
2015-11-16 16:52:19,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-883961033-192.168.54.130-1447721127864 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=3
2015-11-16 16:52:19,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-883961033-192.168.54.130-1447721127864 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-11-16 16:52:19,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3bdd9c9fede,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 56 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 16:52:19,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-883961033-192.168.54.130-1447721127864
2015-11-16 17:11:06,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4c44c707286,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 17:11:06,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-883961033-192.168.54.130-1447721127864
2015-11-16 17:17:47,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741825_1001 src: /192.168.54.130:47844 dest: /192.168.54.130:50010
2015-11-16 17:17:47,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47844, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483757915_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741825_1001, duration: 85998959
2015-11-16 17:17:47,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:17:48,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741826_1002 src: /192.168.54.130:47846 dest: /192.168.54.130:50010
2015-11-16 17:17:48,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47846, dest: /192.168.54.130:50010, bytes: 3573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483757915_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741826_1002, duration: 5997187
2015-11-16 17:17:48,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:17:48,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741827_1003 src: /192.168.54.130:47848 dest: /192.168.54.130:50010
2015-11-16 17:17:48,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47848, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483757915_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741827_1003, duration: 18673676
2015-11-16 17:17:48,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:19:39,386 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-11-16 17:19:39,387 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2015-11-16 17:19:39,387 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2015-11-16 17:19:39,388 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741825_1001 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741825
2015-11-16 17:19:39,388 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741826_1002 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741826
2015-11-16 17:19:39,388 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741827_1003 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741827
2015-11-16 17:21:22,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741828_1004 src: /192.168.54.130:47862 dest: /192.168.54.130:50010
2015-11-16 17:21:22,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47862, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1567422608_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741828_1004, duration: 37307447
2015-11-16 17:21:22,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:21:22,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741829_1005 src: /192.168.54.130:47864 dest: /192.168.54.130:50010
2015-11-16 17:21:22,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47864, dest: /192.168.54.130:50010, bytes: 3573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1567422608_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741829_1005, duration: 16920330
2015-11-16 17:21:22,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:21:22,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741830_1006 src: /192.168.54.130:47866 dest: /192.168.54.130:50010
2015-11-16 17:21:22,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47866, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1567422608_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741830_1006, duration: 12472806
2015-11-16 17:21:22,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:21:30,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2015-11-16 17:21:30,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-11-16 17:21:30,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-11-16 17:21:30,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741828_1004 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741828
2015-11-16 17:21:30,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741829_1005 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741829
2015-11-16 17:21:30,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741830_1006 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741830
2015-11-16 17:29:23,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741831_1007 src: /192.168.54.130:47884 dest: /192.168.54.130:50010
2015-11-16 17:29:23,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47884, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136088352_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741831_1007, duration: 38118350
2015-11-16 17:29:23,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:29:23,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741832_1008 src: /192.168.54.130:47886 dest: /192.168.54.130:50010
2015-11-16 17:29:23,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47886, dest: /192.168.54.130:50010, bytes: 3573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136088352_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741832_1008, duration: 5783026
2015-11-16 17:29:23,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:29:23,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741833_1009 src: /192.168.54.130:47888 dest: /192.168.54.130:50010
2015-11-16 17:29:23,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47888, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136088352_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741833_1009, duration: 10232671
2015-11-16 17:29:23,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741833_1009, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:29:33,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-11-16 17:29:33,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-11-16 17:29:33,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-11-16 17:29:33,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741831_1007 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741831
2015-11-16 17:29:33,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741832_1008 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741832
2015-11-16 17:29:33,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741833_1009 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741833
2015-11-16 17:36:38,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741834_1010 src: /192.168.54.130:47903 dest: /192.168.54.130:50010
2015-11-16 17:36:38,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47903, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2100134948_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741834_1010, duration: 36021011
2015-11-16 17:36:38,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741834_1010, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:36:38,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741835_1011 src: /192.168.54.130:47905 dest: /192.168.54.130:50010
2015-11-16 17:36:38,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47905, dest: /192.168.54.130:50010, bytes: 3573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2100134948_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741835_1011, duration: 8292993
2015-11-16 17:36:38,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741835_1011, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:36:38,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741836_1012 src: /192.168.54.130:47907 dest: /192.168.54.130:50010
2015-11-16 17:36:38,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47907, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2100134948_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741836_1012, duration: 14861461
2015-11-16 17:36:38,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741836_1012, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:36:51,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-11-16 17:36:51,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2015-11-16 17:36:51,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-11-16 17:36:51,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741834_1010 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741834
2015-11-16 17:36:51,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741835_1011 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741835
2015-11-16 17:36:51,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741836_1012 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741836
2015-11-16 17:42:25,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741837_1013 src: /192.168.54.130:47921 dest: /192.168.54.130:50010
2015-11-16 17:42:25,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47921, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-175922118_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741837_1013, duration: 31785186
2015-11-16 17:42:25,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741837_1013, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:42:25,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741838_1014 src: /192.168.54.130:47923 dest: /192.168.54.130:50010
2015-11-16 17:42:25,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47923, dest: /192.168.54.130:50010, bytes: 3573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-175922118_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741838_1014, duration: 5194974
2015-11-16 17:42:25,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741838_1014, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:42:25,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741839_1015 src: /192.168.54.130:47925 dest: /192.168.54.130:50010
2015-11-16 17:42:25,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47925, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-175922118_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741839_1015, duration: 11919286
2015-11-16 17:42:25,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741839_1015, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:42:33,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-11-16 17:42:33,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-11-16 17:42:33,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-11-16 17:42:33,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741837_1013 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741837
2015-11-16 17:42:33,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741838_1014 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741838
2015-11-16 17:42:33,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741839_1015 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741839
2015-11-16 17:58:52,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741840_1016 src: /192.168.54.130:47953 dest: /192.168.54.130:50010
2015-11-16 17:58:52,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47953, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-624785508_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741840_1016, duration: 36495940
2015-11-16 17:58:52,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741840_1016, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:58:52,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741841_1017 src: /192.168.54.130:47955 dest: /192.168.54.130:50010
2015-11-16 17:58:52,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47955, dest: /192.168.54.130:50010, bytes: 3757, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-624785508_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741841_1017, duration: 5557199
2015-11-16 17:58:52,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741841_1017, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:58:52,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741842_1018 src: /192.168.54.130:47957 dest: /192.168.54.130:50010
2015-11-16 17:58:52,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47957, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-624785508_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741842_1018, duration: 10995611
2015-11-16 17:58:52,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741842_1018, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 17:59:00,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-11-16 17:59:00,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-11-16 17:59:00,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-11-16 17:59:00,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741840_1016 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741840
2015-11-16 17:59:00,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741841_1017 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741841
2015-11-16 17:59:00,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741842_1018 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741842
2015-11-16 18:23:04,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741843_1019 src: /192.168.54.130:47989 dest: /192.168.54.130:50010
2015-11-16 18:23:04,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47989, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1447720279_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741843_1019, duration: 35651270
2015-11-16 18:23:04,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741843_1019, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:23:04,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741844_1020 src: /192.168.54.130:47991 dest: /192.168.54.130:50010
2015-11-16 18:23:04,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47991, dest: /192.168.54.130:50010, bytes: 3794, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1447720279_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741844_1020, duration: 5469604
2015-11-16 18:23:04,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741844_1020, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:23:04,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741845_1021 src: /192.168.54.130:47993 dest: /192.168.54.130:50010
2015-11-16 18:23:04,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:47993, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1447720279_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741845_1021, duration: 12777029
2015-11-16 18:23:04,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:23:09,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-11-16 18:23:09,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-11-16 18:23:09,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-11-16 18:23:09,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741843_1019 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741843
2015-11-16 18:23:09,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741844_1020 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741844
2015-11-16 18:23:09,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741845_1021 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741845
2015-11-16 18:24:17,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741846_1022 src: /192.168.54.130:48002 dest: /192.168.54.130:50010
2015-11-16 18:24:17,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48002, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1970525735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741846_1022, duration: 31233216
2015-11-16 18:24:17,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:24:17,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741847_1023 src: /192.168.54.130:48004 dest: /192.168.54.130:50010
2015-11-16 18:24:17,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48004, dest: /192.168.54.130:50010, bytes: 3794, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1970525735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741847_1023, duration: 7115490
2015-11-16 18:24:17,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741847_1023, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:24:17,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741848_1024 src: /192.168.54.130:48006 dest: /192.168.54.130:50010
2015-11-16 18:24:17,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48006, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1970525735_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741848_1024, duration: 10917795
2015-11-16 18:24:17,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741848_1024, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:24:27,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-11-16 18:24:27,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-11-16 18:24:27,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-11-16 18:24:27,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741846_1022 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741846
2015-11-16 18:24:27,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741847_1023 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741847
2015-11-16 18:24:27,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741848_1024 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741848
2015-11-16 18:24:56,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741849_1025 src: /192.168.54.130:48015 dest: /192.168.54.130:50010
2015-11-16 18:24:56,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48015, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_321207349_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741849_1025, duration: 30555378
2015-11-16 18:24:56,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741849_1025, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:24:56,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741850_1026 src: /192.168.54.130:48017 dest: /192.168.54.130:50010
2015-11-16 18:24:56,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48017, dest: /192.168.54.130:50010, bytes: 3794, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_321207349_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741850_1026, duration: 5207223
2015-11-16 18:24:56,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741850_1026, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:24:56,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741851_1027 src: /192.168.54.130:48019 dest: /192.168.54.130:50010
2015-11-16 18:24:56,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48019, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_321207349_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741851_1027, duration: 13684681
2015-11-16 18:24:56,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741851_1027, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 18:25:03,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2015-11-16 18:25:03,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741849_1025 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741849
2015-11-16 18:25:03,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2015-11-16 18:25:03,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741850_1026 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741850
2015-11-16 18:25:03,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2015-11-16 18:25:03,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741851_1027 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741851
2015-11-16 18:31:57,005 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-883961033-192.168.54.130-1447721127864 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-16 22:10:49,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741852_1028 src: /192.168.54.130:48992 dest: /192.168.54.130:50010
2015-11-16 22:10:49,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48992, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1632782573_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741852_1028, duration: 39582163
2015-11-16 22:10:49,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741852_1028, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 22:10:49,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741853_1029 src: /192.168.54.130:48994 dest: /192.168.54.130:50010
2015-11-16 22:10:49,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48994, dest: /192.168.54.130:50010, bytes: 3794, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1632782573_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741853_1029, duration: 4861382
2015-11-16 22:10:49,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741853_1029, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 22:10:49,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741854_1030 src: /192.168.54.130:48996 dest: /192.168.54.130:50010
2015-11-16 22:10:49,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:48996, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1632782573_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741854_1030, duration: 14046564
2015-11-16 22:10:49,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741854_1030, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-16 22:10:55,041 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2015-11-16 22:10:55,041 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-11-16 22:10:55,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-11-16 22:10:55,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741852_1028 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741852
2015-11-16 22:10:55,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741853_1029 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741853
2015-11-16 22:10:55,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741854_1030 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741854
2015-11-19 12:03:42,288 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1429ms
No GCs detected
2015-11-19 13:56:49,923 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2735ms
No GCs detected
2015-11-19 15:46:17,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741855_1031 src: /192.168.54.130:49360 dest: /192.168.54.130:50010
2015-11-19 15:46:17,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49360, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_177007195_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741855_1031, duration: 70095791
2015-11-19 15:46:17,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741855_1031, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:46:17,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741856_1032 src: /192.168.54.130:49362 dest: /192.168.54.130:50010
2015-11-19 15:46:17,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49362, dest: /192.168.54.130:50010, bytes: 4155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_177007195_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741856_1032, duration: 7333744
2015-11-19 15:46:17,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741856_1032, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:46:17,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741857_1033 src: /192.168.54.130:49364 dest: /192.168.54.130:50010
2015-11-19 15:46:17,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49364, dest: /192.168.54.130:50010, bytes: 100517, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_177007195_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741857_1033, duration: 17294787
2015-11-19 15:46:17,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741857_1033, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:46:24,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2015-11-19 15:46:24,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2015-11-19 15:46:24,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2015-11-19 15:46:24,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741856_1032 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741856
2015-11-19 15:46:24,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741857_1033 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741857
2015-11-19 15:46:24,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741855_1031 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741855
2015-11-19 15:47:08,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741858_1034 src: /192.168.54.130:49373 dest: /192.168.54.130:50010
2015-11-19 15:47:08,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49373, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-126684452_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741858_1034, duration: 39214807
2015-11-19 15:47:08,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:47:08,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741859_1035 src: /192.168.54.130:49375 dest: /192.168.54.130:50010
2015-11-19 15:47:08,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49375, dest: /192.168.54.130:50010, bytes: 4155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-126684452_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741859_1035, duration: 5259381
2015-11-19 15:47:08,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741859_1035, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:47:09,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741860_1036 src: /192.168.54.130:49377 dest: /192.168.54.130:50010
2015-11-19 15:47:09,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49377, dest: /192.168.54.130:50010, bytes: 100517, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-126684452_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741860_1036, duration: 12898923
2015-11-19 15:47:09,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741860_1036, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:47:15,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2015-11-19 15:47:15,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741858_1034 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741858
2015-11-19 15:47:15,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2015-11-19 15:47:15,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741859_1035 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741859
2015-11-19 15:47:15,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2015-11-19 15:47:15,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741860_1036 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741860
2015-11-19 15:50:57,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741861_1037 src: /192.168.54.130:49390 dest: /192.168.54.130:50010
2015-11-19 15:50:57,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49390, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1500462760_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741861_1037, duration: 36706564
2015-11-19 15:50:57,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741861_1037, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:50:57,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741862_1038 src: /192.168.54.130:49392 dest: /192.168.54.130:50010
2015-11-19 15:50:57,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49392, dest: /192.168.54.130:50010, bytes: 4155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1500462760_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741862_1038, duration: 10533031
2015-11-19 15:50:58,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741862_1038, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:50:58,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741863_1039 src: /192.168.54.130:49394 dest: /192.168.54.130:50010
2015-11-19 15:50:58,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49394, dest: /192.168.54.130:50010, bytes: 100517, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1500462760_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741863_1039, duration: 13667473
2015-11-19 15:50:58,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741863_1039, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 15:51:03,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2015-11-19 15:51:03,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2015-11-19 15:51:03,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2015-11-19 15:51:03,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741861_1037 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741861
2015-11-19 15:51:03,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741862_1038 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741862
2015-11-19 15:51:03,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741863_1039 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741863
2015-11-19 17:27:51,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x186970b5986b,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 17:27:51,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-883961033-192.168.54.130-1447721127864
2015-11-19 18:48:42,609 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-883961033-192.168.54.130-1447721127864 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-19 19:55:25,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741864_1040 src: /192.168.54.130:49721 dest: /192.168.54.130:50010
2015-11-19 19:55:25,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49721, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1853531419_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741864_1040, duration: 34531194
2015-11-19 19:55:25,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741864_1040, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 19:55:25,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741865_1041 src: /192.168.54.130:49723 dest: /192.168.54.130:50010
2015-11-19 19:55:25,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49723, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1853531419_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741865_1041, duration: 3051190
2015-11-19 19:55:25,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741865_1041, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 19:55:25,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741866_1042 src: /192.168.54.130:49725 dest: /192.168.54.130:50010
2015-11-19 19:55:25,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49725, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1853531419_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741866_1042, duration: 12026422
2015-11-19 19:55:25,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741866_1042, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 19:55:33,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2015-11-19 19:55:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2015-11-19 19:55:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2015-11-19 19:55:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741864_1040 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741864
2015-11-19 19:55:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741865_1041 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741865
2015-11-19 19:55:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741866_1042 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741866
2015-11-19 19:59:38,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741867_1043 src: /192.168.54.130:49738 dest: /192.168.54.130:50010
2015-11-19 19:59:38,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49738, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1549499375_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741867_1043, duration: 35003812
2015-11-19 19:59:38,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741867_1043, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 19:59:38,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741868_1044 src: /192.168.54.130:49740 dest: /192.168.54.130:50010
2015-11-19 19:59:38,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49740, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1549499375_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741868_1044, duration: 11043287
2015-11-19 19:59:38,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741868_1044, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 19:59:38,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741869_1045 src: /192.168.54.130:49742 dest: /192.168.54.130:50010
2015-11-19 19:59:38,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49742, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1549499375_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741869_1045, duration: 12552788
2015-11-19 19:59:38,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 19:59:45,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2015-11-19 19:59:45,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2015-11-19 19:59:45,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2015-11-19 19:59:45,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741867_1043 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741867
2015-11-19 19:59:45,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741868_1044 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741868
2015-11-19 19:59:45,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741869_1045 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741869
2015-11-19 20:13:51,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741870_1046 src: /192.168.54.130:49770 dest: /192.168.54.130:50010
2015-11-19 20:13:51,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49770, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1092926325_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741870_1046, duration: 32010109
2015-11-19 20:13:51,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741870_1046, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:13:51,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741871_1047 src: /192.168.54.130:49772 dest: /192.168.54.130:50010
2015-11-19 20:13:51,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49772, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1092926325_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741871_1047, duration: 5571881
2015-11-19 20:13:51,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741871_1047, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:13:51,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741872_1048 src: /192.168.54.130:49774 dest: /192.168.54.130:50010
2015-11-19 20:13:51,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49774, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1092926325_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741872_1048, duration: 12858235
2015-11-19 20:13:51,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741872_1048, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:13:57,977 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2015-11-19 20:13:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2015-11-19 20:13:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2015-11-19 20:13:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741872_1048 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741872
2015-11-19 20:13:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741870_1046 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741870
2015-11-19 20:13:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741871_1047 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741871
2015-11-19 20:20:00,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741873_1049 src: /192.168.54.130:49789 dest: /192.168.54.130:50010
2015-11-19 20:20:00,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49789, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-550005020_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741873_1049, duration: 34015643
2015-11-19 20:20:00,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741873_1049, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:20:00,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741874_1050 src: /192.168.54.130:49791 dest: /192.168.54.130:50010
2015-11-19 20:20:00,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49791, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-550005020_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741874_1050, duration: 5425915
2015-11-19 20:20:00,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741874_1050, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:20:00,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741875_1051 src: /192.168.54.130:49793 dest: /192.168.54.130:50010
2015-11-19 20:20:00,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49793, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-550005020_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741875_1051, duration: 10770725
2015-11-19 20:20:00,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741875_1051, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:20:06,977 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2015-11-19 20:20:06,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2015-11-19 20:20:06,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2015-11-19 20:20:06,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741873_1049 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741873
2015-11-19 20:20:06,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741874_1050 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741874
2015-11-19 20:20:06,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741875_1051 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741875
2015-11-19 20:20:52,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741876_1052 src: /192.168.54.130:49803 dest: /192.168.54.130:50010
2015-11-19 20:20:53,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49803, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1736827027_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741876_1052, duration: 32308554
2015-11-19 20:20:53,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741876_1052, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:20:53,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741877_1053 src: /192.168.54.130:49805 dest: /192.168.54.130:50010
2015-11-19 20:20:53,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49805, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1736827027_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741877_1053, duration: 5136581
2015-11-19 20:20:53,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741877_1053, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:20:53,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741878_1054 src: /192.168.54.130:49807 dest: /192.168.54.130:50010
2015-11-19 20:20:53,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49807, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1736827027_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741878_1054, duration: 11808856
2015-11-19 20:20:53,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741878_1054, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:20:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2015-11-19 20:20:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2015-11-19 20:20:57,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2015-11-19 20:20:57,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741876_1052 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741876
2015-11-19 20:20:57,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741877_1053 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741877
2015-11-19 20:20:57,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741878_1054 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741878
2015-11-19 20:21:56,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741879_1055 src: /192.168.54.130:49816 dest: /192.168.54.130:50010
2015-11-19 20:21:56,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49816, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1965691437_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741879_1055, duration: 33746179
2015-11-19 20:21:56,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741879_1055, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:21:56,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741880_1056 src: /192.168.54.130:49818 dest: /192.168.54.130:50010
2015-11-19 20:21:56,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49818, dest: /192.168.54.130:50010, bytes: 3794, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1965691437_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741880_1056, duration: 5147436
2015-11-19 20:21:56,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741880_1056, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:21:56,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741881_1057 src: /192.168.54.130:49820 dest: /192.168.54.130:50010
2015-11-19 20:21:56,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49820, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1965691437_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741881_1057, duration: 10358615
2015-11-19 20:21:56,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741881_1057, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:22:00,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2015-11-19 20:22:00,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2015-11-19 20:22:00,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2015-11-19 20:22:00,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741879_1055 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741879
2015-11-19 20:22:00,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741880_1056 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741880
2015-11-19 20:22:00,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741881_1057 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741881
2015-11-19 20:35:22,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741882_1058 src: /192.168.54.130:49845 dest: /192.168.54.130:50010
2015-11-19 20:35:22,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49845, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1005794609_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741882_1058, duration: 48996228
2015-11-19 20:35:22,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741882_1058, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:35:22,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741883_1059 src: /192.168.54.130:49847 dest: /192.168.54.130:50010
2015-11-19 20:35:22,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49847, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1005794609_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741883_1059, duration: 12410530
2015-11-19 20:35:22,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741883_1059, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:35:23,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741884_1060 src: /192.168.54.130:49849 dest: /192.168.54.130:50010
2015-11-19 20:35:23,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49849, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1005794609_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741884_1060, duration: 14585717
2015-11-19 20:35:23,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741884_1060, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:35:45,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2015-11-19 20:35:45,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2015-11-19 20:35:45,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2015-11-19 20:35:45,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741882_1058 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741882
2015-11-19 20:35:45,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741883_1059 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741883
2015-11-19 20:35:45,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741884_1060 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741884
2015-11-19 20:37:03,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741885_1061 src: /192.168.54.130:49866 dest: /192.168.54.130:50010
2015-11-19 20:37:03,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49866, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-168403907_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741885_1061, duration: 42281704
2015-11-19 20:37:03,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741885_1061, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:37:03,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741886_1062 src: /192.168.54.130:49868 dest: /192.168.54.130:50010
2015-11-19 20:37:03,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49868, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-168403907_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741886_1062, duration: 5562301
2015-11-19 20:37:03,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741886_1062, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:37:03,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741887_1063 src: /192.168.54.130:49870 dest: /192.168.54.130:50010
2015-11-19 20:37:03,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49870, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-168403907_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741887_1063, duration: 13143728
2015-11-19 20:37:03,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741887_1063, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:37:06,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2015-11-19 20:37:06,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2015-11-19 20:37:06,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2015-11-19 20:37:06,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741885_1061 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741885
2015-11-19 20:37:06,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741886_1062 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741886
2015-11-19 20:37:06,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741887_1063 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741887
2015-11-19 20:38:00,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741888_1064 src: /192.168.54.130:49881 dest: /192.168.54.130:50010
2015-11-19 20:38:00,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49881, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-807208519_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741888_1064, duration: 30008408
2015-11-19 20:38:00,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741888_1064, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:38:00,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741889_1065 src: /192.168.54.130:49883 dest: /192.168.54.130:50010
2015-11-19 20:38:00,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49883, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-807208519_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741889_1065, duration: 10187318
2015-11-19 20:38:00,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741889_1065, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:38:00,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741890_1066 src: /192.168.54.130:49885 dest: /192.168.54.130:50010
2015-11-19 20:38:00,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:49885, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-807208519_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741890_1066, duration: 10938768
2015-11-19 20:38:00,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741890_1066, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-19 20:38:09,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2015-11-19 20:38:09,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2015-11-19 20:38:09,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2015-11-19 20:38:09,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741888_1064 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741888
2015-11-19 20:38:09,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741889_1065 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741889
2015-11-19 20:38:09,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741890_1066 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741890
2015-11-21 11:47:38,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741891_1067 src: /192.168.54.130:50002 dest: /192.168.54.130:50010
2015-11-21 11:47:38,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50002, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-226199085_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741891_1067, duration: 36841986
2015-11-21 11:47:38,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741891_1067, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 11:47:38,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741892_1068 src: /192.168.54.130:50004 dest: /192.168.54.130:50010
2015-11-21 11:47:38,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50004, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-226199085_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741892_1068, duration: 5329940
2015-11-21 11:47:38,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741892_1068, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 11:47:38,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741893_1069 src: /192.168.54.130:50006 dest: /192.168.54.130:50010
2015-11-21 11:47:38,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50006, dest: /192.168.54.130:50010, bytes: 100529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-226199085_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741893_1069, duration: 15219204
2015-11-21 11:47:38,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741893_1069, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 11:47:50,339 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2015-11-21 11:47:50,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2015-11-21 11:47:50,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2015-11-21 11:47:50,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741891_1067 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741891
2015-11-21 11:47:50,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741892_1068 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741892
2015-11-21 11:47:50,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741893_1069 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741893
2015-11-21 12:43:49,674 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1726ms
No GCs detected
2015-11-21 12:45:58,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741894_1070 src: /192.168.54.130:50121 dest: /192.168.54.130:50010
2015-11-21 12:45:58,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50121, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-455158102_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741894_1070, duration: 37423634
2015-11-21 12:45:58,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741894_1070, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:45:58,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741895_1071 src: /192.168.54.130:50123 dest: /192.168.54.130:50010
2015-11-21 12:45:58,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50123, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-455158102_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741895_1071, duration: 4763591
2015-11-21 12:45:58,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741895_1071, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:45:58,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741896_1072 src: /192.168.54.130:50125 dest: /192.168.54.130:50010
2015-11-21 12:45:58,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50125, dest: /192.168.54.130:50010, bytes: 100525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-455158102_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741896_1072, duration: 24631991
2015-11-21 12:45:58,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741896_1072, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:46:06,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2015-11-21 12:46:06,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2015-11-21 12:46:06,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2015-11-21 12:46:06,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741894_1070 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741894
2015-11-21 12:46:06,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741895_1071 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741895
2015-11-21 12:46:06,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741896_1072 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741896
2015-11-21 12:49:02,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741897_1073 src: /192.168.54.130:50136 dest: /192.168.54.130:50010
2015-11-21 12:49:02,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50136, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_311337824_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741897_1073, duration: 35598152
2015-11-21 12:49:02,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741897_1073, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:49:02,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741898_1074 src: /192.168.54.130:50138 dest: /192.168.54.130:50010
2015-11-21 12:49:02,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50138, dest: /192.168.54.130:50010, bytes: 4155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_311337824_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741898_1074, duration: 8318082
2015-11-21 12:49:02,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741898_1074, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:49:02,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741899_1075 src: /192.168.54.130:50140 dest: /192.168.54.130:50010
2015-11-21 12:49:02,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50140, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_311337824_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741899_1075, duration: 15247025
2015-11-21 12:49:02,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741899_1075, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:49:12,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741897_1073 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741897 for deletion
2015-11-21 12:49:12,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741898_1074 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741898 for deletion
2015-11-21 12:49:12,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2015-11-21 12:49:12,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741897_1073 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741897
2015-11-21 12:49:12,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741898_1074 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741898
2015-11-21 12:49:12,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741899_1075 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741899
2015-11-21 12:53:44,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741900_1076 src: /192.168.54.130:50154 dest: /192.168.54.130:50010
2015-11-21 12:53:44,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50154, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1511178040_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741900_1076, duration: 35163475
2015-11-21 12:53:44,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741900_1076, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:53:44,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741901_1077 src: /192.168.54.130:50156 dest: /192.168.54.130:50010
2015-11-21 12:53:44,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50156, dest: /192.168.54.130:50010, bytes: 3794, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1511178040_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741901_1077, duration: 5552102
2015-11-21 12:53:44,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741901_1077, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:53:45,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741902_1078 src: /192.168.54.130:50158 dest: /192.168.54.130:50010
2015-11-21 12:53:45,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50158, dest: /192.168.54.130:50010, bytes: 100505, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1511178040_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741902_1078, duration: 17958209
2015-11-21 12:53:45,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741902_1078, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 12:53:54,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2015-11-21 12:53:54,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2015-11-21 12:53:54,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2015-11-21 12:53:54,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741900_1076 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741900
2015-11-21 12:53:54,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741901_1077 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741901
2015-11-21 12:53:54,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741902_1078 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741902
2015-11-21 13:00:29,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741903_1079 src: /192.168.54.130:50174 dest: /192.168.54.130:50010
2015-11-21 13:00:29,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50174, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2112889553_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741903_1079, duration: 38467159
2015-11-21 13:00:29,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741903_1079, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:00:29,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741904_1080 src: /192.168.54.130:50176 dest: /192.168.54.130:50010
2015-11-21 13:00:29,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50176, dest: /192.168.54.130:50010, bytes: 4155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2112889553_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741904_1080, duration: 5880137
2015-11-21 13:00:29,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741904_1080, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:00:29,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741905_1081 src: /192.168.54.130:50178 dest: /192.168.54.130:50010
2015-11-21 13:00:29,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50178, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2112889553_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741905_1081, duration: 18381372
2015-11-21 13:00:29,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741905_1081, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:00:36,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741904_1080 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741904 for deletion
2015-11-21 13:00:36,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2015-11-21 13:00:36,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2015-11-21 13:00:36,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741904_1080 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741904
2015-11-21 13:00:36,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741905_1081 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741905
2015-11-21 13:00:36,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741903_1079 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741903
2015-11-21 13:02:11,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741906_1082 src: /192.168.54.130:50188 dest: /192.168.54.130:50010
2015-11-21 13:02:11,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50188, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_188014475_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741906_1082, duration: 38422504
2015-11-21 13:02:11,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741906_1082, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:02:11,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741907_1083 src: /192.168.54.130:50190 dest: /192.168.54.130:50010
2015-11-21 13:02:11,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50190, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_188014475_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741907_1083, duration: 4137291
2015-11-21 13:02:11,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741907_1083, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:02:11,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741908_1084 src: /192.168.54.130:50192 dest: /192.168.54.130:50010
2015-11-21 13:02:11,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50192, dest: /192.168.54.130:50010, bytes: 100525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_188014475_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741908_1084, duration: 15081405
2015-11-21 13:02:11,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741908_1084, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:02:21,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2015-11-21 13:02:21,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2015-11-21 13:02:21,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2015-11-21 13:02:21,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741906_1082 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741906
2015-11-21 13:02:21,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741907_1083 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741907
2015-11-21 13:02:21,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741908_1084 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741908
2015-11-21 13:09:14,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741909_1085 src: /192.168.54.130:50212 dest: /192.168.54.130:50010
2015-11-21 13:09:14,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50212, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_797494262_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741909_1085, duration: 62546810
2015-11-21 13:09:14,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741909_1085, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:09:14,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741910_1086 src: /192.168.54.130:50214 dest: /192.168.54.130:50010
2015-11-21 13:09:14,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50214, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_797494262_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741910_1086, duration: 5624085
2015-11-21 13:09:14,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741910_1086, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:09:14,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741911_1087 src: /192.168.54.130:50216 dest: /192.168.54.130:50010
2015-11-21 13:09:14,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50216, dest: /192.168.54.130:50010, bytes: 100525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_797494262_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741911_1087, duration: 14188040
2015-11-21 13:09:14,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741911_1087, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:09:24,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2015-11-21 13:09:24,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2015-11-21 13:09:24,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2015-11-21 13:09:24,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741909_1085 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741909
2015-11-21 13:09:24,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741910_1086 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741910
2015-11-21 13:09:24,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741911_1087 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741911
2015-11-21 13:16:52,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741912_1088 src: /192.168.54.130:50234 dest: /192.168.54.130:50010
2015-11-21 13:16:52,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50234, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1960080519_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741912_1088, duration: 35116539
2015-11-21 13:16:52,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741912_1088, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:16:52,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741913_1089 src: /192.168.54.130:50236 dest: /192.168.54.130:50010
2015-11-21 13:16:52,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50236, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1960080519_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741913_1089, duration: 18259647
2015-11-21 13:16:52,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741913_1089, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:16:53,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741914_1090 src: /192.168.54.130:50238 dest: /192.168.54.130:50010
2015-11-21 13:16:53,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50238, dest: /192.168.54.130:50010, bytes: 100525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1960080519_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741914_1090, duration: 12395985
2015-11-21 13:16:53,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741914_1090, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:16:57,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2015-11-21 13:16:57,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2015-11-21 13:16:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2015-11-21 13:16:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741912_1088 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741912
2015-11-21 13:16:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741913_1089 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741913
2015-11-21 13:16:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741914_1090 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741914
2015-11-21 13:17:50,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741915_1091 src: /192.168.54.130:50247 dest: /192.168.54.130:50010
2015-11-21 13:17:50,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50247, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1362843993_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741915_1091, duration: 35018038
2015-11-21 13:17:50,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741915_1091, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:17:50,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741916_1092 src: /192.168.54.130:50249 dest: /192.168.54.130:50010
2015-11-21 13:17:50,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50249, dest: /192.168.54.130:50010, bytes: 4155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1362843993_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741916_1092, duration: 5526169
2015-11-21 13:17:50,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741916_1092, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:17:50,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741917_1093 src: /192.168.54.130:50251 dest: /192.168.54.130:50010
2015-11-21 13:17:50,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50251, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1362843993_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741917_1093, duration: 10357817
2015-11-21 13:17:50,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741917_1093, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:18:00,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2015-11-21 13:18:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2015-11-21 13:18:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2015-11-21 13:18:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741915_1091 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741915
2015-11-21 13:18:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741916_1092 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741916
2015-11-21 13:18:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741917_1093 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741917
2015-11-21 13:24:54,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741918_1094 src: /192.168.54.130:50267 dest: /192.168.54.130:50010
2015-11-21 13:24:54,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50267, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_780107220_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741918_1094, duration: 52757365
2015-11-21 13:24:54,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741918_1094, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:24:54,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741919_1095 src: /192.168.54.130:50269 dest: /192.168.54.130:50010
2015-11-21 13:24:54,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50269, dest: /192.168.54.130:50010, bytes: 4147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_780107220_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741919_1095, duration: 3982724
2015-11-21 13:24:54,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741919_1095, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:24:54,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741920_1096 src: /192.168.54.130:50271 dest: /192.168.54.130:50010
2015-11-21 13:24:54,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50271, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_780107220_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741920_1096, duration: 26038203
2015-11-21 13:24:54,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741920_1096, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:25:00,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2015-11-21 13:25:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2015-11-21 13:25:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2015-11-21 13:25:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741920_1096 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741920
2015-11-21 13:25:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741918_1094 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741918
2015-11-21 13:25:00,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741919_1095 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741919
2015-11-21 13:30:48,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741921_1097 src: /192.168.54.130:50297 dest: /192.168.54.130:50010
2015-11-21 13:30:49,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50297, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-407653639_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741921_1097, duration: 43782204
2015-11-21 13:30:49,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741921_1097, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:30:49,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741922_1098 src: /192.168.54.130:50299 dest: /192.168.54.130:50010
2015-11-21 13:30:49,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50299, dest: /192.168.54.130:50010, bytes: 4147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-407653639_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741922_1098, duration: 9929940
2015-11-21 13:30:49,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741922_1098, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:30:49,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741923_1099 src: /192.168.54.130:50301 dest: /192.168.54.130:50010
2015-11-21 13:30:49,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50301, dest: /192.168.54.130:50010, bytes: 100516, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-407653639_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741923_1099, duration: 11307878
2015-11-21 13:30:49,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741923_1099, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:31:03,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2015-11-21 13:31:03,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2015-11-21 13:31:03,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741923_1099 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741923 for deletion
2015-11-21 13:31:03,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741921_1097 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741921
2015-11-21 13:31:03,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741922_1098 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741922
2015-11-21 13:31:03,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741923_1099 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741923
2015-11-21 13:32:22,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741924_1100 src: /192.168.54.130:50311 dest: /192.168.54.130:50010
2015-11-21 13:32:22,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50311, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1773527620_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741924_1100, duration: 39252893
2015-11-21 13:32:22,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741924_1100, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:32:22,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741925_1101 src: /192.168.54.130:50313 dest: /192.168.54.130:50010
2015-11-21 13:32:22,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50313, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1773527620_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741925_1101, duration: 5743408
2015-11-21 13:32:22,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741925_1101, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:32:22,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741926_1102 src: /192.168.54.130:50315 dest: /192.168.54.130:50010
2015-11-21 13:32:22,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50315, dest: /192.168.54.130:50010, bytes: 100528, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1773527620_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741926_1102, duration: 17627787
2015-11-21 13:32:22,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741926_1102, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:32:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741924_1100 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741924 for deletion
2015-11-21 13:32:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2015-11-21 13:32:33,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2015-11-21 13:32:33,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741924_1100 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741924
2015-11-21 13:32:33,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741925_1101 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741925
2015-11-21 13:32:33,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741926_1102 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741926
2015-11-21 13:39:11,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741927_1103 src: /192.168.54.130:50330 dest: /192.168.54.130:50010
2015-11-21 13:39:11,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50330, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1876499431_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741927_1103, duration: 55622895
2015-11-21 13:39:11,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741927_1103, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:39:11,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741928_1104 src: /192.168.54.130:50332 dest: /192.168.54.130:50010
2015-11-21 13:39:11,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50332, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1876499431_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741928_1104, duration: 3258425
2015-11-21 13:39:11,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741928_1104, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:39:11,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741929_1105 src: /192.168.54.130:50334 dest: /192.168.54.130:50010
2015-11-21 13:39:11,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50334, dest: /192.168.54.130:50010, bytes: 100528, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1876499431_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741929_1105, duration: 14620071
2015-11-21 13:39:11,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:39:18,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2015-11-21 13:39:18,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741928_1104 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741928 for deletion
2015-11-21 13:39:18,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2015-11-21 13:39:18,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741927_1103 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741927
2015-11-21 13:39:18,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741928_1104 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741928
2015-11-21 13:39:18,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741929_1105 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741929
2015-11-21 13:40:18,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741930_1106 src: /192.168.54.130:50344 dest: /192.168.54.130:50010
2015-11-21 13:40:19,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50344, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-223987487_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741930_1106, duration: 35395925
2015-11-21 13:40:19,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741930_1106, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:40:19,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741931_1107 src: /192.168.54.130:50346 dest: /192.168.54.130:50010
2015-11-21 13:40:19,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50346, dest: /192.168.54.130:50010, bytes: 4147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-223987487_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741931_1107, duration: 5029475
2015-11-21 13:40:19,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741931_1107, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:40:19,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741932_1108 src: /192.168.54.130:50348 dest: /192.168.54.130:50010
2015-11-21 13:40:19,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50348, dest: /192.168.54.130:50010, bytes: 100516, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-223987487_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741932_1108, duration: 11314395
2015-11-21 13:40:19,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741932_1108, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:40:45,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2015-11-21 13:40:45,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2015-11-21 13:40:45,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2015-11-21 13:40:45,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741930_1106 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741930
2015-11-21 13:40:45,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741931_1107 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741931
2015-11-21 13:40:45,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741932_1108 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741932
2015-11-21 13:49:13,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741933_1109 src: /192.168.54.130:50366 dest: /192.168.54.130:50010
2015-11-21 13:49:13,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50366, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_354304345_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741933_1109, duration: 34096918
2015-11-21 13:49:13,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741933_1109, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:13,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741934_1110 src: /192.168.54.130:50368 dest: /192.168.54.130:50010
2015-11-21 13:49:13,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50368, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_354304345_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741934_1110, duration: 4750756
2015-11-21 13:49:13,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741934_1110, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:13,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741935_1111 src: /192.168.54.130:50370 dest: /192.168.54.130:50010
2015-11-21 13:49:13,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50370, dest: /192.168.54.130:50010, bytes: 100528, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_354304345_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741935_1111, duration: 15628591
2015-11-21 13:49:13,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741935_1111, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2015-11-21 13:49:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2015-11-21 13:49:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2015-11-21 13:49:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741933_1109 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741933
2015-11-21 13:49:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741934_1110 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741934
2015-11-21 13:49:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741935_1111 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741935
2015-11-21 13:49:26,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741936_1112 src: /192.168.54.130:50379 dest: /192.168.54.130:50010
2015-11-21 13:49:26,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50379, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_480932989_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741936_1112, duration: 39840558
2015-11-21 13:49:26,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741936_1112, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:26,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741937_1113 src: /192.168.54.130:50381 dest: /192.168.54.130:50010
2015-11-21 13:49:26,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50381, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_480932989_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741937_1113, duration: 5044272
2015-11-21 13:49:26,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741937_1113, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:26,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741938_1114 src: /192.168.54.130:50383 dest: /192.168.54.130:50010
2015-11-21 13:49:26,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50383, dest: /192.168.54.130:50010, bytes: 100528, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_480932989_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741938_1114, duration: 13477436
2015-11-21 13:49:26,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741938_1114, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:33,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741936_1112 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741936 for deletion
2015-11-21 13:49:33,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741936_1112 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741936
2015-11-21 13:49:33,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2015-11-21 13:49:33,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741937_1113 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741937
2015-11-21 13:49:33,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741938_1114 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741938 for deletion
2015-11-21 13:49:33,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741938_1114 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741938
2015-11-21 13:49:37,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741939_1115 src: /192.168.54.130:50391 dest: /192.168.54.130:50010
2015-11-21 13:49:37,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50391, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_899496611_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741939_1115, duration: 31479311
2015-11-21 13:49:37,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741939_1115, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:37,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741940_1116 src: /192.168.54.130:50393 dest: /192.168.54.130:50010
2015-11-21 13:49:37,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50393, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_899496611_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741940_1116, duration: 5938170
2015-11-21 13:49:37,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741940_1116, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:37,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741941_1117 src: /192.168.54.130:50395 dest: /192.168.54.130:50010
2015-11-21 13:49:37,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50395, dest: /192.168.54.130:50010, bytes: 100528, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_899496611_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741941_1117, duration: 12919911
2015-11-21 13:49:37,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741941_1117, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:49:45,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741939_1115 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741939 for deletion
2015-11-21 13:49:45,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741939_1115 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741939
2015-11-21 13:49:45,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741940_1116 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741940 for deletion
2015-11-21 13:49:45,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741940_1116 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741940
2015-11-21 13:49:45,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2015-11-21 13:49:45,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741941_1117 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741941
2015-11-21 13:50:38,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741942_1118 src: /192.168.54.130:50405 dest: /192.168.54.130:50010
2015-11-21 13:50:38,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50405, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1467543206_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741942_1118, duration: 33820336
2015-11-21 13:50:38,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741942_1118, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:50:38,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741943_1119 src: /192.168.54.130:50407 dest: /192.168.54.130:50010
2015-11-21 13:50:38,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50407, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1467543206_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741943_1119, duration: 5099810
2015-11-21 13:50:38,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741943_1119, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:50:38,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741944_1120 src: /192.168.54.130:50409 dest: /192.168.54.130:50010
2015-11-21 13:50:38,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50409, dest: /192.168.54.130:50010, bytes: 100532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1467543206_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741944_1120, duration: 24883160
2015-11-21 13:50:38,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741944_1120, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:50:42,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741942_1118 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741942 for deletion
2015-11-21 13:50:42,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741942_1118 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741942
2015-11-21 13:50:42,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2015-11-21 13:50:42,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2015-11-21 13:50:42,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741943_1119 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741943
2015-11-21 13:50:42,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741944_1120 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741944
2015-11-21 13:54:50,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741945_1121 src: /192.168.54.130:50459 dest: /192.168.54.130:50010
2015-11-21 13:54:50,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50459, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-528024318_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741945_1121, duration: 62297104
2015-11-21 13:54:50,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741945_1121, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:54:50,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741946_1122 src: /192.168.54.130:50461 dest: /192.168.54.130:50010
2015-11-21 13:54:50,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50461, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-528024318_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741946_1122, duration: 5032426
2015-11-21 13:54:50,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741946_1122, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:54:50,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741947_1123 src: /192.168.54.130:50463 dest: /192.168.54.130:50010
2015-11-21 13:54:50,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50463, dest: /192.168.54.130:50010, bytes: 100528, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-528024318_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741947_1123, duration: 10937152
2015-11-21 13:54:50,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741947_1123, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:55:15,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741945_1121 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741945 for deletion
2015-11-21 13:55:15,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741946_1122 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741946 for deletion
2015-11-21 13:55:15,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741945_1121 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741945
2015-11-21 13:55:15,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741947_1123 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741947 for deletion
2015-11-21 13:55:15,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741946_1122 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741946
2015-11-21 13:55:15,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741947_1123 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741947
2015-11-21 13:56:34,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741948_1124 src: /192.168.54.130:50476 dest: /192.168.54.130:50010
2015-11-21 13:56:34,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50476, dest: /192.168.54.130:50010, bytes: 118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_354759532_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741948_1124, duration: 39728192
2015-11-21 13:56:34,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741948_1124, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:56:34,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741949_1125 src: /192.168.54.130:50478 dest: /192.168.54.130:50010
2015-11-21 13:56:34,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50478, dest: /192.168.54.130:50010, bytes: 4147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_354759532_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741949_1125, duration: 5930583
2015-11-21 13:56:34,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741949_1125, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 13:56:34,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741950_1126 src: /192.168.54.130:50480 dest: /192.168.54.130:50010
2015-11-21 13:56:34,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50480, dest: /192.168.54.130:50010, bytes: 100516, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_354759532_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741950_1126, duration: 12844020
2015-11-21 13:56:34,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741950_1126, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 14:01:57,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741948_1124 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741948 for deletion
2015-11-21 14:01:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741949_1125 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741949 for deletion
2015-11-21 14:01:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741950_1126 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741950 for deletion
2015-11-21 14:01:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741948_1124 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741948
2015-11-21 14:01:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741949_1125 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741949
2015-11-21 14:01:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741950_1126 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741950
2015-11-21 14:05:47,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741951_1127 src: /192.168.54.130:50507 dest: /192.168.54.130:50010
2015-11-21 14:05:47,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50507, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000319023_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741951_1127, duration: 36163222
2015-11-21 14:05:47,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741951_1127, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 14:05:47,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741952_1128 src: /192.168.54.130:50509 dest: /192.168.54.130:50010
2015-11-21 14:05:47,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50509, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000319023_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741952_1128, duration: 98680721
2015-11-21 14:05:47,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741952_1128, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 14:05:47,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741953_1129 src: /192.168.54.130:50511 dest: /192.168.54.130:50010
2015-11-21 14:05:47,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50511, dest: /192.168.54.130:50010, bytes: 100532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000319023_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741953_1129, duration: 12322796
2015-11-21 14:05:47,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741953_1129, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 14:05:57,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2015-11-21 14:05:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2015-11-21 14:05:57,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2015-11-21 14:05:57,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741952_1128 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741952
2015-11-21 14:05:57,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741953_1129 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741953
2015-11-21 14:05:57,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741951_1127 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741951
2015-11-21 14:32:27,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2c0e9507b79f,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-21 14:32:27,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-883961033-192.168.54.130-1447721127864
2015-11-21 15:53:18,611 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-883961033-192.168.54.130-1447721127864 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-21 16:05:37,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741954_1130 src: /192.168.54.130:50752 dest: /192.168.54.130:50010
2015-11-21 16:05:37,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50752, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2103898316_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741954_1130, duration: 37448866
2015-11-21 16:05:37,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:05:37,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741955_1131 src: /192.168.54.130:50754 dest: /192.168.54.130:50010
2015-11-21 16:05:37,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50754, dest: /192.168.54.130:50010, bytes: 4147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2103898316_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741955_1131, duration: 5871802
2015-11-21 16:05:37,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741955_1131, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:05:37,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741956_1132 src: /192.168.54.130:50756 dest: /192.168.54.130:50010
2015-11-21 16:05:37,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50756, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2103898316_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741956_1132, duration: 13766958
2015-11-21 16:05:37,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741956_1132, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:05:48,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2015-11-21 16:05:48,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2015-11-21 16:05:48,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741956_1132 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741956 for deletion
2015-11-21 16:05:48,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741954_1130 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741954
2015-11-21 16:05:48,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741955_1131 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741955
2015-11-21 16:05:48,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741956_1132 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741956
2015-11-21 16:06:47,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741957_1133 src: /192.168.54.130:50767 dest: /192.168.54.130:50010
2015-11-21 16:06:47,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50767, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-503541502_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741957_1133, duration: 31886905
2015-11-21 16:06:47,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741957_1133, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:06:47,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741958_1134 src: /192.168.54.130:50769 dest: /192.168.54.130:50010
2015-11-21 16:06:47,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50769, dest: /192.168.54.130:50010, bytes: 4147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-503541502_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741958_1134, duration: 4734996
2015-11-21 16:06:47,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741958_1134, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:06:47,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741959_1135 src: /192.168.54.130:50771 dest: /192.168.54.130:50010
2015-11-21 16:06:47,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50771, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-503541502_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741959_1135, duration: 13991670
2015-11-21 16:06:47,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741959_1135, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:07:24,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2015-11-21 16:07:24,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741958_1134 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741958 for deletion
2015-11-21 16:07:24,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741959_1135 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741959 for deletion
2015-11-21 16:07:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741957_1133 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741957
2015-11-21 16:07:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741958_1134 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741958
2015-11-21 16:07:24,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741959_1135 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741959
2015-11-21 16:47:47,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741960_1136 src: /192.168.54.130:50848 dest: /192.168.54.130:50010
2015-11-21 16:47:47,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50848, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165721809_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741960_1136, duration: 38735801
2015-11-21 16:47:47,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741960_1136, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:47:47,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741961_1137 src: /192.168.54.130:50850 dest: /192.168.54.130:50010
2015-11-21 16:47:47,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50850, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165721809_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741961_1137, duration: 5430758
2015-11-21 16:47:47,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741961_1137, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:47:47,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741962_1138 src: /192.168.54.130:50852 dest: /192.168.54.130:50010
2015-11-21 16:47:47,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50852, dest: /192.168.54.130:50010, bytes: 100532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165721809_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741962_1138, duration: 15795064
2015-11-21 16:47:47,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741962_1138, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 16:47:57,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741960_1136 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741960 for deletion
2015-11-21 16:47:57,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741961_1137 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741961 for deletion
2015-11-21 16:47:57,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741962_1138 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741962 for deletion
2015-11-21 16:47:57,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741960_1136 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741960
2015-11-21 16:47:57,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741961_1137 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741961
2015-11-21 16:47:57,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741962_1138 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741962
2015-11-21 17:45:17,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741963_1139 src: /192.168.54.130:50939 dest: /192.168.54.130:50010
2015-11-21 17:45:17,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50939, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1287355843_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741963_1139, duration: 35386377
2015-11-21 17:45:17,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741963_1139, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:45:17,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741964_1140 src: /192.168.54.130:50941 dest: /192.168.54.130:50010
2015-11-21 17:45:17,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50941, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1287355843_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741964_1140, duration: 4594831
2015-11-21 17:45:17,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741964_1140, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:45:17,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741965_1141 src: /192.168.54.130:50943 dest: /192.168.54.130:50010
2015-11-21 17:45:17,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50943, dest: /192.168.54.130:50010, bytes: 100532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1287355843_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741965_1141, duration: 20924488
2015-11-21 17:45:17,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741965_1141, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:45:27,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2015-11-21 17:45:27,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2015-11-21 17:45:27,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2015-11-21 17:45:27,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741963_1139 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741963
2015-11-21 17:45:27,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741964_1140 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741964
2015-11-21 17:45:27,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741965_1141 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741965
2015-11-21 17:50:45,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741966_1142 src: /192.168.54.130:50958 dest: /192.168.54.130:50010
2015-11-21 17:50:45,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50958, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1722186024_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741966_1142, duration: 31285663
2015-11-21 17:50:45,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741966_1142, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:50:45,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741967_1143 src: /192.168.54.130:50960 dest: /192.168.54.130:50010
2015-11-21 17:50:45,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50960, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1722186024_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741967_1143, duration: 5657466
2015-11-21 17:50:45,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741967_1143, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:50:45,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741968_1144 src: /192.168.54.130:50962 dest: /192.168.54.130:50010
2015-11-21 17:50:45,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50962, dest: /192.168.54.130:50010, bytes: 100532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1722186024_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741968_1144, duration: 23713116
2015-11-21 17:50:45,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741968_1144, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:50:54,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741968_1144 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741968 for deletion
2015-11-21 17:50:54,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2015-11-21 17:50:54,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2015-11-21 17:50:54,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741968_1144 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741968
2015-11-21 17:50:54,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741966_1142 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741966
2015-11-21 17:50:54,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741967_1143 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741967
2015-11-21 17:51:58,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741969_1145 src: /192.168.54.130:50971 dest: /192.168.54.130:50010
2015-11-21 17:51:58,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50971, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-594094095_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741969_1145, duration: 32996461
2015-11-21 17:51:58,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741969_1145, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:51:58,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741970_1146 src: /192.168.54.130:50973 dest: /192.168.54.130:50010
2015-11-21 17:51:58,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50973, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-594094095_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741970_1146, duration: 5713739
2015-11-21 17:51:58,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741970_1146, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:51:58,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741971_1147 src: /192.168.54.130:50976 dest: /192.168.54.130:50010
2015-11-21 17:51:58,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50976, dest: /192.168.54.130:50010, bytes: 100532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-594094095_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741971_1147, duration: 26589472
2015-11-21 17:51:58,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741971_1147, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:52:06,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2015-11-21 17:52:06,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741970_1146 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741970 for deletion
2015-11-21 17:52:06,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2015-11-21 17:52:06,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741969_1145 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741969
2015-11-21 17:52:06,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741970_1146 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741970
2015-11-21 17:52:06,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741971_1147 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741971
2015-11-21 17:55:18,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741972_1148 src: /192.168.54.130:50987 dest: /192.168.54.130:50010
2015-11-21 17:55:18,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50987, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988333788_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741972_1148, duration: 39941618
2015-11-21 17:55:18,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741972_1148, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:55:18,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741973_1149 src: /192.168.54.130:50989 dest: /192.168.54.130:50010
2015-11-21 17:55:18,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50989, dest: /192.168.54.130:50010, bytes: 3963, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988333788_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741973_1149, duration: 4816528
2015-11-21 17:55:18,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741973_1149, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:55:18,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741974_1150 src: /192.168.54.130:50991 dest: /192.168.54.130:50010
2015-11-21 17:55:18,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:50991, dest: /192.168.54.130:50010, bytes: 100532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988333788_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741974_1150, duration: 17783143
2015-11-21 17:55:18,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741974_1150, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 17:55:24,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741972_1148 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741972 for deletion
2015-11-21 17:55:24,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741973_1149 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741973 for deletion
2015-11-21 17:55:24,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741974_1150 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741974 for deletion
2015-11-21 17:55:24,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741972_1148 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741972
2015-11-21 17:55:24,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741973_1149 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741973
2015-11-21 17:55:24,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741974_1150 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741974
2015-11-21 20:12:52,613 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1778ms
No GCs detected
2015-11-21 20:29:53,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741975_1151 src: /192.168.54.130:51039 dest: /192.168.54.130:50010
2015-11-21 20:29:54,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51039, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-953805007_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741975_1151, duration: 87196653
2015-11-21 20:29:54,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741975_1151, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 20:29:54,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741976_1152 src: /192.168.54.130:51041 dest: /192.168.54.130:50010
2015-11-21 20:29:54,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51041, dest: /192.168.54.130:50010, bytes: 3944, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-953805007_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741976_1152, duration: 21778915
2015-11-21 20:29:54,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741976_1152, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 20:29:54,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741977_1153 src: /192.168.54.130:51043 dest: /192.168.54.130:50010
2015-11-21 20:29:54,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51043, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-953805007_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741977_1153, duration: 18591752
2015-11-21 20:29:54,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741977_1153, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 20:30:03,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741975_1151 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741975 for deletion
2015-11-21 20:30:03,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2015-11-21 20:30:03,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741977_1153 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741977 for deletion
2015-11-21 20:30:03,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741975_1151 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741975
2015-11-21 20:30:03,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741976_1152 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741976
2015-11-21 20:30:03,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741977_1153 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741977
2015-11-21 20:35:15,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741978_1154 src: /192.168.54.130:51056 dest: /192.168.54.130:50010
2015-11-21 20:35:15,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51056, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1563952998_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741978_1154, duration: 36196445
2015-11-21 20:35:15,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741978_1154, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 20:35:15,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741979_1155 src: /192.168.54.130:51058 dest: /192.168.54.130:50010
2015-11-21 20:35:15,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51058, dest: /192.168.54.130:50010, bytes: 3954, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1563952998_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741979_1155, duration: 12501570
2015-11-21 20:35:15,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741979_1155, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 20:35:15,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741980_1156 src: /192.168.54.130:51060 dest: /192.168.54.130:50010
2015-11-21 20:35:15,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51060, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1563952998_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741980_1156, duration: 13670000
2015-11-21 20:35:15,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741980_1156, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 20:35:27,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741978_1154 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741978 for deletion
2015-11-21 20:35:27,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741979_1155 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741979 for deletion
2015-11-21 20:35:27,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741980_1156 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741980 for deletion
2015-11-21 20:35:27,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741978_1154 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741978
2015-11-21 20:35:27,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741979_1155 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741979
2015-11-21 20:35:27,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741980_1156 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741980
2015-11-21 21:13:53,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741981_1157 src: /192.168.54.130:51116 dest: /192.168.54.130:50010
2015-11-21 21:13:53,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51116, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1261138783_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741981_1157, duration: 50495458
2015-11-21 21:13:53,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741981_1157, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:13:53,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741982_1158 src: /192.168.54.130:51118 dest: /192.168.54.130:50010
2015-11-21 21:13:53,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51118, dest: /192.168.54.130:50010, bytes: 3986, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1261138783_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741982_1158, duration: 8705284
2015-11-21 21:13:53,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741982_1158, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:13:53,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741983_1159 src: /192.168.54.130:51120 dest: /192.168.54.130:50010
2015-11-21 21:13:53,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51120, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1261138783_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741983_1159, duration: 18325793
2015-11-21 21:13:53,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741983_1159, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:14:03,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741981_1157 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741981 for deletion
2015-11-21 21:14:03,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741982_1158 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741982 for deletion
2015-11-21 21:14:03,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741983_1159 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741983 for deletion
2015-11-21 21:14:03,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741981_1157 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741981
2015-11-21 21:14:03,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741982_1158 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741982
2015-11-21 21:14:03,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741983_1159 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741983
2015-11-21 21:16:25,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741984_1160 src: /192.168.54.130:51135 dest: /192.168.54.130:50010
2015-11-21 21:16:25,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51135, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-242892198_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741984_1160, duration: 37736844
2015-11-21 21:16:25,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741984_1160, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:16:25,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741985_1161 src: /192.168.54.130:51137 dest: /192.168.54.130:50010
2015-11-21 21:16:25,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51137, dest: /192.168.54.130:50010, bytes: 3961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-242892198_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741985_1161, duration: 4548811
2015-11-21 21:16:25,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741985_1161, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:16:25,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741986_1162 src: /192.168.54.130:51139 dest: /192.168.54.130:50010
2015-11-21 21:16:25,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51139, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-242892198_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741986_1162, duration: 12239913
2015-11-21 21:16:25,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741986_1162, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:16:36,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741984_1160 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741984 for deletion
2015-11-21 21:16:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741985_1161 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741985 for deletion
2015-11-21 21:16:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741986_1162 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741986 for deletion
2015-11-21 21:16:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741984_1160 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741984
2015-11-21 21:16:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741985_1161 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741985
2015-11-21 21:16:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741986_1162 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741986
2015-11-21 21:21:24,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741987_1163 src: /192.168.54.130:51156 dest: /192.168.54.130:50010
2015-11-21 21:21:24,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51156, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1737177405_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741987_1163, duration: 44414758
2015-11-21 21:21:24,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741987_1163, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:21:24,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741988_1164 src: /192.168.54.130:51158 dest: /192.168.54.130:50010
2015-11-21 21:21:24,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51158, dest: /192.168.54.130:50010, bytes: 3924, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1737177405_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741988_1164, duration: 6084658
2015-11-21 21:21:24,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741988_1164, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:21:24,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741989_1165 src: /192.168.54.130:51160 dest: /192.168.54.130:50010
2015-11-21 21:21:24,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51160, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1737177405_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741989_1165, duration: 11827667
2015-11-21 21:21:24,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741989_1165, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:30:34,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741990_1166 src: /192.168.54.130:51182 dest: /192.168.54.130:50010
2015-11-21 21:30:34,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51182, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-763313029_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741990_1166, duration: 47353833
2015-11-21 21:30:34,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741990_1166, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:30:34,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741991_1167 src: /192.168.54.130:51184 dest: /192.168.54.130:50010
2015-11-21 21:30:34,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51184, dest: /192.168.54.130:50010, bytes: 3924, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-763313029_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741991_1167, duration: 5526551
2015-11-21 21:30:34,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741991_1167, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:30:34,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741992_1168 src: /192.168.54.130:51186 dest: /192.168.54.130:50010
2015-11-21 21:30:35,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51186, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-763313029_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741992_1168, duration: 14083253
2015-11-21 21:30:35,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741992_1168, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:33:18,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741990_1166 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741990 for deletion
2015-11-21 21:33:18,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741991_1167 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741991 for deletion
2015-11-21 21:33:18,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741992_1168 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741992 for deletion
2015-11-21 21:33:18,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741990_1166 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741990
2015-11-21 21:33:18,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741991_1167 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741991
2015-11-21 21:33:18,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741992_1168 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741992
2015-11-21 21:34:06,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741993_1169 src: /192.168.54.130:51198 dest: /192.168.54.130:50010
2015-11-21 21:34:06,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51198, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-320066079_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741993_1169, duration: 42248961
2015-11-21 21:34:06,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741993_1169, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:34:06,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741994_1170 src: /192.168.54.130:51200 dest: /192.168.54.130:50010
2015-11-21 21:34:06,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51200, dest: /192.168.54.130:50010, bytes: 3922, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-320066079_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741994_1170, duration: 6149000
2015-11-21 21:34:06,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741994_1170, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:34:07,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741995_1171 src: /192.168.54.130:51202 dest: /192.168.54.130:50010
2015-11-21 21:34:07,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51202, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-320066079_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741995_1171, duration: 17101819
2015-11-21 21:34:07,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741995_1171, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:34:15,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741993_1169 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741993 for deletion
2015-11-21 21:34:15,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741993_1169 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741993
2015-11-21 21:34:15,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741994_1170 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741994 for deletion
2015-11-21 21:34:15,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741994_1170 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741994
2015-11-21 21:34:15,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741995_1171 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741995 for deletion
2015-11-21 21:34:15,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741995_1171 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741995
2015-11-21 21:35:19,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741996_1172 src: /192.168.54.130:51211 dest: /192.168.54.130:50010
2015-11-21 21:35:19,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51211, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1183433182_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741996_1172, duration: 43623928
2015-11-21 21:35:19,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741996_1172, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:35:19,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741997_1173 src: /192.168.54.130:51213 dest: /192.168.54.130:50010
2015-11-21 21:35:19,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51213, dest: /192.168.54.130:50010, bytes: 3922, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1183433182_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741997_1173, duration: 5130541
2015-11-21 21:35:19,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741997_1173, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:35:19,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741998_1174 src: /192.168.54.130:51215 dest: /192.168.54.130:50010
2015-11-21 21:35:19,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51215, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1183433182_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741998_1174, duration: 14012992
2015-11-21 21:35:19,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741998_1174, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:37:45,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741996_1172 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741996 for deletion
2015-11-21 21:37:45,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741997_1173 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741997 for deletion
2015-11-21 21:37:45,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741998_1174 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741998 for deletion
2015-11-21 21:37:45,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741996_1172 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741996
2015-11-21 21:37:45,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741997_1173 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741997
2015-11-21 21:37:45,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741998_1174 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741998
2015-11-21 21:39:24,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073741999_1175 src: /192.168.54.130:51230 dest: /192.168.54.130:50010
2015-11-21 21:39:24,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51230, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723297134_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073741999_1175, duration: 46911426
2015-11-21 21:39:24,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073741999_1175, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:39:24,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742000_1176 src: /192.168.54.130:51232 dest: /192.168.54.130:50010
2015-11-21 21:39:24,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51232, dest: /192.168.54.130:50010, bytes: 3920, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723297134_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742000_1176, duration: 7213064
2015-11-21 21:39:24,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742000_1176, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:39:25,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742001_1177 src: /192.168.54.130:51234 dest: /192.168.54.130:50010
2015-11-21 21:39:25,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51234, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723297134_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742001_1177, duration: 69386092
2015-11-21 21:39:25,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742001_1177, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:39:36,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742000_1176 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742000 for deletion
2015-11-21 21:39:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742001_1177 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742001 for deletion
2015-11-21 21:39:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741999_1175 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741999 for deletion
2015-11-21 21:39:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742000_1176 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742000
2015-11-21 21:39:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742001_1177 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742001
2015-11-21 21:39:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741999_1175 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741999
2015-11-21 21:40:09,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742002_1178 src: /192.168.54.130:51242 dest: /192.168.54.130:50010
2015-11-21 21:40:09,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51242, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-360487454_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742002_1178, duration: 51406265
2015-11-21 21:40:09,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742002_1178, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:40:09,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742003_1179 src: /192.168.54.130:51244 dest: /192.168.54.130:50010
2015-11-21 21:40:09,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51244, dest: /192.168.54.130:50010, bytes: 3920, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-360487454_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742003_1179, duration: 4974251
2015-11-21 21:40:09,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742003_1179, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:40:09,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742004_1180 src: /192.168.54.130:51246 dest: /192.168.54.130:50010
2015-11-21 21:40:09,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51246, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-360487454_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742004_1180, duration: 14756041
2015-11-21 21:40:09,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742004_1180, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:40:21,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742002_1178 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742002 for deletion
2015-11-21 21:40:21,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742002_1178 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742002
2015-11-21 21:40:21,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742003_1179 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742003 for deletion
2015-11-21 21:40:21,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742003_1179 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742003
2015-11-21 21:40:21,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742004_1180 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742004 for deletion
2015-11-21 21:40:21,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742004_1180 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742004
2015-11-21 21:40:27,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742005_1181 src: /192.168.54.130:51255 dest: /192.168.54.130:50010
2015-11-21 21:40:27,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51255, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_17782095_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742005_1181, duration: 41515898
2015-11-21 21:40:27,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742005_1181, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:40:27,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742006_1182 src: /192.168.54.130:51257 dest: /192.168.54.130:50010
2015-11-21 21:40:27,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51257, dest: /192.168.54.130:50010, bytes: 3920, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_17782095_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742006_1182, duration: 5873707
2015-11-21 21:40:27,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742006_1182, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:40:27,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742007_1183 src: /192.168.54.130:51259 dest: /192.168.54.130:50010
2015-11-21 21:40:27,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51259, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_17782095_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742007_1183, duration: 16675225
2015-11-21 21:40:27,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742007_1183, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:40:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742005_1181 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742005 for deletion
2015-11-21 21:40:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742005_1181 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742005
2015-11-21 21:40:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742006_1182 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742006 for deletion
2015-11-21 21:40:36,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742006_1182 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742006
2015-11-21 21:40:36,026 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742007_1183 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742007 for deletion
2015-11-21 21:40:36,027 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742007_1183 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742007
2015-11-21 21:43:02,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742008_1184 src: /192.168.54.130:51269 dest: /192.168.54.130:50010
2015-11-21 21:43:02,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51269, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-441937441_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742008_1184, duration: 64268576
2015-11-21 21:43:02,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742008_1184, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:43:02,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742009_1185 src: /192.168.54.130:51271 dest: /192.168.54.130:50010
2015-11-21 21:43:02,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51271, dest: /192.168.54.130:50010, bytes: 3920, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-441937441_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742009_1185, duration: 6446482
2015-11-21 21:43:02,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742009_1185, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:43:02,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742010_1186 src: /192.168.54.130:51273 dest: /192.168.54.130:50010
2015-11-21 21:43:02,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51273, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-441937441_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742010_1186, duration: 16277080
2015-11-21 21:43:02,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742010_1186, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:45:54,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742008_1184 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742008 for deletion
2015-11-21 21:45:54,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742009_1185 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742009 for deletion
2015-11-21 21:45:54,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742010_1186 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742010 for deletion
2015-11-21 21:45:54,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742008_1184 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742008
2015-11-21 21:45:54,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742009_1185 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742009
2015-11-21 21:45:54,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742010_1186 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742010
2015-11-21 21:47:27,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742011_1187 src: /192.168.54.130:51288 dest: /192.168.54.130:50010
2015-11-21 21:47:28,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51288, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-663390868_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742011_1187, duration: 59116489
2015-11-21 21:47:28,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742011_1187, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:47:28,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742012_1188 src: /192.168.54.130:51290 dest: /192.168.54.130:50010
2015-11-21 21:47:28,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51290, dest: /192.168.54.130:50010, bytes: 3966, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-663390868_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742012_1188, duration: 10613134
2015-11-21 21:47:28,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742012_1188, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:47:28,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742013_1189 src: /192.168.54.130:51292 dest: /192.168.54.130:50010
2015-11-21 21:47:28,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51292, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-663390868_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742013_1189, duration: 104199556
2015-11-21 21:47:28,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742013_1189, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:50:15,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742011_1187 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742011 for deletion
2015-11-21 21:50:15,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742012_1188 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742012 for deletion
2015-11-21 21:50:15,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742013_1189 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742013 for deletion
2015-11-21 21:50:15,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742011_1187 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742011
2015-11-21 21:50:15,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742012_1188 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742012
2015-11-21 21:50:15,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742013_1189 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742013
2015-11-21 21:57:12,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741987_1163 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741987 for deletion
2015-11-21 21:57:12,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741988_1164 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741988 for deletion
2015-11-21 21:57:12,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741989_1165 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741989 for deletion
2015-11-21 21:57:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741987_1163 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741987
2015-11-21 21:57:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741988_1164 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741988
2015-11-21 21:57:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073741989_1165 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073741989
2015-11-21 21:58:22,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742014_1190 src: /192.168.54.130:51318 dest: /192.168.54.130:50010
2015-11-21 21:58:22,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51318, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-28046642_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742014_1190, duration: 35874523
2015-11-21 21:58:22,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742014_1190, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:58:22,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742015_1191 src: /192.168.54.130:51320 dest: /192.168.54.130:50010
2015-11-21 21:58:22,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51320, dest: /192.168.54.130:50010, bytes: 4020, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-28046642_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742015_1191, duration: 7780614
2015-11-21 21:58:22,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742015_1191, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:58:22,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742016_1192 src: /192.168.54.130:51322 dest: /192.168.54.130:50010
2015-11-21 21:58:22,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51322, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-28046642_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742016_1192, duration: 19568165
2015-11-21 21:58:22,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742016_1192, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:58:30,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742016_1192 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742016 for deletion
2015-11-21 21:58:30,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742014_1190 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742014 for deletion
2015-11-21 21:58:30,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742015_1191 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742015 for deletion
2015-11-21 21:58:30,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742016_1192 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742016
2015-11-21 21:58:30,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742014_1190 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742014
2015-11-21 21:58:30,024 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742015_1191 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742015
2015-11-21 21:59:16,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742017_1193 src: /192.168.54.130:51332 dest: /192.168.54.130:50010
2015-11-21 21:59:16,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51332, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11158537_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742017_1193, duration: 43291732
2015-11-21 21:59:16,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742017_1193, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:59:16,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742018_1194 src: /192.168.54.130:51334 dest: /192.168.54.130:50010
2015-11-21 21:59:16,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51334, dest: /192.168.54.130:50010, bytes: 4020, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11158537_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742018_1194, duration: 8139120
2015-11-21 21:59:16,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742018_1194, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 21:59:17,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742019_1195 src: /192.168.54.130:51336 dest: /192.168.54.130:50010
2015-11-21 21:59:17,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51336, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11158537_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742019_1195, duration: 12487263
2015-11-21 21:59:17,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742019_1195, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:26:20,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742020_1196 src: /192.168.54.130:51388 dest: /192.168.54.130:50010
2015-11-21 22:26:20,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51388, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152389319_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742020_1196, duration: 61422448
2015-11-21 22:26:20,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742020_1196, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:26:21,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742021_1197 src: /192.168.54.130:51390 dest: /192.168.54.130:50010
2015-11-21 22:26:21,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51390, dest: /192.168.54.130:50010, bytes: 4039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152389319_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742021_1197, duration: 12958749
2015-11-21 22:26:21,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742021_1197, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:26:21,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742022_1198 src: /192.168.54.130:51392 dest: /192.168.54.130:50010
2015-11-21 22:26:21,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51392, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-152389319_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742022_1198, duration: 18708917
2015-11-21 22:26:21,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742022_1198, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:26:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742020_1196 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742020 for deletion
2015-11-21 22:26:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742021_1197 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742021 for deletion
2015-11-21 22:26:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742022_1198 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742022 for deletion
2015-11-21 22:26:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742020_1196 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742020
2015-11-21 22:26:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742021_1197 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742021
2015-11-21 22:26:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742022_1198 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742022
2015-11-21 22:28:01,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742023_1199 src: /192.168.54.130:51407 dest: /192.168.54.130:50010
2015-11-21 22:28:01,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51407, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2145856594_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742023_1199, duration: 52789643
2015-11-21 22:28:01,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742023_1199, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:28:01,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742024_1200 src: /192.168.54.130:51409 dest: /192.168.54.130:50010
2015-11-21 22:28:01,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51409, dest: /192.168.54.130:50010, bytes: 4039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2145856594_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742024_1200, duration: 7649452
2015-11-21 22:28:01,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742024_1200, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:28:01,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742025_1201 src: /192.168.54.130:51411 dest: /192.168.54.130:50010
2015-11-21 22:28:01,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51411, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2145856594_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742025_1201, duration: 19182099
2015-11-21 22:28:01,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742025_1201, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:28:12,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742023_1199 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742023 for deletion
2015-11-21 22:28:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742024_1200 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742024 for deletion
2015-11-21 22:28:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742025_1201 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742025 for deletion
2015-11-21 22:28:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742023_1199 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742023
2015-11-21 22:28:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742024_1200 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742024
2015-11-21 22:28:12,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742025_1201 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742025
2015-11-21 22:29:09,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742017_1193 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742017 for deletion
2015-11-21 22:29:09,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742018_1194 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742018 for deletion
2015-11-21 22:29:09,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742019_1195 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742019 for deletion
2015-11-21 22:29:09,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742017_1193 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742017
2015-11-21 22:29:09,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742018_1194 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742018
2015-11-21 22:29:09,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742019_1195 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742019
2015-11-21 22:29:40,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742026_1202 src: /192.168.54.130:51422 dest: /192.168.54.130:50010
2015-11-21 22:29:40,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51422, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1595393678_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742026_1202, duration: 57418464
2015-11-21 22:29:40,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742026_1202, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:29:40,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742027_1203 src: /192.168.54.130:51424 dest: /192.168.54.130:50010
2015-11-21 22:29:40,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51424, dest: /192.168.54.130:50010, bytes: 4039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1595393678_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742027_1203, duration: 6975661
2015-11-21 22:29:40,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742027_1203, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:29:40,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742028_1204 src: /192.168.54.130:51426 dest: /192.168.54.130:50010
2015-11-21 22:29:40,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51426, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1595393678_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742028_1204, duration: 23022445
2015-11-21 22:29:40,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742028_1204, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:29:48,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742026_1202 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742026 for deletion
2015-11-21 22:29:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742026_1202 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742026
2015-11-21 22:29:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742027_1203 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742027 for deletion
2015-11-21 22:29:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742027_1203 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742027
2015-11-21 22:29:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742028_1204 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742028 for deletion
2015-11-21 22:29:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742028_1204 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742028
2015-11-21 22:49:24,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742029_1205 src: /192.168.54.130:51458 dest: /192.168.54.130:50010
2015-11-21 22:49:24,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51458, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-189125069_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742029_1205, duration: 34479850
2015-11-21 22:49:24,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742029_1205, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:49:24,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742030_1206 src: /192.168.54.130:51460 dest: /192.168.54.130:50010
2015-11-21 22:49:24,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51460, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-189125069_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742030_1206, duration: 7185160
2015-11-21 22:49:24,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742030_1206, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:49:24,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742031_1207 src: /192.168.54.130:51462 dest: /192.168.54.130:50010
2015-11-21 22:49:24,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51462, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-189125069_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742031_1207, duration: 13697886
2015-11-21 22:49:24,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742031_1207, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:49:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742029_1205 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742029 for deletion
2015-11-21 22:49:48,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742030_1206 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742030 for deletion
2015-11-21 22:49:48,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742031_1207 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742031 for deletion
2015-11-21 22:49:48,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742029_1205 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742029
2015-11-21 22:49:48,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742030_1206 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742030
2015-11-21 22:49:48,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742031_1207 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742031
2015-11-21 22:50:30,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3fb3b952c598,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-21 22:50:30,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-883961033-192.168.54.130-1447721127864
2015-11-21 22:53:57,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742032_1208 src: /192.168.54.130:51474 dest: /192.168.54.130:50010
2015-11-21 22:53:57,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51474, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1354663511_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742032_1208, duration: 39729380
2015-11-21 22:53:57,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742032_1208, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:53:57,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742033_1209 src: /192.168.54.130:51476 dest: /192.168.54.130:50010
2015-11-21 22:53:57,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51476, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1354663511_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742033_1209, duration: 8506224
2015-11-21 22:53:57,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742033_1209, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:53:57,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742034_1210 src: /192.168.54.130:51478 dest: /192.168.54.130:50010
2015-11-21 22:53:57,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51478, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1354663511_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742034_1210, duration: 16594717
2015-11-21 22:53:57,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742034_1210, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:54:21,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742032_1208 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742032 for deletion
2015-11-21 22:54:21,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742033_1209 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742033 for deletion
2015-11-21 22:54:21,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742034_1210 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742034 for deletion
2015-11-21 22:54:21,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742032_1208 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742032
2015-11-21 22:54:21,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742033_1209 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742033
2015-11-21 22:54:21,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742034_1210 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742034
2015-11-21 22:55:00,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742035_1211 src: /192.168.54.130:51487 dest: /192.168.54.130:50010
2015-11-21 22:55:00,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51487, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_200022443_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742035_1211, duration: 39342823
2015-11-21 22:55:00,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742035_1211, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:55:00,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742036_1212 src: /192.168.54.130:51489 dest: /192.168.54.130:50010
2015-11-21 22:55:00,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51489, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_200022443_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742036_1212, duration: 7620759
2015-11-21 22:55:00,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742036_1212, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:55:00,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742037_1213 src: /192.168.54.130:51491 dest: /192.168.54.130:50010
2015-11-21 22:55:00,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51491, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_200022443_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742037_1213, duration: 14447425
2015-11-21 22:55:00,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742037_1213, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 22:55:24,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742035_1211 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742035 for deletion
2015-11-21 22:55:24,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742036_1212 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742036 for deletion
2015-11-21 22:55:24,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742037_1213 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742037 for deletion
2015-11-21 22:55:24,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742035_1211 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742035
2015-11-21 22:55:24,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742036_1212 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742036
2015-11-21 22:55:24,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742037_1213 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742037
2015-11-21 23:00:02,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742038_1214 src: /192.168.54.130:51504 dest: /192.168.54.130:50010
2015-11-21 23:00:02,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51504, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1314452674_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742038_1214, duration: 38599093
2015-11-21 23:00:02,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742038_1214, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:00:02,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742039_1215 src: /192.168.54.130:51506 dest: /192.168.54.130:50010
2015-11-21 23:00:02,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51506, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1314452674_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742039_1215, duration: 7015179
2015-11-21 23:00:02,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742039_1215, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:00:03,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742040_1216 src: /192.168.54.130:51508 dest: /192.168.54.130:50010
2015-11-21 23:00:03,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51508, dest: /192.168.54.130:50010, bytes: 100521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1314452674_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742040_1216, duration: 18853384
2015-11-21 23:00:03,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742040_1216, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:00:51,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742038_1214 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742038 for deletion
2015-11-21 23:00:51,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742039_1215 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742039 for deletion
2015-11-21 23:00:51,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742040_1216 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742040 for deletion
2015-11-21 23:00:51,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742038_1214 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742038
2015-11-21 23:00:51,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742039_1215 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742039
2015-11-21 23:00:51,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742040_1216 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742040
2015-11-21 23:05:09,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742041_1217 src: /192.168.54.130:51521 dest: /192.168.54.130:50010
2015-11-21 23:05:09,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51521, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1674079927_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742041_1217, duration: 47634106
2015-11-21 23:05:09,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742041_1217, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:05:09,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742042_1218 src: /192.168.54.130:51523 dest: /192.168.54.130:50010
2015-11-21 23:05:09,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51523, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1674079927_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742042_1218, duration: 6389555
2015-11-21 23:05:09,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742042_1218, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:05:09,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742043_1219 src: /192.168.54.130:51525 dest: /192.168.54.130:50010
2015-11-21 23:05:09,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51525, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1674079927_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742043_1219, duration: 18704848
2015-11-21 23:05:09,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742043_1219, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:05:30,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742041_1217 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742041 for deletion
2015-11-21 23:05:30,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742042_1218 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742042 for deletion
2015-11-21 23:05:30,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742043_1219 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742043 for deletion
2015-11-21 23:05:30,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742041_1217 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742041
2015-11-21 23:05:30,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742042_1218 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742042
2015-11-21 23:05:30,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742043_1219 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742043
2015-11-21 23:09:20,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742044_1220 src: /192.168.54.130:51537 dest: /192.168.54.130:50010
2015-11-21 23:09:20,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51537, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2046704522_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742044_1220, duration: 50643020
2015-11-21 23:09:20,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742044_1220, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:09:20,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742045_1221 src: /192.168.54.130:51539 dest: /192.168.54.130:50010
2015-11-21 23:09:20,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51539, dest: /192.168.54.130:50010, bytes: 4147, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2046704522_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742045_1221, duration: 10870439
2015-11-21 23:09:20,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742045_1221, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:09:20,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742046_1222 src: /192.168.54.130:51541 dest: /192.168.54.130:50010
2015-11-21 23:09:20,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:51541, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2046704522_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742046_1222, duration: 29236828
2015-11-21 23:09:20,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742046_1222, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-21 23:10:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742044_1220 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742044 for deletion
2015-11-21 23:10:12,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742045_1221 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742045 for deletion
2015-11-21 23:10:12,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742046_1222 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742046 for deletion
2015-11-21 23:10:12,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742044_1220 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742044
2015-11-21 23:10:12,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742045_1221 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742045
2015-11-21 23:10:12,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742046_1222 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742046
2015-11-21 23:15:54,986 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1350ms
No GCs detected
2015-11-22 00:11:20,658 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-883961033-192.168.54.130-1447721127864 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-22 04:50:30,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5358ddade5a8,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-22 04:50:30,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-883961033-192.168.54.130-1447721127864
2015-11-22 06:11:20,649 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-883961033-192.168.54.130-1447721127864 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-22 08:35:24,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742047_1223 src: /192.168.54.130:52243 dest: /192.168.54.130:50010
2015-11-22 08:35:25,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52243, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-387506083_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742047_1223, duration: 43003365
2015-11-22 08:35:25,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742047_1223, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:35:25,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742048_1224 src: /192.168.54.130:52245 dest: /192.168.54.130:50010
2015-11-22 08:35:25,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52245, dest: /192.168.54.130:50010, bytes: 5203, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-387506083_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742048_1224, duration: 4858039
2015-11-22 08:35:25,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742048_1224, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:35:25,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742049_1225 src: /192.168.54.130:52247 dest: /192.168.54.130:50010
2015-11-22 08:35:25,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52247, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-387506083_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742049_1225, duration: 25243243
2015-11-22 08:35:25,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742049_1225, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:36:38,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742048_1224 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742048 for deletion
2015-11-22 08:36:38,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742049_1225 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742049 for deletion
2015-11-22 08:36:38,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742047_1223 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742047 for deletion
2015-11-22 08:36:38,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742048_1224 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742048
2015-11-22 08:36:38,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742049_1225 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742049
2015-11-22 08:36:38,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742047_1223 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742047
2015-11-22 08:40:30,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742050_1226 src: /192.168.54.130:52272 dest: /192.168.54.130:50010
2015-11-22 08:40:30,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52272, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2140621254_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742050_1226, duration: 44160172
2015-11-22 08:40:30,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742050_1226, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:40:30,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742051_1227 src: /192.168.54.130:52274 dest: /192.168.54.130:50010
2015-11-22 08:40:30,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52274, dest: /192.168.54.130:50010, bytes: 5227, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2140621254_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742051_1227, duration: 6224140
2015-11-22 08:40:30,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742051_1227, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:40:31,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742052_1228 src: /192.168.54.130:52276 dest: /192.168.54.130:50010
2015-11-22 08:40:31,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52276, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2140621254_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742052_1228, duration: 17907835
2015-11-22 08:40:31,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742052_1228, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:40:38,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742050_1226 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742050 for deletion
2015-11-22 08:40:38,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742051_1227 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742051 for deletion
2015-11-22 08:40:38,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742052_1228 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742052 for deletion
2015-11-22 08:40:38,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742050_1226 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742050
2015-11-22 08:40:38,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742051_1227 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742051
2015-11-22 08:40:38,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742052_1228 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742052
2015-11-22 08:43:47,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742053_1229 src: /192.168.54.130:52289 dest: /192.168.54.130:50010
2015-11-22 08:43:47,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52289, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_976456052_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742053_1229, duration: 36928401
2015-11-22 08:43:47,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742053_1229, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:43:47,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742054_1230 src: /192.168.54.130:52291 dest: /192.168.54.130:50010
2015-11-22 08:43:47,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52291, dest: /192.168.54.130:50010, bytes: 5227, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_976456052_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742054_1230, duration: 5745393
2015-11-22 08:43:47,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742054_1230, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:43:47,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742055_1231 src: /192.168.54.130:52293 dest: /192.168.54.130:50010
2015-11-22 08:43:47,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52293, dest: /192.168.54.130:50010, bytes: 100498, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_976456052_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742055_1231, duration: 16897202
2015-11-22 08:43:47,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742055_1231, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 08:43:59,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742053_1229 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742053 for deletion
2015-11-22 08:43:59,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742054_1230 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742054 for deletion
2015-11-22 08:43:59,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742055_1231 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742055 for deletion
2015-11-22 08:43:59,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742053_1229 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742053
2015-11-22 08:43:59,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742054_1230 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742054
2015-11-22 08:43:59,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742055_1231 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742055
2015-11-22 11:06:32,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742056_1232 src: /192.168.54.130:52524 dest: /192.168.54.130:50010
2015-11-22 11:06:32,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52524, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2068991968_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742056_1232, duration: 39371755
2015-11-22 11:06:32,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742056_1232, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 11:06:32,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742057_1233 src: /192.168.54.130:52526 dest: /192.168.54.130:50010
2015-11-22 11:06:32,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52526, dest: /192.168.54.130:50010, bytes: 4687, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2068991968_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742057_1233, duration: 5141172
2015-11-22 11:06:32,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742057_1233, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 11:06:33,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742058_1234 src: /192.168.54.130:52528 dest: /192.168.54.130:50010
2015-11-22 11:06:33,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52528, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2068991968_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742058_1234, duration: 26000782
2015-11-22 11:06:33,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742058_1234, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 11:06:41,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742056_1232 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742056 for deletion
2015-11-22 11:06:41,378 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742057_1233 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742057 for deletion
2015-11-22 11:06:41,378 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742058_1234 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742058 for deletion
2015-11-22 11:06:41,378 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742056_1232 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742056
2015-11-22 11:06:41,390 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742057_1233 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742057
2015-11-22 11:06:41,390 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742058_1234 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742058
2015-11-22 11:17:29,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742059_1235 src: /192.168.54.130:52553 dest: /192.168.54.130:50010
2015-11-22 11:17:29,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52553, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1610402337_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742059_1235, duration: 31361974
2015-11-22 11:17:29,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742059_1235, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 11:17:29,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742060_1236 src: /192.168.54.130:52555 dest: /192.168.54.130:50010
2015-11-22 11:17:29,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52555, dest: /192.168.54.130:50010, bytes: 4739, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1610402337_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742060_1236, duration: 7256896
2015-11-22 11:17:29,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742060_1236, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 11:17:29,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742061_1237 src: /192.168.54.130:52557 dest: /192.168.54.130:50010
2015-11-22 11:17:29,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52557, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1610402337_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742061_1237, duration: 27242195
2015-11-22 11:17:29,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742061_1237, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 11:17:35,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742059_1235 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742059 for deletion
2015-11-22 11:17:35,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742060_1236 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742060 for deletion
2015-11-22 11:17:35,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742061_1237 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742061 for deletion
2015-11-22 11:17:35,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742059_1235 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742059
2015-11-22 11:17:35,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742060_1236 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742060
2015-11-22 11:17:35,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742061_1237 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742061
2015-11-22 13:07:13,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742062_1238 src: /192.168.54.130:52594 dest: /192.168.54.130:50010
2015-11-22 13:07:13,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52594, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2063318496_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742062_1238, duration: 50359877
2015-11-22 13:07:13,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742062_1238, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:07:13,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742063_1239 src: /192.168.54.130:52596 dest: /192.168.54.130:50010
2015-11-22 13:07:13,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52596, dest: /192.168.54.130:50010, bytes: 4839, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2063318496_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742063_1239, duration: 18055618
2015-11-22 13:07:13,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742063_1239, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:07:13,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742064_1240 src: /192.168.54.130:52598 dest: /192.168.54.130:50010
2015-11-22 13:07:13,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52598, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2063318496_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742064_1240, duration: 13488826
2015-11-22 13:07:13,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742064_1240, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:07:22,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742064_1240 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742064 for deletion
2015-11-22 13:07:22,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742062_1238 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742062 for deletion
2015-11-22 13:07:22,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742063_1239 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742063 for deletion
2015-11-22 13:07:22,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742064_1240 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742064
2015-11-22 13:07:22,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742062_1238 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742062
2015-11-22 13:07:22,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742063_1239 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742063
2015-11-22 13:11:54,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742065_1241 src: /192.168.54.130:52612 dest: /192.168.54.130:50010
2015-11-22 13:11:55,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52612, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1938729712_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742065_1241, duration: 63206980
2015-11-22 13:11:55,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742065_1241, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:11:55,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742066_1242 src: /192.168.54.130:52614 dest: /192.168.54.130:50010
2015-11-22 13:11:55,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52614, dest: /192.168.54.130:50010, bytes: 4892, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1938729712_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742066_1242, duration: 9057434
2015-11-22 13:11:55,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742066_1242, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:11:55,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742067_1243 src: /192.168.54.130:52616 dest: /192.168.54.130:50010
2015-11-22 13:11:55,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52616, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1938729712_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742067_1243, duration: 20049974
2015-11-22 13:11:55,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742067_1243, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:12:16,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742065_1241 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742065 for deletion
2015-11-22 13:12:16,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742066_1242 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742066 for deletion
2015-11-22 13:12:16,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742067_1243 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742067 for deletion
2015-11-22 13:12:16,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742065_1241 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742065
2015-11-22 13:12:16,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742066_1242 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742066
2015-11-22 13:12:16,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742067_1243 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742067
2015-11-22 13:16:00,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742068_1244 src: /192.168.54.130:52628 dest: /192.168.54.130:50010
2015-11-22 13:16:00,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52628, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_773929500_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742068_1244, duration: 42740525
2015-11-22 13:16:00,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742068_1244, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:16:00,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742069_1245 src: /192.168.54.130:52630 dest: /192.168.54.130:50010
2015-11-22 13:16:00,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52630, dest: /192.168.54.130:50010, bytes: 4870, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_773929500_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742069_1245, duration: 7250754
2015-11-22 13:16:00,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742069_1245, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:16:01,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742070_1246 src: /192.168.54.130:52632 dest: /192.168.54.130:50010
2015-11-22 13:16:01,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52632, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_773929500_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742070_1246, duration: 25281732
2015-11-22 13:16:01,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742070_1246, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:16:22,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742068_1244 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742068 for deletion
2015-11-22 13:16:22,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742069_1245 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742069 for deletion
2015-11-22 13:16:22,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742070_1246 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742070 for deletion
2015-11-22 13:16:22,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742068_1244 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742068
2015-11-22 13:16:22,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742069_1245 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742069
2015-11-22 13:16:22,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742070_1246 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742070
2015-11-22 13:17:44,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742071_1247 src: /192.168.54.130:52646 dest: /192.168.54.130:50010
2015-11-22 13:17:44,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52646, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_803499807_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742071_1247, duration: 51200354
2015-11-22 13:17:44,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742071_1247, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:17:44,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742072_1248 src: /192.168.54.130:52648 dest: /192.168.54.130:50010
2015-11-22 13:17:44,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52648, dest: /192.168.54.130:50010, bytes: 4870, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_803499807_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742072_1248, duration: 8435293
2015-11-22 13:17:44,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742072_1248, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:17:44,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742073_1249 src: /192.168.54.130:52650 dest: /192.168.54.130:50010
2015-11-22 13:17:44,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52650, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_803499807_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742073_1249, duration: 15015884
2015-11-22 13:17:44,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742073_1249, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:18:46,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742071_1247 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742071 for deletion
2015-11-22 13:18:46,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742072_1248 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742072 for deletion
2015-11-22 13:18:46,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742073_1249 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742073 for deletion
2015-11-22 13:18:46,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742071_1247 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742071
2015-11-22 13:18:46,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742072_1248 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742072
2015-11-22 13:18:46,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742073_1249 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742073
2015-11-22 13:22:07,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x66fe01f53354,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-22 13:22:07,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-883961033-192.168.54.130-1447721127864
2015-11-22 13:24:17,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742074_1250 src: /192.168.54.130:52668 dest: /192.168.54.130:50010
2015-11-22 13:24:17,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52668, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1791847927_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742074_1250, duration: 44318416
2015-11-22 13:24:17,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742074_1250, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:24:17,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742075_1251 src: /192.168.54.130:52670 dest: /192.168.54.130:50010
2015-11-22 13:24:17,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52670, dest: /192.168.54.130:50010, bytes: 4870, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1791847927_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742075_1251, duration: 6762434
2015-11-22 13:24:17,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742075_1251, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:24:17,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742076_1252 src: /192.168.54.130:52672 dest: /192.168.54.130:50010
2015-11-22 13:24:17,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52672, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1791847927_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742076_1252, duration: 20040839
2015-11-22 13:24:17,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742076_1252, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:24:25,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742074_1250 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742074 for deletion
2015-11-22 13:24:25,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742075_1251 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742075 for deletion
2015-11-22 13:24:25,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742076_1252 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742076 for deletion
2015-11-22 13:24:25,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742074_1250 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742074
2015-11-22 13:24:25,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742075_1251 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742075
2015-11-22 13:24:25,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742076_1252 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742076
2015-11-22 13:28:36,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742077_1253 src: /192.168.54.130:52704 dest: /192.168.54.130:50010
2015-11-22 13:28:36,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52704, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1786834849_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742077_1253, duration: 40628526
2015-11-22 13:28:36,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742077_1253, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:28:36,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742078_1254 src: /192.168.54.130:52706 dest: /192.168.54.130:50010
2015-11-22 13:28:36,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52706, dest: /192.168.54.130:50010, bytes: 4889, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1786834849_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742078_1254, duration: 9073077
2015-11-22 13:28:36,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742078_1254, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:28:36,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742079_1255 src: /192.168.54.130:52708 dest: /192.168.54.130:50010
2015-11-22 13:28:36,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52708, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1786834849_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742079_1255, duration: 15717424
2015-11-22 13:28:36,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742079_1255, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:28:43,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742077_1253 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742077 for deletion
2015-11-22 13:28:43,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742078_1254 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742078 for deletion
2015-11-22 13:28:43,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742079_1255 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742079 for deletion
2015-11-22 13:28:43,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742077_1253 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742077
2015-11-22 13:28:43,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742078_1254 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742078
2015-11-22 13:28:43,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742079_1255 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir0/blk_1073742079
2015-11-22 13:30:00,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742080_1256 src: /192.168.54.130:52717 dest: /192.168.54.130:50010
2015-11-22 13:30:00,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52717, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1646107616_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742080_1256, duration: 35438320
2015-11-22 13:30:00,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742080_1256, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:30:00,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742081_1257 src: /192.168.54.130:52719 dest: /192.168.54.130:50010
2015-11-22 13:30:00,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52719, dest: /192.168.54.130:50010, bytes: 4889, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1646107616_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742081_1257, duration: 6654463
2015-11-22 13:30:00,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742081_1257, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:30:01,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742082_1258 src: /192.168.54.130:52721 dest: /192.168.54.130:50010
2015-11-22 13:30:01,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52721, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1646107616_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742082_1258, duration: 23718998
2015-11-22 13:30:01,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742082_1258, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:30:07,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742080_1256 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742080 for deletion
2015-11-22 13:30:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742081_1257 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742081 for deletion
2015-11-22 13:30:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742082_1258 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742082 for deletion
2015-11-22 13:30:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742080_1256 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742080
2015-11-22 13:30:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742081_1257 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742081
2015-11-22 13:30:07,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742082_1258 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742082
2015-11-22 13:37:49,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742083_1259 src: /192.168.54.130:52742 dest: /192.168.54.130:50010
2015-11-22 13:37:49,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52742, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-709990542_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742083_1259, duration: 42634418
2015-11-22 13:37:49,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742083_1259, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:37:49,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742084_1260 src: /192.168.54.130:52744 dest: /192.168.54.130:50010
2015-11-22 13:37:49,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52744, dest: /192.168.54.130:50010, bytes: 4925, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-709990542_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742084_1260, duration: 6163456
2015-11-22 13:37:49,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742084_1260, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:37:49,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742085_1261 src: /192.168.54.130:52746 dest: /192.168.54.130:50010
2015-11-22 13:37:49,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52746, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-709990542_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742085_1261, duration: 17979872
2015-11-22 13:37:49,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742085_1261, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:37:55,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742083_1259 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742083 for deletion
2015-11-22 13:37:55,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742084_1260 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742084 for deletion
2015-11-22 13:37:55,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742085_1261 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742085 for deletion
2015-11-22 13:37:55,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742083_1259 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742083
2015-11-22 13:37:55,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742084_1260 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742084
2015-11-22 13:37:55,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742085_1261 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742085
2015-11-22 13:42:32,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742086_1262 src: /192.168.54.130:52761 dest: /192.168.54.130:50010
2015-11-22 13:42:32,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52761, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_571649337_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742086_1262, duration: 39903108
2015-11-22 13:42:32,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742086_1262, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:42:32,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742087_1263 src: /192.168.54.130:52763 dest: /192.168.54.130:50010
2015-11-22 13:42:32,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52763, dest: /192.168.54.130:50010, bytes: 4931, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_571649337_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742087_1263, duration: 5907515
2015-11-22 13:42:32,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742087_1263, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:42:32,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742088_1264 src: /192.168.54.130:52765 dest: /192.168.54.130:50010
2015-11-22 13:42:32,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52765, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_571649337_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742088_1264, duration: 16574668
2015-11-22 13:42:32,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742088_1264, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 13:42:40,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742086_1262 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742086 for deletion
2015-11-22 13:42:40,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742087_1263 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742087 for deletion
2015-11-22 13:42:40,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742088_1264 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742088 for deletion
2015-11-22 13:42:40,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742086_1262 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742086
2015-11-22 13:42:40,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742087_1263 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742087
2015-11-22 13:42:40,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742088_1264 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742088
2015-11-22 14:13:06,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742089_1265 src: /192.168.54.130:52813 dest: /192.168.54.130:50010
2015-11-22 14:13:06,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52813, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474254614_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742089_1265, duration: 38206300
2015-11-22 14:13:06,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742089_1265, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:13:06,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742090_1266 src: /192.168.54.130:52815 dest: /192.168.54.130:50010
2015-11-22 14:13:06,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52815, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474254614_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742090_1266, duration: 6759451
2015-11-22 14:13:06,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742090_1266, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:13:07,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742091_1267 src: /192.168.54.130:52817 dest: /192.168.54.130:50010
2015-11-22 14:13:07,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52817, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474254614_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742091_1267, duration: 16766770
2015-11-22 14:13:07,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742091_1267, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:13:13,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742089_1265 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742089 for deletion
2015-11-22 14:13:13,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742090_1266 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742090 for deletion
2015-11-22 14:13:13,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742091_1267 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742091 for deletion
2015-11-22 14:13:13,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742089_1265 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742089
2015-11-22 14:13:13,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742090_1266 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742090
2015-11-22 14:13:13,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742091_1267 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742091
2015-11-22 14:13:59,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742092_1268 src: /192.168.54.130:52826 dest: /192.168.54.130:50010
2015-11-22 14:14:00,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52826, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1078923162_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742092_1268, duration: 35423344
2015-11-22 14:14:00,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742092_1268, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:14:00,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742093_1269 src: /192.168.54.130:52828 dest: /192.168.54.130:50010
2015-11-22 14:14:00,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52828, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1078923162_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742093_1269, duration: 5279892
2015-11-22 14:14:00,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742093_1269, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:14:00,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742094_1270 src: /192.168.54.130:52830 dest: /192.168.54.130:50010
2015-11-22 14:14:00,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52830, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1078923162_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742094_1270, duration: 17362282
2015-11-22 14:14:00,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742094_1270, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:14:07,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742092_1268 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742092 for deletion
2015-11-22 14:14:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742092_1268 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742092
2015-11-22 14:14:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742093_1269 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742093 for deletion
2015-11-22 14:14:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742093_1269 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742093
2015-11-22 14:14:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742094_1270 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742094 for deletion
2015-11-22 14:14:07,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742094_1270 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742094
2015-11-22 14:14:55,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742095_1271 src: /192.168.54.130:52839 dest: /192.168.54.130:50010
2015-11-22 14:14:56,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52839, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-96682728_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742095_1271, duration: 39484305
2015-11-22 14:14:56,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742095_1271, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:14:56,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742096_1272 src: /192.168.54.130:52841 dest: /192.168.54.130:50010
2015-11-22 14:14:56,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52841, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-96682728_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742096_1272, duration: 9659307
2015-11-22 14:14:56,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742096_1272, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:14:56,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742097_1273 src: /192.168.54.130:52843 dest: /192.168.54.130:50010
2015-11-22 14:14:56,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52843, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-96682728_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742097_1273, duration: 13585402
2015-11-22 14:14:56,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742097_1273, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:15:04,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742096_1272 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742096 for deletion
2015-11-22 14:15:04,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742097_1273 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742097 for deletion
2015-11-22 14:15:04,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742095_1271 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742095 for deletion
2015-11-22 14:15:04,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742096_1272 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742096
2015-11-22 14:15:04,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742097_1273 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742097
2015-11-22 14:15:04,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742095_1271 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742095
2015-11-22 14:15:54,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742098_1274 src: /192.168.54.130:52854 dest: /192.168.54.130:50010
2015-11-22 14:15:54,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52854, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_818080323_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742098_1274, duration: 53514231
2015-11-22 14:15:54,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742098_1274, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:15:54,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742099_1275 src: /192.168.54.130:52856 dest: /192.168.54.130:50010
2015-11-22 14:15:54,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52856, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_818080323_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742099_1275, duration: 10031646
2015-11-22 14:15:54,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742099_1275, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:15:55,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742100_1276 src: /192.168.54.130:52858 dest: /192.168.54.130:50010
2015-11-22 14:15:55,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52858, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_818080323_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742100_1276, duration: 33266403
2015-11-22 14:15:55,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742100_1276, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:16:10,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742098_1274 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742098 for deletion
2015-11-22 14:16:10,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742099_1275 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742099 for deletion
2015-11-22 14:16:10,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742100_1276 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742100 for deletion
2015-11-22 14:16:10,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742098_1274 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742098
2015-11-22 14:16:10,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742099_1275 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742099
2015-11-22 14:16:10,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742100_1276 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742100
2015-11-22 14:16:22,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742101_1277 src: /192.168.54.130:52868 dest: /192.168.54.130:50010
2015-11-22 14:16:22,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52868, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-234895230_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742101_1277, duration: 44027262
2015-11-22 14:16:22,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742101_1277, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:16:22,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742102_1278 src: /192.168.54.130:52870 dest: /192.168.54.130:50010
2015-11-22 14:16:22,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52870, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-234895230_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742102_1278, duration: 11761619
2015-11-22 14:16:22,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742102_1278, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:16:23,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742103_1279 src: /192.168.54.130:52872 dest: /192.168.54.130:50010
2015-11-22 14:16:23,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52872, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-234895230_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742103_1279, duration: 16281182
2015-11-22 14:16:23,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742103_1279, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:16:34,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742101_1277 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742101 for deletion
2015-11-22 14:16:34,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742102_1278 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742102 for deletion
2015-11-22 14:16:34,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742103_1279 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742103 for deletion
2015-11-22 14:16:34,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742101_1277 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742101
2015-11-22 14:16:34,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742102_1278 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742102
2015-11-22 14:16:34,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742103_1279 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742103
2015-11-22 14:16:54,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742104_1280 src: /192.168.54.130:52880 dest: /192.168.54.130:50010
2015-11-22 14:16:54,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52880, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_288246891_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742104_1280, duration: 45022230
2015-11-22 14:16:54,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742104_1280, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:16:54,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742105_1281 src: /192.168.54.130:52882 dest: /192.168.54.130:50010
2015-11-22 14:16:54,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52882, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_288246891_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742105_1281, duration: 7451265
2015-11-22 14:16:54,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742105_1281, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:16:54,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742106_1282 src: /192.168.54.130:52884 dest: /192.168.54.130:50010
2015-11-22 14:16:54,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52884, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_288246891_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742106_1282, duration: 11749736
2015-11-22 14:16:54,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742106_1282, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 14:19:31,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742104_1280 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742104 for deletion
2015-11-22 14:19:31,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742105_1281 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742105 for deletion
2015-11-22 14:19:31,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742106_1282 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742106 for deletion
2015-11-22 14:19:31,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742104_1280 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742104
2015-11-22 14:19:31,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742105_1281 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742105
2015-11-22 14:19:31,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742106_1282 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742106
2015-11-22 14:42:58,307 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-883961033-192.168.54.130-1447721127864 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-22 15:19:28,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742107_1283 src: /192.168.54.130:52981 dest: /192.168.54.130:50010
2015-11-22 15:19:28,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52981, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1768170243_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742107_1283, duration: 42911115
2015-11-22 15:19:28,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742107_1283, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:19:28,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742108_1284 src: /192.168.54.130:52983 dest: /192.168.54.130:50010
2015-11-22 15:19:28,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52983, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1768170243_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742108_1284, duration: 5915789
2015-11-22 15:19:28,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742108_1284, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:19:28,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742109_1285 src: /192.168.54.130:52985 dest: /192.168.54.130:50010
2015-11-22 15:19:28,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:52985, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1768170243_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742109_1285, duration: 15867231
2015-11-22 15:19:28,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742109_1285, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:21:16,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742107_1283 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742107 for deletion
2015-11-22 15:21:16,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742108_1284 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742108 for deletion
2015-11-22 15:21:16,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742109_1285 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742109 for deletion
2015-11-22 15:21:16,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742107_1283 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742107
2015-11-22 15:21:16,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742108_1284 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742108
2015-11-22 15:21:16,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742109_1285 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742109
2015-11-22 15:26:53,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742110_1286 src: /192.168.54.130:53012 dest: /192.168.54.130:50010
2015-11-22 15:26:53,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53012, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1752416742_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742110_1286, duration: 50719240
2015-11-22 15:26:53,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742110_1286, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:26:53,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742111_1287 src: /192.168.54.130:53014 dest: /192.168.54.130:50010
2015-11-22 15:26:53,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53014, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1752416742_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742111_1287, duration: 4773153
2015-11-22 15:26:53,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742111_1287, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:26:53,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742112_1288 src: /192.168.54.130:53016 dest: /192.168.54.130:50010
2015-11-22 15:26:53,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53016, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1752416742_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742112_1288, duration: 24443087
2015-11-22 15:26:53,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742112_1288, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:51:13,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742112_1288 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742112 for deletion
2015-11-22 15:51:13,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742110_1286 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742110 for deletion
2015-11-22 15:51:13,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742111_1287 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742111 for deletion
2015-11-22 15:51:13,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742112_1288 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742112
2015-11-22 15:51:13,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742110_1286 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742110
2015-11-22 15:51:13,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742111_1287 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742111
2015-11-22 15:55:49,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742113_1289 src: /192.168.54.130:53057 dest: /192.168.54.130:50010
2015-11-22 15:55:49,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53057, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-256714324_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742113_1289, duration: 52410373
2015-11-22 15:55:49,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742113_1289, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:55:49,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742114_1290 src: /192.168.54.130:53059 dest: /192.168.54.130:50010
2015-11-22 15:55:49,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53059, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-256714324_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742114_1290, duration: 4781345
2015-11-22 15:55:49,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742114_1290, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 15:55:50,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742115_1291 src: /192.168.54.130:53061 dest: /192.168.54.130:50010
2015-11-22 15:55:50,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53061, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-256714324_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742115_1291, duration: 12538260
2015-11-22 15:55:50,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742115_1291, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:02:52,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742113_1289 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742113 for deletion
2015-11-22 16:02:52,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742114_1290 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742114 for deletion
2015-11-22 16:02:52,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742115_1291 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742115 for deletion
2015-11-22 16:02:52,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742113_1289 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742113
2015-11-22 16:02:52,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742114_1290 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742114
2015-11-22 16:02:52,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742115_1291 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742115
2015-11-22 16:19:14,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742116_1292 src: /192.168.54.130:53099 dest: /192.168.54.130:50010
2015-11-22 16:19:14,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53099, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-105489820_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742116_1292, duration: 84963100
2015-11-22 16:19:14,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742116_1292, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:19:14,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742117_1293 src: /192.168.54.130:53101 dest: /192.168.54.130:50010
2015-11-22 16:19:14,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53101, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-105489820_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742117_1293, duration: 7039389
2015-11-22 16:19:14,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742117_1293, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:19:14,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742118_1294 src: /192.168.54.130:53103 dest: /192.168.54.130:50010
2015-11-22 16:19:14,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53103, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-105489820_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742118_1294, duration: 10498934
2015-11-22 16:19:14,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742118_1294, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:26:07,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742116_1292 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742116 for deletion
2015-11-22 16:26:07,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742117_1293 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742117 for deletion
2015-11-22 16:26:07,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742118_1294 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742118 for deletion
2015-11-22 16:26:07,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742116_1292 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742116
2015-11-22 16:26:07,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742117_1293 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742117
2015-11-22 16:26:07,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742118_1294 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742118
2015-11-22 16:30:42,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742119_1295 src: /192.168.54.130:53127 dest: /192.168.54.130:50010
2015-11-22 16:30:42,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53127, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327900213_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742119_1295, duration: 40913792
2015-11-22 16:30:42,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742119_1295, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:30:42,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742120_1296 src: /192.168.54.130:53129 dest: /192.168.54.130:50010
2015-11-22 16:30:42,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53129, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327900213_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742120_1296, duration: 5232223
2015-11-22 16:30:42,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742120_1296, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:30:42,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742121_1297 src: /192.168.54.130:53131 dest: /192.168.54.130:50010
2015-11-22 16:30:42,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53131, dest: /192.168.54.130:50010, bytes: 100509, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327900213_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742121_1297, duration: 16938599
2015-11-22 16:30:42,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742121_1297, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:31:04,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742119_1295 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742119 for deletion
2015-11-22 16:31:04,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742120_1296 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742120 for deletion
2015-11-22 16:31:04,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742121_1297 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742121 for deletion
2015-11-22 16:31:04,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742119_1295 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742119
2015-11-22 16:31:04,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742120_1296 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742120
2015-11-22 16:31:04,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742121_1297 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742121
2015-11-22 16:32:21,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742122_1298 src: /192.168.54.130:53141 dest: /192.168.54.130:50010
2015-11-22 16:32:21,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53141, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1411839604_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742122_1298, duration: 43919978
2015-11-22 16:32:21,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742122_1298, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:32:21,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742123_1299 src: /192.168.54.130:53143 dest: /192.168.54.130:50010
2015-11-22 16:32:21,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53143, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1411839604_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742123_1299, duration: 5331538
2015-11-22 16:32:21,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742123_1299, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:32:21,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742124_1300 src: /192.168.54.130:53145 dest: /192.168.54.130:50010
2015-11-22 16:32:21,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53145, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1411839604_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742124_1300, duration: 16993895
2015-11-22 16:32:21,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742124_1300, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:32:25,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742122_1298 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742122 for deletion
2015-11-22 16:32:25,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742123_1299 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742123 for deletion
2015-11-22 16:32:25,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742124_1300 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742124 for deletion
2015-11-22 16:32:25,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742122_1298 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742122
2015-11-22 16:32:25,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742123_1299 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742123
2015-11-22 16:32:25,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742124_1300 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742124
2015-11-22 16:46:52,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742125_1301 src: /192.168.54.130:53169 dest: /192.168.54.130:50010
2015-11-22 16:46:52,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53169, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957352380_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742125_1301, duration: 37602179
2015-11-22 16:46:52,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742125_1301, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:46:52,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742126_1302 src: /192.168.54.130:53171 dest: /192.168.54.130:50010
2015-11-22 16:46:52,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53171, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957352380_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742126_1302, duration: 5020036
2015-11-22 16:46:52,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742126_1302, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:46:52,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742127_1303 src: /192.168.54.130:53173 dest: /192.168.54.130:50010
2015-11-22 16:46:52,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53173, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1957352380_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742127_1303, duration: 15310997
2015-11-22 16:46:52,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742127_1303, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:46:58,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742125_1301 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742125 for deletion
2015-11-22 16:46:58,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742126_1302 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742126 for deletion
2015-11-22 16:46:58,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742127_1303 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742127 for deletion
2015-11-22 16:46:58,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742125_1301 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742125
2015-11-22 16:46:58,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742126_1302 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742126
2015-11-22 16:46:58,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742127_1303 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742127
2015-11-22 16:51:10,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742128_1304 src: /192.168.54.130:53187 dest: /192.168.54.130:50010
2015-11-22 16:51:10,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53187, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1258247725_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742128_1304, duration: 39269126
2015-11-22 16:51:10,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742128_1304, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:51:10,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742129_1305 src: /192.168.54.130:53189 dest: /192.168.54.130:50010
2015-11-22 16:51:10,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53189, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1258247725_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742129_1305, duration: 4771984
2015-11-22 16:51:10,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742129_1305, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:51:11,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742130_1306 src: /192.168.54.130:53191 dest: /192.168.54.130:50010
2015-11-22 16:51:11,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53191, dest: /192.168.54.130:50010, bytes: 100513, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1258247725_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742130_1306, duration: 13987163
2015-11-22 16:51:11,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742130_1306, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 16:52:19,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742128_1304 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742128 for deletion
2015-11-22 16:52:19,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742129_1305 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742129 for deletion
2015-11-22 16:52:19,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742130_1306 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742130 for deletion
2015-11-22 16:52:19,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742128_1304 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742128
2015-11-22 16:52:19,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742129_1305 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742129
2015-11-22 16:52:19,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742130_1306 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742130
2015-11-22 17:39:29,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742131_1307 src: /192.168.54.130:53267 dest: /192.168.54.130:50010
2015-11-22 17:39:29,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53267, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-43889092_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742131_1307, duration: 72831086
2015-11-22 17:39:29,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742131_1307, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:39:29,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742132_1308 src: /192.168.54.130:53269 dest: /192.168.54.130:50010
2015-11-22 17:39:29,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53269, dest: /192.168.54.130:50010, bytes: 5092, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-43889092_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742132_1308, duration: 5044554
2015-11-22 17:39:29,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742132_1308, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:39:29,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742133_1309 src: /192.168.54.130:53271 dest: /192.168.54.130:50010
2015-11-22 17:39:29,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53271, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-43889092_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742133_1309, duration: 14118871
2015-11-22 17:39:29,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742133_1309, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:39:36,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742131_1307 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742131 for deletion
2015-11-22 17:39:36,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742132_1308 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742132 for deletion
2015-11-22 17:39:36,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742133_1309 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742133 for deletion
2015-11-22 17:39:36,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742131_1307 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742131
2015-11-22 17:39:36,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742132_1308 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742132
2015-11-22 17:39:36,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742133_1309 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742133
2015-11-22 17:40:04,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742134_1310 src: /192.168.54.130:53280 dest: /192.168.54.130:50010
2015-11-22 17:40:04,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53280, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1328866893_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742134_1310, duration: 42531974
2015-11-22 17:40:04,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742134_1310, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:40:04,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742135_1311 src: /192.168.54.130:53282 dest: /192.168.54.130:50010
2015-11-22 17:40:04,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53282, dest: /192.168.54.130:50010, bytes: 5092, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1328866893_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742135_1311, duration: 5297914
2015-11-22 17:40:04,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742135_1311, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:40:04,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742136_1312 src: /192.168.54.130:53284 dest: /192.168.54.130:50010
2015-11-22 17:40:04,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53284, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1328866893_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742136_1312, duration: 13379388
2015-11-22 17:40:04,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742136_1312, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:40:12,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742134_1310 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742134 for deletion
2015-11-22 17:40:12,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742134_1310 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742134
2015-11-22 17:40:12,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742135_1311 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742135 for deletion
2015-11-22 17:40:12,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742135_1311 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742135
2015-11-22 17:40:12,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742136_1312 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742136 for deletion
2015-11-22 17:40:12,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742136_1312 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742136
2015-11-22 17:40:47,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742137_1313 src: /192.168.54.130:53293 dest: /192.168.54.130:50010
2015-11-22 17:40:47,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53293, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1614523681_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742137_1313, duration: 38532562
2015-11-22 17:40:47,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742137_1313, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:40:47,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742138_1314 src: /192.168.54.130:53295 dest: /192.168.54.130:50010
2015-11-22 17:40:47,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53295, dest: /192.168.54.130:50010, bytes: 5092, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1614523681_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742138_1314, duration: 5972971
2015-11-22 17:40:47,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742138_1314, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:40:48,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742139_1315 src: /192.168.54.130:53297 dest: /192.168.54.130:50010
2015-11-22 17:40:48,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53297, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1614523681_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742139_1315, duration: 22435390
2015-11-22 17:40:48,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742139_1315, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:40:54,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742137_1313 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742137 for deletion
2015-11-22 17:40:54,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742137_1313 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742137
2015-11-22 17:40:54,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742138_1314 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742138 for deletion
2015-11-22 17:40:54,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742139_1315 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742139 for deletion
2015-11-22 17:40:54,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742138_1314 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742138
2015-11-22 17:40:54,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742139_1315 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742139
2015-11-22 17:41:57,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742140_1316 src: /192.168.54.130:53306 dest: /192.168.54.130:50010
2015-11-22 17:41:57,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53306, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-905818132_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742140_1316, duration: 40641518
2015-11-22 17:41:57,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742140_1316, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:41:57,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742141_1317 src: /192.168.54.130:53308 dest: /192.168.54.130:50010
2015-11-22 17:41:57,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53308, dest: /192.168.54.130:50010, bytes: 5092, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-905818132_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742141_1317, duration: 6462585
2015-11-22 17:41:57,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742141_1317, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:41:57,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742142_1318 src: /192.168.54.130:53310 dest: /192.168.54.130:50010
2015-11-22 17:41:58,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53310, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-905818132_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742142_1318, duration: 17135246
2015-11-22 17:41:58,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742142_1318, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:45:00,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742140_1316 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742140 for deletion
2015-11-22 17:45:00,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742141_1317 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742141 for deletion
2015-11-22 17:45:00,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742142_1318 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742142 for deletion
2015-11-22 17:45:00,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742140_1316 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742140
2015-11-22 17:45:00,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742141_1317 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742141
2015-11-22 17:45:00,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742142_1318 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742142
2015-11-22 17:58:52,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742143_1319 src: /192.168.54.130:53332 dest: /192.168.54.130:50010
2015-11-22 17:58:52,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53332, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1337190821_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742143_1319, duration: 35544847
2015-11-22 17:58:52,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742143_1319, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:58:52,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742144_1320 src: /192.168.54.130:53334 dest: /192.168.54.130:50010
2015-11-22 17:58:52,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53334, dest: /192.168.54.130:50010, bytes: 5092, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1337190821_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742144_1320, duration: 5242961
2015-11-22 17:58:52,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742144_1320, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 17:58:52,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742145_1321 src: /192.168.54.130:53336 dest: /192.168.54.130:50010
2015-11-22 17:58:52,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53336, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1337190821_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742145_1321, duration: 12017565
2015-11-22 17:58:52,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742145_1321, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 18:01:25,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742146_1322 src: /192.168.54.130:53347 dest: /192.168.54.130:50010
2015-11-22 18:01:25,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53347, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1644688798_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742146_1322, duration: 54448417
2015-11-22 18:01:25,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742146_1322, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 18:01:25,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742147_1323 src: /192.168.54.130:53349 dest: /192.168.54.130:50010
2015-11-22 18:01:25,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53349, dest: /192.168.54.130:50010, bytes: 5082, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1644688798_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742147_1323, duration: 7391564
2015-11-22 18:01:25,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742147_1323, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 18:01:25,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742148_1324 src: /192.168.54.130:53351 dest: /192.168.54.130:50010
2015-11-22 18:01:25,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53351, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1644688798_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742148_1324, duration: 16500629
2015-11-22 18:01:25,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742148_1324, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 18:01:37,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742146_1322 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742146 for deletion
2015-11-22 18:01:37,711 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742147_1323 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742147 for deletion
2015-11-22 18:01:37,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742148_1324 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742148 for deletion
2015-11-22 18:01:37,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742146_1322 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742146
2015-11-22 18:01:37,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742147_1323 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742147
2015-11-22 18:01:37,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742148_1324 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742148
2015-11-22 18:02:13,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742144_1320 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742144 for deletion
2015-11-22 18:02:13,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742145_1321 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742145 for deletion
2015-11-22 18:02:13,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742143_1319 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742143 for deletion
2015-11-22 18:02:13,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742144_1320 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742144
2015-11-22 18:02:13,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742145_1321 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742145
2015-11-22 18:02:13,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742143_1319 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742143
2015-11-22 18:03:21,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742149_1325 src: /192.168.54.130:53362 dest: /192.168.54.130:50010
2015-11-22 18:03:21,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53362, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1309679213_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742149_1325, duration: 37724407
2015-11-22 18:03:21,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742149_1325, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 18:03:21,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742150_1326 src: /192.168.54.130:53364 dest: /192.168.54.130:50010
2015-11-22 18:03:21,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53364, dest: /192.168.54.130:50010, bytes: 5082, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1309679213_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742150_1326, duration: 10347036
2015-11-22 18:03:21,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742150_1326, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 18:03:21,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742151_1327 src: /192.168.54.130:53366 dest: /192.168.54.130:50010
2015-11-22 18:03:21,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53366, dest: /192.168.54.130:50010, bytes: 100514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1309679213_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742151_1327, duration: 14621927
2015-11-22 18:03:21,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742151_1327, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 18:03:28,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742149_1325 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742149 for deletion
2015-11-22 18:03:28,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742150_1326 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742150 for deletion
2015-11-22 18:03:28,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742151_1327 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742151 for deletion
2015-11-22 18:03:28,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742149_1325 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742149
2015-11-22 18:03:28,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742150_1326 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742150
2015-11-22 18:03:28,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742151_1327 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742151
2015-11-22 19:00:59,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742152_1328 src: /192.168.54.130:53403 dest: /192.168.54.130:50010
2015-11-22 19:00:59,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53403, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1733850728_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742152_1328, duration: 42405968
2015-11-22 19:00:59,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742152_1328, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:00:59,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742153_1329 src: /192.168.54.130:53405 dest: /192.168.54.130:50010
2015-11-22 19:00:59,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53405, dest: /192.168.54.130:50010, bytes: 5027, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1733850728_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742153_1329, duration: 5080068
2015-11-22 19:00:59,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742153_1329, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:00:59,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742154_1330 src: /192.168.54.130:53407 dest: /192.168.54.130:50010
2015-11-22 19:00:59,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53407, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1733850728_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742154_1330, duration: 16995535
2015-11-22 19:00:59,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742154_1330, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:02:12,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742152_1328 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742152 for deletion
2015-11-22 19:02:12,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742153_1329 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742153 for deletion
2015-11-22 19:02:12,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742152_1328 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742152
2015-11-22 19:02:12,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742154_1330 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742154 for deletion
2015-11-22 19:02:12,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742153_1329 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742153
2015-11-22 19:02:12,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742154_1330 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742154
2015-11-22 19:03:07,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742155_1331 src: /192.168.54.130:53422 dest: /192.168.54.130:50010
2015-11-22 19:03:07,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53422, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-960838084_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742155_1331, duration: 39940657
2015-11-22 19:03:07,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742155_1331, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:03:07,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742156_1332 src: /192.168.54.130:53424 dest: /192.168.54.130:50010
2015-11-22 19:03:07,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53424, dest: /192.168.54.130:50010, bytes: 5027, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-960838084_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742156_1332, duration: 4775350
2015-11-22 19:03:07,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742156_1332, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:03:07,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742157_1333 src: /192.168.54.130:53426 dest: /192.168.54.130:50010
2015-11-22 19:03:07,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53426, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-960838084_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742157_1333, duration: 12102559
2015-11-22 19:03:07,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742157_1333, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:04:24,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742155_1331 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742155 for deletion
2015-11-22 19:04:24,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742156_1332 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742156 for deletion
2015-11-22 19:04:24,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742157_1333 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742157 for deletion
2015-11-22 19:04:24,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742155_1331 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742155
2015-11-22 19:04:24,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742156_1332 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742156
2015-11-22 19:04:24,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742157_1333 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742157
2015-11-22 19:05:52,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742158_1334 src: /192.168.54.130:53438 dest: /192.168.54.130:50010
2015-11-22 19:05:52,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53438, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1319186075_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742158_1334, duration: 42786423
2015-11-22 19:05:52,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742158_1334, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:05:52,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742159_1335 src: /192.168.54.130:53440 dest: /192.168.54.130:50010
2015-11-22 19:05:52,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53440, dest: /192.168.54.130:50010, bytes: 5024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1319186075_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742159_1335, duration: 7222448
2015-11-22 19:05:52,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742159_1335, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:05:52,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742160_1336 src: /192.168.54.130:53442 dest: /192.168.54.130:50010
2015-11-22 19:05:52,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53442, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1319186075_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742160_1336, duration: 20132813
2015-11-22 19:05:52,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742160_1336, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:07:03,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742160_1336 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742160 for deletion
2015-11-22 19:07:03,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742158_1334 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742158 for deletion
2015-11-22 19:07:03,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742159_1335 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742159 for deletion
2015-11-22 19:07:03,965 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742160_1336 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742160
2015-11-22 19:07:03,965 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742158_1334 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742158
2015-11-22 19:07:03,965 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742159_1335 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742159
2015-11-22 19:07:58,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742161_1337 src: /192.168.54.130:53454 dest: /192.168.54.130:50010
2015-11-22 19:07:58,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53454, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-650553907_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742161_1337, duration: 50043993
2015-11-22 19:07:58,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742161_1337, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:07:58,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742162_1338 src: /192.168.54.130:53456 dest: /192.168.54.130:50010
2015-11-22 19:07:58,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53456, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-650553907_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742162_1338, duration: 6358356
2015-11-22 19:07:58,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742162_1338, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:07:58,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742163_1339 src: /192.168.54.130:53458 dest: /192.168.54.130:50010
2015-11-22 19:07:58,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53458, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-650553907_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742163_1339, duration: 26741411
2015-11-22 19:07:58,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742163_1339, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:08:06,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742161_1337 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742161 for deletion
2015-11-22 19:08:06,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742162_1338 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742162 for deletion
2015-11-22 19:08:06,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742163_1339 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742163 for deletion
2015-11-22 19:08:06,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742161_1337 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742161
2015-11-22 19:08:06,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742162_1338 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742162
2015-11-22 19:08:06,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742163_1339 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742163
2015-11-22 19:10:24,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742164_1340 src: /192.168.54.130:53470 dest: /192.168.54.130:50010
2015-11-22 19:10:24,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53470, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_61652505_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742164_1340, duration: 37536165
2015-11-22 19:10:24,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742164_1340, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:10:24,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742165_1341 src: /192.168.54.130:53472 dest: /192.168.54.130:50010
2015-11-22 19:10:24,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53472, dest: /192.168.54.130:50010, bytes: 5059, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_61652505_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742165_1341, duration: 6990603
2015-11-22 19:10:24,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742165_1341, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:10:25,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742166_1342 src: /192.168.54.130:53474 dest: /192.168.54.130:50010
2015-11-22 19:10:25,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53474, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_61652505_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742166_1342, duration: 10367247
2015-11-22 19:10:25,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742166_1342, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 19:11:33,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742164_1340 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742164 for deletion
2015-11-22 19:11:33,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742165_1341 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742165 for deletion
2015-11-22 19:11:33,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742166_1342 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742166 for deletion
2015-11-22 19:11:33,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742164_1340 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742164
2015-11-22 19:11:33,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742165_1341 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742165
2015-11-22 19:11:33,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742166_1342 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742166
2015-11-22 19:19:08,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742167_1343 src: /192.168.54.130:53496 dest: /192.168.54.130:50010
2015-11-22 19:19:10,276 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742167_1343 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:19:10,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742167_1343 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:19:10,523 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53496 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:10,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 src: /192.168.54.130:53498 dest: /192.168.54.130:50010
2015-11-22 19:19:10,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53498, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-738322010_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344, duration: 38041105
2015-11-22 19:19:10,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:19:11,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742169_1345 src: /192.168.54.130:53499 dest: /192.168.54.130:50010
2015-11-22 19:19:13,238 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742169_1345 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:19:13,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742169_1345 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:19:13,238 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53499 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:13,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 src: /192.168.54.130:53501 dest: /192.168.54.130:50010
2015-11-22 19:19:13,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53501, dest: /192.168.54.130:50010, bytes: 5074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-738322010_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346, duration: 12187582
2015-11-22 19:19:13,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:19:13,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742171_1347 src: /192.168.54.130:53502 dest: /192.168.54.130:50010
2015-11-22 19:19:16,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 
2015-11-22 19:19:16,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:19:16,238 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=2}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742171_1347 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:19:16,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742171_1347 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:19:16,238 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53502 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:16,239 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:16,245 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:16,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 src: /192.168.54.130:53506 dest: /192.168.54.130:50010
2015-11-22 19:19:16,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2015-11-22 19:19:16,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53506, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-738322010_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348, duration: 7397934
2015-11-22 19:19:16,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:19:18,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 
2015-11-22 19:19:19,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:21,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 
2015-11-22 19:19:21,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:19:22,238 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:22,239 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:24,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 
2015-11-22 19:19:25,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:27,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:19:28,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:19:34,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:39,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:19:40,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:45,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:19:46,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:51,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:19:52,236 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:19:57,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:20:00,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:20:03,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:20:06,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:24,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742173_1349 src: /192.168.54.130:53537 dest: /192.168.54.130:50010
2015-11-22 19:22:27,757 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742173_1349 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:22:27,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742173_1349 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:22:27,758 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53537 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:27,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 src: /192.168.54.130:53539 dest: /192.168.54.130:50010
2015-11-22 19:22:27,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53539, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-551832542_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350, duration: 17084673
2015-11-22 19:22:27,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:22:27,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742175_1351 src: /192.168.54.130:53540 dest: /192.168.54.130:50010
2015-11-22 19:22:30,757 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742175_1351 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:22:30,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742175_1351 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:22:30,757 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53540 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:30,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 src: /192.168.54.130:53542 dest: /192.168.54.130:50010
2015-11-22 19:22:30,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53542, dest: /192.168.54.130:50010, bytes: 5074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-551832542_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352, duration: 16191662
2015-11-22 19:22:30,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:22:30,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 
2015-11-22 19:22:31,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742177_1353 src: /192.168.54.130:53544 dest: /192.168.54.130:50010
2015-11-22 19:22:33,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:33,763 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742177_1353 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:22:33,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742177_1353 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:22:33,763 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53544 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:33,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 src: /192.168.54.130:53546 dest: /192.168.54.130:50010
2015-11-22 19:22:33,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53546, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-551832542_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354, duration: 2605993
2015-11-22 19:22:33,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:22:33,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:22:36,758 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:36,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 
2015-11-22 19:22:36,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:22:36,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742177_1353 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742177 for deletion
2015-11-22 19:22:36,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742177_1353 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742177
2015-11-22 19:22:39,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:39,758 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:39,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 
2015-11-22 19:22:42,758 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:42,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:22:42,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 
2015-11-22 19:22:45,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:45,758 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:48,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:22:51,758 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:22:54,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:22:57,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:00,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:23:03,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:06,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:23:09,756 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:12,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:23:15,960 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:17,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742179_1355 src: /192.168.54.130:53574 dest: /192.168.54.130:50010
2015-11-22 19:23:18,965 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742179_1355 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:23:18,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742179_1355 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:23:18,965 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53574 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:18,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:23:19,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 src: /192.168.54.130:53577 dest: /192.168.54.130:50010
2015-11-22 19:23:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53577, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-924781231_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356, duration: 5032363
2015-11-22 19:23:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:23:19,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742181_1357 src: /192.168.54.130:53578 dest: /192.168.54.130:50010
2015-11-22 19:23:21,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 
2015-11-22 19:23:21,967 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742181_1357 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:23:21,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742181_1357 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:23:21,967 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53578 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:21,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 src: /192.168.54.130:53580 dest: /192.168.54.130:50010
2015-11-22 19:23:21,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53580, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-924781231_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358, duration: 1756584
2015-11-22 19:23:22,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:23:22,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742183_1359 src: /192.168.54.130:53582 dest: /192.168.54.130:50010
2015-11-22 19:23:24,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:24,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:23:24,968 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742183_1359 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:23:24,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742183_1359 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:23:24,968 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53582 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:24,969 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742181_1357 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742181 for deletion
2015-11-22 19:23:24,972 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742181_1357 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742181
2015-11-22 19:23:24,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 src: /192.168.54.130:53584 dest: /192.168.54.130:50010
2015-11-22 19:23:24,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53584, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-924781231_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360, duration: 11098185
2015-11-22 19:23:25,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:23:27,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:27,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 
2015-11-22 19:23:27,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742183_1359 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742183 for deletion
2015-11-22 19:23:27,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742183_1359 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742183
2015-11-22 19:23:30,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:23:30,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:33,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 
2015-11-22 19:23:33,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:36,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:23:36,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:39,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 
2015-11-22 19:23:39,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:42,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:23:42,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:45,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:23:45,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:48,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:51,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:23:54,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:23:57,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:24:00,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:24:03,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:24:06,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:24:09,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:24:12,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:15,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 
2015-11-22 19:27:15,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:27:18,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:18,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:21,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 
2015-11-22 19:27:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 
2015-11-22 19:27:24,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:24,964 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:27,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:27:27,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 
2015-11-22 19:27:30,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:30,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:27:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:27:36,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:36,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:39,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:27:42,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:45,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:27:48,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:51,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:27:54,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:27:57,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:28:00,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:28:03,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:28:06,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:30,559 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1340ms
No GCs detected
2015-11-22 19:32:42,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 
2015-11-22 19:32:42,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 
2015-11-22 19:32:42,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 
2015-11-22 19:32:45,020 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:45,021 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:45,021 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:45,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:32:45,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:32:48,021 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:48,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:48,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 
2015-11-22 19:32:48,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 
2015-11-22 19:32:51,020 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:51,023 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:51,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:32:51,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 
2015-11-22 19:32:54,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:54,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:54,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 
2015-11-22 19:32:54,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 
2015-11-22 19:32:57,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:57,023 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:32:57,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:32:57,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:00,021 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:00,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:00,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:33:03,021 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:03,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:03,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:33:06,021 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:06,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:09,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:09,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:33:12,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:12,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:15,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:15,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:33:18,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:18,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:21,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:21,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:33:24,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:24,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:27,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:27,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:33:30,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:30,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:36,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:33:41,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:33:44,273 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:15,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 
2015-11-22 19:37:15,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:37:18,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:18,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 
2015-11-22 19:37:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 
2015-11-22 19:37:21,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:21,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742172_1348 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:24,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:37:24,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 
2015-11-22 19:37:24,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:24,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742168_1344 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:27,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:37:27,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:37:33,968 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:39,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:37:42,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:45,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:37:48,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:51,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:37:54,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:37:57,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:38:00,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:38:03,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 
2015-11-22 19:38:06,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742170_1346 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:15,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 
2015-11-22 19:42:15,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 
2015-11-22 19:42:18,796 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:18,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:18,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 
2015-11-22 19:42:18,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:42:20,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742185_1361 src: /192.168.54.130:53780 dest: /192.168.54.130:50010
2015-11-22 19:42:21,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:21,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:21,800 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742185_1361 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:42:21,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742185_1361 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:42:21,800 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53780 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:21,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742186_1362 src: /192.168.54.130:53782 dest: /192.168.54.130:50010
2015-11-22 19:42:21,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53782, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_42910473_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742186_1362, duration: 18956990
2015-11-22 19:42:21,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742186_1362, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:42:21,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742187_1363 src: /192.168.54.130:53783 dest: /192.168.54.130:50010
2015-11-22 19:42:21,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:42:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 
2015-11-22 19:42:24,797 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=2}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742187_1363 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:42:24,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742187_1363 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:42:24,798 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53783 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:24,800 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:24,803 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:24,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 src: /192.168.54.130:53787 dest: /192.168.54.130:50010
2015-11-22 19:42:24,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53787, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_42910473_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364, duration: 6457729
2015-11-22 19:42:24,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:42:24,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 
2015-11-22 19:42:24,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 
2015-11-22 19:42:24,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742185_1361 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742185 for deletion
2015-11-22 19:42:24,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742185_1361 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742185
2015-11-22 19:42:25,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742189_1365 src: /192.168.54.130:53790 dest: /192.168.54.130:50010
2015-11-22 19:42:27,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742174_1350 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:27,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742184_1360 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:27,801 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742189_1365 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:42:27,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742189_1365 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:42:27,802 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53790 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:27,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742190_1366 src: /192.168.54.130:53792 dest: /192.168.54.130:50010
2015-11-22 19:42:27,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53792, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_42910473_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742190_1366, duration: 3530702
2015-11-22 19:42:27,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742190_1366, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:42:27,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:42:27,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 
2015-11-22 19:42:30,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:30,799 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742178_1354 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:30,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 
2015-11-22 19:42:30,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:42:30,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742189_1365 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742189 for deletion
2015-11-22 19:42:30,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742189_1365 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742189
2015-11-22 19:42:33,796 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742180_1356 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:33,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742186_1362 to 192.168.54.132:50010 
2015-11-22 19:42:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:42:36,796 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742186_1362 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:36,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:36,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:42:36,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:42:39,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:39,799 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:39,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742190_1366 to 192.168.54.132:50010 
2015-11-22 19:42:39,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:42:42,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742190_1366 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:42,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:42,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:42:42,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742186_1362 to 192.168.54.132:50010 
2015-11-22 19:42:45,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:45,800 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742186_1362 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:45,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742190_1366 to 192.168.54.132:50010 
2015-11-22 19:42:45,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:42:48,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742190_1366 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:48,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:48,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:42:48,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:42:51,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:51,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:51,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:42:51,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:42:54,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:54,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:54,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:42:54,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:42:57,796 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:57,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:42:57,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:42:57,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:43:00,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:00,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:00,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:43:03,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:03,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:43:03,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:43:06,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:06,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:06,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:43:09,796 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:09,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 
2015-11-22 19:43:09,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 
2015-11-22 19:43:12,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742182_1358 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:12,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742176_1352 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:12,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:43:15,799 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:15,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:43:18,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:21,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 
2015-11-22 19:43:24,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742188_1364 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:43:54,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742176_1352 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742176 for deletion
2015-11-22 19:43:54,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742178_1354 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742178 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742180_1356 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742180 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742182_1358 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742182 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742184_1360 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742184 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742168_1344 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742168 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742170_1346 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742170 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742186_1362 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742186 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742172_1348 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742172 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742188_1364 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742188 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742190_1366 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742190 for deletion
2015-11-22 19:43:54,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742174_1350 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742174 for deletion
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742176_1352 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742176
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742178_1354 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742178
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742180_1356 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742180
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742182_1358 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742182
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742184_1360 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742184
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742168_1344 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742168
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742170_1346 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742170
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742186_1362 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742186
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742172_1348 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742172
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742188_1364 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742188
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742190_1366 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742190
2015-11-22 19:43:54,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742174_1350 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742174
2015-11-22 19:44:16,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742191_1367 src: /192.168.54.130:53878 dest: /192.168.54.130:50010
2015-11-22 19:44:18,797 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742191_1367 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:44:18,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742191_1367 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:44:18,797 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53878 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:44:18,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742192_1368 src: /192.168.54.130:53880 dest: /192.168.54.130:50010
2015-11-22 19:44:18,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53880, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_944221204_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742192_1368, duration: 16034599
2015-11-22 19:44:18,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742192_1368, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:44:18,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742193_1369 src: /192.168.54.130:53881 dest: /192.168.54.130:50010
2015-11-22 19:44:21,797 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742193_1369 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:44:21,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742193_1369 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:44:21,798 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53881 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:44:21,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742194_1370 src: /192.168.54.130:53883 dest: /192.168.54.130:50010
2015-11-22 19:44:21,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53883, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_944221204_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742194_1370, duration: 7016098
2015-11-22 19:44:21,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742194_1370, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:44:21,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742192_1368 to 192.168.54.132:50010 
2015-11-22 19:44:21,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742191_1367 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742191 for deletion
2015-11-22 19:44:21,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742191_1367 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742191
2015-11-22 19:44:21,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742195_1371 src: /192.168.54.130:53885 dest: /192.168.54.130:50010
2015-11-22 19:44:24,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742192_1368 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:44:24,801 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/usr/local/hadoop-2.7.1/hdfs/datanode/current]'}, localName='hadoopmaster:50010', datanodeUuid='f89b6cdf-a730-4a11-9e8f-4220150b1d5c', xmitsInProgress=0}:Exception transfering block BP-883961033-192.168.54.130-1447721127864:blk_1073742195_1371 to mirror 192.168.54.132:50010: java.net.NoRouteToHostException: No route to host
2015-11-22 19:44:24,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-883961033-192.168.54.130-1447721127864:blk_1073742195_1371 received exception java.net.NoRouteToHostException: No route to host
2015-11-22 19:44:24,801 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hadoopmaster:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.54.130:53885 dst: /192.168.54.130:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:44:24,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-883961033-192.168.54.130-1447721127864:blk_1073742196_1372 src: /192.168.54.130:53887 dest: /192.168.54.130:50010
2015-11-22 19:44:24,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:53887, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_944221204_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-883961033-192.168.54.130-1447721127864:blk_1073742196_1372, duration: 4227991
2015-11-22 19:44:24,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-883961033-192.168.54.130-1447721127864:blk_1073742196_1372, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-22 19:44:24,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742194_1370 to 192.168.54.132:50010 
2015-11-22 19:44:27,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742194_1370 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:44:27,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742192_1368 to 192.168.54.132:50010 
2015-11-22 19:44:27,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0) Starting thread to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742194_1370 to 192.168.54.132:50010 
2015-11-22 19:44:27,965 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742195_1371 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742195 for deletion
2015-11-22 19:44:27,965 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742195_1371 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/rbw/blk_1073742195
2015-11-22 19:44:30,797 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742192_1368 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:44:30,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.54.130:50010, datanodeUuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59;nsid=739049323;c=0):Failed to transfer BP-883961033-192.168.54.130-1447721127864:blk_1073742194_1370 to 192.168.54.132:50010 got 
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2090)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:44:30,994 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Can't replicate block BP-883961033-192.168.54.130-1447721127864:blk_1073742196_1372 because on-disk length 100512 is shorter than NameNode recorded length 9223372036854775807
2015-11-22 19:44:30,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742192_1368 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742192 for deletion
2015-11-22 19:44:30,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742192_1368 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742192
2015-11-22 19:44:30,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742194_1370 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742194 for deletion
2015-11-22 19:44:30,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742194_1370 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742194
2015-11-22 19:44:30,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742196_1372 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742196 for deletion
2015-11-22 19:44:30,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-883961033-192.168.54.130-1447721127864 blk_1073742196_1372 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-883961033-192.168.54.130-1447721127864/current/finalized/subdir0/subdir1/blk_1073742196
2015-11-22 19:51:06,977 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoopmaster/192.168.54.130"; destination host is: "hadoopmaster":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-22 19:51:10,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:11,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:12,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:13,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:14,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:15,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:16,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:17,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:18,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:19,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:19,977 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-22 19:51:20,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:21,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:22,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:23,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:24,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:25,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:26,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:27,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:28,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:29,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:29,990 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-22 19:51:30,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:31,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:32,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:33,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:34,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:35,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:36,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:37,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:38,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:40,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:40,001 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-22 19:51:41,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:42,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:43,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:44,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:45,007 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:46,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:47,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:48,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:49,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:50,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:50,012 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-22 19:51:51,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:52,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:53,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:54,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:55,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:56,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:57,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:58,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:51:59,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:52:00,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:52:00,023 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From hadoopmaster/192.168.54.130 to hadoopmaster:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-22 19:52:01,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:52:02,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:52:03,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:52:04,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:52:05,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoopmaster/192.168.54.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-22 19:52:05,382 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-22 19:52:05,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-11-22 19:56:19,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-commons-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-core-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-graph-0.7.0.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-22 19:56:19,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-22 19:56:19,865 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-11-22 19:56:20,355 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-22 19:56:20,504 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-22 19:56:20,504 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-22 19:56:20,524 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-22 19:56:20,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-11-22 19:56:20,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-22 19:56:20,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-22 19:56:20,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-22 19:56:20,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-22 19:56:20,811 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-22 19:56:20,832 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-22 19:56:20,844 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-22 19:56:20,854 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-22 19:56:20,865 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-22 19:56:20,865 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-22 19:56:20,865 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-22 19:56:20,893 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54397
2015-11-22 19:56:20,893 INFO org.mortbay.log: jetty-6.1.26
2015-11-22 19:56:21,168 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54397
2015-11-22 19:56:21,358 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-22 19:56:21,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-11-22 19:56:21,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-22 19:56:21,462 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-22 19:56:21,490 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-22 19:56:21,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-22 19:56:21,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-22 19:56:21,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-22 19:56:21,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-11-22 19:56:21,655 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-22 19:56:21,657 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-22 19:56:22,050 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 7307@hadoopmaster
2015-11-22 19:56:22,056 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop-2.7.1/hdfs/datanode: namenode clusterID = CID-e01bde2d-adbc-4f38-a3b8-f1ebd8c35878; datanode clusterID = CID-b47b390e-382f-4d00-8d6f-4acfa1dddc59
2015-11-22 19:56:22,057 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-11-22 19:56:22,061 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000
2015-11-22 19:56:22,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-11-22 19:56:24,065 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-11-22 19:56:24,067 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-11-22 19:56:24,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoopmaster/192.168.54.130
************************************************************/
2015-11-22 20:01:04,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoopmaster/192.168.54.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-commons-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-core-0.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hama-graph-0.7.0.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-22 20:01:04,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-22 20:01:05,584 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-11-22 20:01:06,177 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-22 20:01:06,335 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-22 20:01:06,335 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-22 20:01:06,404 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-22 20:01:06,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoopmaster
2015-11-22 20:01:06,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-22 20:01:06,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-22 20:01:06,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-22 20:01:06,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-22 20:01:06,708 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-22 20:01:06,735 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-22 20:01:06,749 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-22 20:01:06,759 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-22 20:01:06,771 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-22 20:01:06,771 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-22 20:01:06,771 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-22 20:01:06,794 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34000
2015-11-22 20:01:06,794 INFO org.mortbay.log: jetty-6.1.26
2015-11-22 20:01:07,079 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34000
2015-11-22 20:01:07,237 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-22 20:01:07,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = de-weikung
2015-11-22 20:01:07,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-22 20:01:07,365 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-22 20:01:07,386 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-22 20:01:07,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-22 20:01:07,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-22 20:01:07,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-22 20:01:07,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoopmaster/192.168.54.130:9000 starting to offer service
2015-11-22 20:01:07,587 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-22 20:01:07,588 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-22 20:01:07,843 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop-2.7.1/hdfs/datanode/in_use.lock acquired by nodename 7676@hadoopmaster
2015-11-22 20:01:07,974 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1577938953-192.168.54.130-1448250860506
2015-11-22 20:01:07,974 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506
2015-11-22 20:01:07,975 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506 is not formatted for BP-1577938953-192.168.54.130-1448250860506
2015-11-22 20:01:07,975 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-11-22 20:01:07,975 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1577938953-192.168.54.130-1448250860506 directory /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current
2015-11-22 20:01:07,976 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-22 20:01:07,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=293514691;bpid=BP-1577938953-192.168.54.130-1448250860506;lv=-56;nsInfo=lv=-63;cid=CID-e01bde2d-adbc-4f38-a3b8-f1ebd8c35878;nsid=293514691;c=0;bpid=BP-1577938953-192.168.54.130-1448250860506;dnuuid=f89b6cdf-a730-4a11-9e8f-4220150b1d5c
2015-11-22 20:01:08,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-9542acc5-b77c-490d-b6f6-36292d9799cd
2015-11-22 20:01:08,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop-2.7.1/hdfs/datanode/current, StorageType: DISK
2015-11-22 20:01:08,129 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-22 20:01:08,129 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1577938953-192.168.54.130-1448250860506
2015-11-22 20:01:08,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1577938953-192.168.54.130-1448250860506 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-22 20:01:08,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1577938953-192.168.54.130-1448250860506 on /usr/local/hadoop-2.7.1/hdfs/datanode/current: 19ms
2015-11-22 20:01:08,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1577938953-192.168.54.130-1448250860506: 21ms
2015-11-22 20:01:08,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1577938953-192.168.54.130-1448250860506 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current...
2015-11-22 20:01:08,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1577938953-192.168.54.130-1448250860506 on volume /usr/local/hadoop-2.7.1/hdfs/datanode/current: 0ms
2015-11-22 20:01:08,151 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2015-11-22 20:01:08,344 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1577938953-192.168.54.130-1448250860506 on volume /usr/local/hadoop-2.7.1/hdfs/datanode
2015-11-22 20:01:08,349 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): finished scanning block pool BP-1577938953-192.168.54.130-1448250860506
2015-11-22 20:01:08,354 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448258177354 with interval 21600000
2015-11-22 20:01:08,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1577938953-192.168.54.130-1448250860506 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 beginning handshake with NN
2015-11-22 20:01:08,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1577938953-192.168.54.130-1448250860506 (Datanode Uuid null) service to hadoopmaster/192.168.54.130:9000 successfully registered with NN
2015-11-22 20:01:08,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoopmaster/192.168.54.130:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-22 20:01:08,479 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop-2.7.1/hdfs/datanode, DS-9542acc5-b77c-490d-b6f6-36292d9799cd): no suitable block pools found to scan.  Waiting 1814399865 ms.
2015-11-22 20:01:08,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1577938953-192.168.54.130-1448250860506 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000 trying to claim ACTIVE state with txid=11
2015-11-22 20:01:08,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1577938953-192.168.54.130-1448250860506 (Datanode Uuid f89b6cdf-a730-4a11-9e8f-4220150b1d5c) service to hadoopmaster/192.168.54.130:9000
2015-11-22 20:01:08,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x79c77f036127,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 70 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-22 20:01:08,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1577938953-192.168.54.130-1448250860506
2015-11-22 20:06:58,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741825_1001 src: /192.168.54.130:54038 dest: /192.168.54.130:50010
2015-11-22 20:06:58,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54038, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1258311876_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741825_1001, duration: 104109674
2015-11-22 20:06:58,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 20:06:58,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741826_1002 src: /192.168.54.130:54040 dest: /192.168.54.130:50010
2015-11-22 20:06:58,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54040, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1258311876_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741826_1002, duration: 5916182
2015-11-22 20:06:58,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 20:06:59,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741827_1003 src: /192.168.54.130:54042 dest: /192.168.54.130:50010
2015-11-22 20:06:59,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54042, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1258311876_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741827_1003, duration: 15021867
2015-11-22 20:06:59,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 20:07:10,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-11-22 20:07:10,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2015-11-22 20:07:10,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2015-11-22 20:07:10,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741825_1001 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741825
2015-11-22 20:07:10,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741826_1002 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741826
2015-11-22 20:07:10,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741827_1003 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741827
2015-11-22 20:17:13,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741828_1004 src: /192.168.54.130:54077 dest: /192.168.54.130:50010
2015-11-22 20:17:13,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54077, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1007122265_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741828_1004, duration: 36759206
2015-11-22 20:17:13,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 20:17:13,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741829_1005 src: /192.168.54.130:54079 dest: /192.168.54.130:50010
2015-11-22 20:17:13,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54079, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1007122265_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741829_1005, duration: 5394991
2015-11-22 20:17:13,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 20:17:14,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741830_1006 src: /192.168.54.130:54081 dest: /192.168.54.130:50010
2015-11-22 20:17:14,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54081, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1007122265_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741830_1006, duration: 11618116
2015-11-22 20:17:14,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 20:48:52,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2015-11-22 20:48:52,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-11-22 20:48:52,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-11-22 20:48:52,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741828_1004 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741828
2015-11-22 20:48:52,471 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741829_1005 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741829
2015-11-22 20:48:52,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741830_1006 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741830
2015-11-22 21:02:07,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741831_1007 src: /192.168.54.130:54147 dest: /192.168.54.130:50010
2015-11-22 21:02:07,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54147, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_50079310_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741831_1007, duration: 47970711
2015-11-22 21:02:07,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:02:07,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741832_1008 src: /192.168.54.130:54149 dest: /192.168.54.130:50010
2015-11-22 21:02:07,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54149, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_50079310_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741832_1008, duration: 7165623
2015-11-22 21:02:07,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:02:07,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741833_1009 src: /192.168.54.130:54151 dest: /192.168.54.130:50010
2015-11-22 21:02:07,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54151, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_50079310_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741833_1009, duration: 7987714
2015-11-22 21:02:07,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741833_1009, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:02:40,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-11-22 21:02:40,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-11-22 21:02:40,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-11-22 21:02:40,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741831_1007 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741831
2015-11-22 21:02:40,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741832_1008 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741832
2015-11-22 21:02:40,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741833_1009 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741833
2015-11-22 21:04:03,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741834_1010 src: /192.168.54.130:54161 dest: /192.168.54.130:50010
2015-11-22 21:04:03,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54161, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-169490600_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741834_1010, duration: 67532129
2015-11-22 21:04:03,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741834_1010, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:04:03,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741835_1011 src: /192.168.54.130:54163 dest: /192.168.54.130:50010
2015-11-22 21:04:03,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54163, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-169490600_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741835_1011, duration: 5204119
2015-11-22 21:04:03,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741835_1011, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:04:04,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741836_1012 src: /192.168.54.130:54165 dest: /192.168.54.130:50010
2015-11-22 21:04:04,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54165, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-169490600_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741836_1012, duration: 9084251
2015-11-22 21:04:04,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741836_1012, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:04:34,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-11-22 21:04:34,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2015-11-22 21:04:34,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-11-22 21:04:34,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741834_1010 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741834
2015-11-22 21:04:34,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741835_1011 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741835
2015-11-22 21:04:34,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741836_1012 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741836
2015-11-22 21:09:11,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741837_1013 src: /192.168.54.130:54179 dest: /192.168.54.130:50010
2015-11-22 21:09:11,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54179, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-912074147_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741837_1013, duration: 31460361
2015-11-22 21:09:11,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741837_1013, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:09:11,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741838_1014 src: /192.168.54.130:54181 dest: /192.168.54.130:50010
2015-11-22 21:09:11,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54181, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-912074147_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741838_1014, duration: 3897901
2015-11-22 21:09:11,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741838_1014, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:09:11,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741839_1015 src: /192.168.54.130:54183 dest: /192.168.54.130:50010
2015-11-22 21:09:11,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54183, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-912074147_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741839_1015, duration: 10629192
2015-11-22 21:09:11,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741839_1015, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:09:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-11-22 21:09:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-11-22 21:09:19,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-11-22 21:09:19,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741837_1013 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741837
2015-11-22 21:09:19,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741838_1014 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741838
2015-11-22 21:09:19,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741839_1015 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741839
2015-11-22 21:10:54,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741840_1016 src: /192.168.54.130:54194 dest: /192.168.54.130:50010
2015-11-22 21:10:54,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54194, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-30560596_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741840_1016, duration: 37207209
2015-11-22 21:10:54,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741840_1016, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:10:54,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741841_1017 src: /192.168.54.130:54196 dest: /192.168.54.130:50010
2015-11-22 21:10:54,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54196, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-30560596_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741841_1017, duration: 3723981
2015-11-22 21:10:54,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741841_1017, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:10:54,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741842_1018 src: /192.168.54.130:54198 dest: /192.168.54.130:50010
2015-11-22 21:10:54,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54198, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-30560596_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741842_1018, duration: 7982375
2015-11-22 21:10:54,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741842_1018, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:11:19,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-11-22 21:11:19,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-11-22 21:11:19,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-11-22 21:11:19,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741840_1016 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741840
2015-11-22 21:11:19,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741841_1017 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741841
2015-11-22 21:11:19,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741842_1018 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741842
2015-11-22 21:14:06,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741843_1019 src: /192.168.54.130:54214 dest: /192.168.54.130:50010
2015-11-22 21:14:06,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54214, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1139815646_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741843_1019, duration: 27208144
2015-11-22 21:14:06,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741843_1019, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:14:06,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741844_1020 src: /192.168.54.130:54216 dest: /192.168.54.130:50010
2015-11-22 21:14:06,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54216, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1139815646_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741844_1020, duration: 3745883
2015-11-22 21:14:06,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741844_1020, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:14:06,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741845_1021 src: /192.168.54.130:54218 dest: /192.168.54.130:50010
2015-11-22 21:14:06,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54218, dest: /192.168.54.130:50010, bytes: 100511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1139815646_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741845_1021, duration: 7126913
2015-11-22 21:14:06,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:14:25,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-11-22 21:14:25,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-11-22 21:14:25,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-11-22 21:14:25,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741843_1019 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741843
2015-11-22 21:14:25,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741844_1020 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741844
2015-11-22 21:14:25,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741845_1021 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741845
2015-11-22 21:18:02,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741846_1022 src: /192.168.54.130:54231 dest: /192.168.54.130:50010
2015-11-22 21:18:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54231, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_167537360_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741846_1022, duration: 38677065
2015-11-22 21:18:02,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:18:02,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741847_1023 src: /192.168.54.130:54233 dest: /192.168.54.130:50010
2015-11-22 21:18:02,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54233, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_167537360_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741847_1023, duration: 6184222
2015-11-22 21:18:02,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741847_1023, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:18:02,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741848_1024 src: /192.168.54.130:54235 dest: /192.168.54.130:50010
2015-11-22 21:18:02,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54235, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_167537360_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741848_1024, duration: 12047152
2015-11-22 21:18:02,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741848_1024, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:18:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-11-22 21:18:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-11-22 21:18:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-11-22 21:18:13,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741846_1022 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741846
2015-11-22 21:18:13,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741847_1023 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741847
2015-11-22 21:18:13,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741848_1024 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741848
2015-11-22 21:24:58,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741849_1025 src: /192.168.54.130:54251 dest: /192.168.54.130:50010
2015-11-22 21:24:58,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54251, dest: /192.168.54.130:50010, bytes: 120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1155598964_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741849_1025, duration: 37503830
2015-11-22 21:24:58,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741849_1025, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:24:58,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741850_1026 src: /192.168.54.130:54253 dest: /192.168.54.130:50010
2015-11-22 21:24:58,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54253, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1155598964_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741850_1026, duration: 3912372
2015-11-22 21:24:58,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741850_1026, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:24:58,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741851_1027 src: /192.168.54.130:54255 dest: /192.168.54.130:50010
2015-11-22 21:24:58,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54255, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1155598964_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741851_1027, duration: 6928779
2015-11-22 21:24:58,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741851_1027, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:25:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2015-11-22 21:25:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2015-11-22 21:25:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2015-11-22 21:25:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741849_1025 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741849
2015-11-22 21:25:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741850_1026 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741850
2015-11-22 21:25:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741851_1027 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741851
2015-11-22 21:28:40,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741852_1028 src: /192.168.54.130:54268 dest: /192.168.54.130:50010
2015-11-22 21:28:40,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54268, dest: /192.168.54.130:50010, bytes: 120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_603123127_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741852_1028, duration: 31024647
2015-11-22 21:28:40,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741852_1028, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:28:40,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741853_1029 src: /192.168.54.130:54270 dest: /192.168.54.130:50010
2015-11-22 21:28:40,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54270, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_603123127_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741853_1029, duration: 3995873
2015-11-22 21:28:40,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741853_1029, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:28:40,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741854_1030 src: /192.168.54.130:54272 dest: /192.168.54.130:50010
2015-11-22 21:28:40,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54272, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_603123127_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741854_1030, duration: 7416721
2015-11-22 21:28:40,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741854_1030, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:28:52,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2015-11-22 21:28:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-11-22 21:28:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-11-22 21:28:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741852_1028 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741852
2015-11-22 21:28:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741853_1029 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741853
2015-11-22 21:28:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741854_1030 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741854
2015-11-22 21:36:56,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741855_1031 src: /192.168.54.130:54290 dest: /192.168.54.130:50010
2015-11-22 21:36:56,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54290, dest: /192.168.54.130:50010, bytes: 120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1637685878_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741855_1031, duration: 49310799
2015-11-22 21:36:56,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741855_1031, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:36:56,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741856_1032 src: /192.168.54.130:54292 dest: /192.168.54.130:50010
2015-11-22 21:36:56,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54292, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1637685878_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741856_1032, duration: 3937460
2015-11-22 21:36:56,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741856_1032, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:36:56,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741857_1033 src: /192.168.54.130:54294 dest: /192.168.54.130:50010
2015-11-22 21:36:57,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54294, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1637685878_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741857_1033, duration: 9066676
2015-11-22 21:36:57,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741857_1033, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:37:07,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2015-11-22 21:37:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2015-11-22 21:37:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2015-11-22 21:37:07,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741856_1032 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741856
2015-11-22 21:37:07,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741857_1033 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741857
2015-11-22 21:37:07,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741855_1031 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741855
2015-11-22 21:39:55,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741858_1034 src: /192.168.54.130:54305 dest: /192.168.54.130:50010
2015-11-22 21:39:55,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54305, dest: /192.168.54.130:50010, bytes: 120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1363294562_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741858_1034, duration: 30296761
2015-11-22 21:39:55,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:39:55,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741859_1035 src: /192.168.54.130:54307 dest: /192.168.54.130:50010
2015-11-22 21:39:55,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54307, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1363294562_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741859_1035, duration: 4285367
2015-11-22 21:39:55,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741859_1035, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:39:55,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741860_1036 src: /192.168.54.130:54309 dest: /192.168.54.130:50010
2015-11-22 21:39:55,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54309, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1363294562_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741860_1036, duration: 8471138
2015-11-22 21:39:55,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741860_1036, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:40:04,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2015-11-22 21:40:04,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2015-11-22 21:40:04,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2015-11-22 21:40:04,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741858_1034 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741858
2015-11-22 21:40:04,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741859_1035 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741859
2015-11-22 21:40:04,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741860_1036 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741860
2015-11-22 21:42:13,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741861_1037 src: /192.168.54.130:54320 dest: /192.168.54.130:50010
2015-11-22 21:42:13,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54320, dest: /192.168.54.130:50010, bytes: 120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394813868_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741861_1037, duration: 32728989
2015-11-22 21:42:13,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741861_1037, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:42:13,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741862_1038 src: /192.168.54.130:54322 dest: /192.168.54.130:50010
2015-11-22 21:42:13,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54322, dest: /192.168.54.130:50010, bytes: 5034, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394813868_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741862_1038, duration: 10644394
2015-11-22 21:42:13,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741862_1038, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:42:13,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741863_1039 src: /192.168.54.130:54324 dest: /192.168.54.130:50010
2015-11-22 21:42:13,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54324, dest: /192.168.54.130:50010, bytes: 100510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394813868_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741863_1039, duration: 8187690
2015-11-22 21:42:13,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741863_1039, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 21:42:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2015-11-22 21:42:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2015-11-22 21:42:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2015-11-22 21:42:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741861_1037 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741861
2015-11-22 21:42:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741862_1038 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741862
2015-11-22 21:42:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741863_1039 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741863
2015-11-22 21:56:59,290 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1577938953-192.168.54.130-1448250860506 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-22 22:03:56,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741864_1040 src: /192.168.54.130:54361 dest: /192.168.54.130:50010
2015-11-22 22:03:56,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54361, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_543102006_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741864_1040, duration: 29930640
2015-11-22 22:03:56,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741864_1040, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:03:56,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741865_1041 src: /192.168.54.130:54363 dest: /192.168.54.130:50010
2015-11-22 22:03:56,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54363, dest: /192.168.54.130:50010, bytes: 5077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_543102006_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741865_1041, duration: 4289148
2015-11-22 22:03:56,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741865_1041, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:03:57,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741866_1042 src: /192.168.54.130:54365 dest: /192.168.54.130:50010
2015-11-22 22:03:57,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54365, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_543102006_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741866_1042, duration: 11979081
2015-11-22 22:03:57,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741866_1042, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:04:01,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2015-11-22 22:04:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2015-11-22 22:04:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2015-11-22 22:04:01,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741864_1040 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741864
2015-11-22 22:04:01,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741865_1041 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741865
2015-11-22 22:04:01,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741866_1042 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741866
2015-11-22 22:11:17,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741867_1043 src: /192.168.54.130:54382 dest: /192.168.54.130:50010
2015-11-22 22:11:17,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54382, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_345923469_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741867_1043, duration: 29552613
2015-11-22 22:11:17,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741867_1043, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:11:17,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741868_1044 src: /192.168.54.130:54384 dest: /192.168.54.130:50010
2015-11-22 22:11:17,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54384, dest: /192.168.54.130:50010, bytes: 5103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_345923469_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741868_1044, duration: 7688775
2015-11-22 22:11:17,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741868_1044, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:11:17,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741869_1045 src: /192.168.54.130:54386 dest: /192.168.54.130:50010
2015-11-22 22:11:17,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54386, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_345923469_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741869_1045, duration: 12515583
2015-11-22 22:11:17,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:11:22,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2015-11-22 22:11:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2015-11-22 22:11:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2015-11-22 22:11:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741867_1043 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741867
2015-11-22 22:11:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741868_1044 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741868
2015-11-22 22:11:22,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741869_1045 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741869
2015-11-22 22:12:08,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741870_1046 src: /192.168.54.130:54395 dest: /192.168.54.130:50010
2015-11-22 22:12:08,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54395, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1609140893_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741870_1046, duration: 32187232
2015-11-22 22:12:08,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741870_1046, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:12:08,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741871_1047 src: /192.168.54.130:54397 dest: /192.168.54.130:50010
2015-11-22 22:12:08,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54397, dest: /192.168.54.130:50010, bytes: 5103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1609140893_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741871_1047, duration: 3959543
2015-11-22 22:12:08,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741871_1047, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:12:08,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741872_1048 src: /192.168.54.130:54399 dest: /192.168.54.130:50010
2015-11-22 22:12:08,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54399, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1609140893_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741872_1048, duration: 9295886
2015-11-22 22:12:08,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741872_1048, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:12:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2015-11-22 22:12:13,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741872_1048 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741872
2015-11-22 22:12:13,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2015-11-22 22:12:13,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741870_1046 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741870
2015-11-22 22:12:13,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2015-11-22 22:12:13,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741871_1047 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741871
2015-11-22 22:13:32,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741873_1049 src: /192.168.54.130:54409 dest: /192.168.54.130:50010
2015-11-22 22:13:32,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54409, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1724313821_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741873_1049, duration: 30796716
2015-11-22 22:13:32,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741873_1049, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:13:32,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741874_1050 src: /192.168.54.130:54411 dest: /192.168.54.130:50010
2015-11-22 22:13:32,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54411, dest: /192.168.54.130:50010, bytes: 5103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1724313821_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741874_1050, duration: 4273463
2015-11-22 22:13:32,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741874_1050, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:13:32,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741875_1051 src: /192.168.54.130:54413 dest: /192.168.54.130:50010
2015-11-22 22:13:32,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54413, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1724313821_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741875_1051, duration: 7124775
2015-11-22 22:13:32,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741875_1051, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:14:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2015-11-22 22:14:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2015-11-22 22:14:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2015-11-22 22:14:43,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741873_1049 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741873
2015-11-22 22:14:43,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741874_1050 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741874
2015-11-22 22:14:43,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741875_1051 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741875
2015-11-22 22:18:25,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741876_1052 src: /192.168.54.130:54429 dest: /192.168.54.130:50010
2015-11-22 22:18:25,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54429, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1620325675_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741876_1052, duration: 31231605
2015-11-22 22:18:25,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741876_1052, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:18:25,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741877_1053 src: /192.168.54.130:54431 dest: /192.168.54.130:50010
2015-11-22 22:18:25,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54431, dest: /192.168.54.130:50010, bytes: 5103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1620325675_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741877_1053, duration: 11196192
2015-11-22 22:18:25,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741877_1053, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:18:26,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741878_1054 src: /192.168.54.130:54433 dest: /192.168.54.130:50010
2015-11-22 22:18:26,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54433, dest: /192.168.54.130:50010, bytes: 100500, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1620325675_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741878_1054, duration: 7318555
2015-11-22 22:18:26,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741878_1054, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:21:01,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2015-11-22 22:21:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2015-11-22 22:21:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2015-11-22 22:21:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741876_1052 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741876
2015-11-22 22:21:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741877_1053 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741877
2015-11-22 22:21:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741878_1054 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741878
2015-11-22 22:22:49,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741879_1055 src: /192.168.54.130:54452 dest: /192.168.54.130:50010
2015-11-22 22:22:49,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54452, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-308835977_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741879_1055, duration: 50790399
2015-11-22 22:22:49,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741879_1055, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:22:49,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741880_1056 src: /192.168.54.130:54454 dest: /192.168.54.130:50010
2015-11-22 22:22:49,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54454, dest: /192.168.54.130:50010, bytes: 5103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-308835977_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741880_1056, duration: 8127798
2015-11-22 22:22:49,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741880_1056, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:22:50,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741881_1057 src: /192.168.54.130:54456 dest: /192.168.54.130:50010
2015-11-22 22:22:50,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54456, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-308835977_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741881_1057, duration: 21645977
2015-11-22 22:22:50,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741881_1057, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:23:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2015-11-22 22:23:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2015-11-22 22:23:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2015-11-22 22:23:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741879_1055 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741879
2015-11-22 22:23:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741880_1056 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741880
2015-11-22 22:23:19,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741881_1057 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741881
2015-11-22 22:25:07,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741882_1058 src: /192.168.54.130:54466 dest: /192.168.54.130:50010
2015-11-22 22:25:07,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54466, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1365361739_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741882_1058, duration: 35761868
2015-11-22 22:25:07,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741882_1058, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:25:07,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741883_1059 src: /192.168.54.130:54468 dest: /192.168.54.130:50010
2015-11-22 22:25:07,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54468, dest: /192.168.54.130:50010, bytes: 5103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1365361739_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741883_1059, duration: 59024588
2015-11-22 22:25:07,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741883_1059, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:25:07,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741884_1060 src: /192.168.54.130:54470 dest: /192.168.54.130:50010
2015-11-22 22:25:07,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54470, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1365361739_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741884_1060, duration: 12542174
2015-11-22 22:25:07,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741884_1060, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:27:40,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2015-11-22 22:27:40,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2015-11-22 22:27:40,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741882_1058 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741882
2015-11-22 22:27:40,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741883_1059 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741883
2015-11-22 22:27:40,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2015-11-22 22:27:40,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741884_1060 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741884
2015-11-22 22:31:45,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741885_1061 src: /192.168.54.130:54490 dest: /192.168.54.130:50010
2015-11-22 22:31:45,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54490, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131269957_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741885_1061, duration: 42539566
2015-11-22 22:31:45,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741885_1061, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:31:45,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741886_1062 src: /192.168.54.130:54492 dest: /192.168.54.130:50010
2015-11-22 22:31:45,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54492, dest: /192.168.54.130:50010, bytes: 4752, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131269957_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741886_1062, duration: 8291458
2015-11-22 22:31:45,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741886_1062, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:31:45,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741887_1063 src: /192.168.54.130:54494 dest: /192.168.54.130:50010
2015-11-22 22:31:45,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54494, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1131269957_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741887_1063, duration: 21779028
2015-11-22 22:31:45,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741887_1063, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:33:52,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2015-11-22 22:33:52,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2015-11-22 22:33:52,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2015-11-22 22:33:52,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741885_1061 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741885
2015-11-22 22:33:52,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741886_1062 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741886
2015-11-22 22:33:52,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741887_1063 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741887
2015-11-22 22:37:57,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741888_1064 src: /192.168.54.130:54511 dest: /192.168.54.130:50010
2015-11-22 22:37:57,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54511, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1756792194_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741888_1064, duration: 39604890
2015-11-22 22:37:57,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741888_1064, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:37:57,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741889_1065 src: /192.168.54.130:54513 dest: /192.168.54.130:50010
2015-11-22 22:37:57,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54513, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1756792194_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741889_1065, duration: 7669532
2015-11-22 22:37:57,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741889_1065, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:37:57,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741890_1066 src: /192.168.54.130:54515 dest: /192.168.54.130:50010
2015-11-22 22:37:57,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54515, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1756792194_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741890_1066, duration: 12613779
2015-11-22 22:37:57,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741890_1066, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:38:04,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2015-11-22 22:38:04,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2015-11-22 22:38:04,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2015-11-22 22:38:04,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741888_1064 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741888
2015-11-22 22:38:04,471 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741889_1065 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741889
2015-11-22 22:38:04,471 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741890_1066 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741890
2015-11-22 22:38:50,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741891_1067 src: /192.168.54.130:54524 dest: /192.168.54.130:50010
2015-11-22 22:38:50,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54524, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-943699305_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741891_1067, duration: 33331902
2015-11-22 22:38:50,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741891_1067, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:38:50,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741892_1068 src: /192.168.54.130:54526 dest: /192.168.54.130:50010
2015-11-22 22:38:50,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54526, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-943699305_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741892_1068, duration: 6229068
2015-11-22 22:38:50,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741892_1068, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:38:51,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741893_1069 src: /192.168.54.130:54528 dest: /192.168.54.130:50010
2015-11-22 22:38:51,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54528, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-943699305_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741893_1069, duration: 15372058
2015-11-22 22:38:51,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741893_1069, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:41:13,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2015-11-22 22:41:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2015-11-22 22:41:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2015-11-22 22:41:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741891_1067 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741891
2015-11-22 22:41:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741892_1068 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741892
2015-11-22 22:41:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741893_1069 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741893
2015-11-22 22:43:11,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741894_1070 src: /192.168.54.130:54545 dest: /192.168.54.130:50010
2015-11-22 22:43:11,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54545, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_503010782_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741894_1070, duration: 43876362
2015-11-22 22:43:11,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741894_1070, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:43:11,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741895_1071 src: /192.168.54.130:54547 dest: /192.168.54.130:50010
2015-11-22 22:43:11,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54547, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_503010782_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741895_1071, duration: 7642750
2015-11-22 22:43:11,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741895_1071, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:43:11,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741896_1072 src: /192.168.54.130:54549 dest: /192.168.54.130:50010
2015-11-22 22:43:11,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54549, dest: /192.168.54.130:50010, bytes: 100502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_503010782_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741896_1072, duration: 17660087
2015-11-22 22:43:11,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741896_1072, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:43:52,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2015-11-22 22:43:52,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2015-11-22 22:43:52,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2015-11-22 22:43:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741894_1070 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741894
2015-11-22 22:43:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741895_1071 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741895
2015-11-22 22:43:52,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741896_1072 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741896
2015-11-22 22:47:08,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741897_1073 src: /192.168.54.130:54562 dest: /192.168.54.130:50010
2015-11-22 22:47:08,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54562, dest: /192.168.54.130:50010, bytes: 123, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-391281992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741897_1073, duration: 32136062
2015-11-22 22:47:08,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741897_1073, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:47:08,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741898_1074 src: /192.168.54.130:54564 dest: /192.168.54.130:50010
2015-11-22 22:47:08,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54564, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-391281992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741898_1074, duration: 6017958
2015-11-22 22:47:08,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741898_1074, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:47:08,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741899_1075 src: /192.168.54.130:54566 dest: /192.168.54.130:50010
2015-11-22 22:47:08,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54566, dest: /192.168.54.130:50010, bytes: 100503, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-391281992_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741899_1075, duration: 13367632
2015-11-22 22:47:08,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741899_1075, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:49:13,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741897_1073 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741897 for deletion
2015-11-22 22:49:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741898_1074 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741898 for deletion
2015-11-22 22:49:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2015-11-22 22:49:13,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741897_1073 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741897
2015-11-22 22:49:13,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741898_1074 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741898
2015-11-22 22:49:13,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741899_1075 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741899
2015-11-22 22:53:02,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741900_1076 src: /192.168.54.130:54583 dest: /192.168.54.130:50010
2015-11-22 22:53:03,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54583, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376176102_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741900_1076, duration: 46543791
2015-11-22 22:53:03,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741900_1076, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:53:03,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741901_1077 src: /192.168.54.130:54585 dest: /192.168.54.130:50010
2015-11-22 22:53:03,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54585, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376176102_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741901_1077, duration: 7431460
2015-11-22 22:53:03,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741901_1077, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:53:03,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741902_1078 src: /192.168.54.130:54587 dest: /192.168.54.130:50010
2015-11-22 22:53:03,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54587, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376176102_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741902_1078, duration: 17179251
2015-11-22 22:53:03,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741902_1078, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 22:53:13,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2015-11-22 22:53:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2015-11-22 22:53:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2015-11-22 22:53:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741900_1076 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741900
2015-11-22 22:53:13,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741901_1077 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741901
2015-11-22 22:53:13,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741902_1078 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741902
2015-11-22 23:06:50,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741903_1079 src: /192.168.54.130:54615 dest: /192.168.54.130:50010
2015-11-22 23:06:50,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54615, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2000576018_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741903_1079, duration: 34573498
2015-11-22 23:06:50,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741903_1079, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:06:50,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741904_1080 src: /192.168.54.130:54617 dest: /192.168.54.130:50010
2015-11-22 23:06:50,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54617, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2000576018_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741904_1080, duration: 5282710
2015-11-22 23:06:50,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741904_1080, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:06:50,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741905_1081 src: /192.168.54.130:54619 dest: /192.168.54.130:50010
2015-11-22 23:06:50,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54619, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2000576018_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741905_1081, duration: 22482110
2015-11-22 23:06:50,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741905_1081, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:07:07,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741904_1080 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741904 for deletion
2015-11-22 23:07:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2015-11-22 23:07:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2015-11-22 23:07:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741904_1080 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741904
2015-11-22 23:07:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741905_1081 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741905
2015-11-22 23:07:07,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741903_1079 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741903
2015-11-22 23:10:59,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741906_1082 src: /192.168.54.130:54631 dest: /192.168.54.130:50010
2015-11-22 23:10:59,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54631, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_128152182_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741906_1082, duration: 46402475
2015-11-22 23:10:59,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741906_1082, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:10:59,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741907_1083 src: /192.168.54.130:54633 dest: /192.168.54.130:50010
2015-11-22 23:10:59,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54633, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_128152182_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741907_1083, duration: 6172000
2015-11-22 23:10:59,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741907_1083, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:10:59,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741908_1084 src: /192.168.54.130:54635 dest: /192.168.54.130:50010
2015-11-22 23:10:59,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54635, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_128152182_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741908_1084, duration: 14115171
2015-11-22 23:10:59,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741908_1084, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:11:07,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2015-11-22 23:11:07,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2015-11-22 23:11:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2015-11-22 23:11:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741906_1082 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741906
2015-11-22 23:11:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741907_1083 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741907
2015-11-22 23:11:07,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741908_1084 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741908
2015-11-22 23:19:10,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741909_1085 src: /192.168.54.130:54655 dest: /192.168.54.130:50010
2015-11-22 23:19:10,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54655, dest: /192.168.54.130:50010, bytes: 120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-846491791_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741909_1085, duration: 34730695
2015-11-22 23:19:10,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741909_1085, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:19:10,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741910_1086 src: /192.168.54.130:54657 dest: /192.168.54.130:50010
2015-11-22 23:19:10,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54657, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-846491791_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741910_1086, duration: 6404799
2015-11-22 23:19:10,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741910_1086, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:19:10,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741911_1087 src: /192.168.54.130:54659 dest: /192.168.54.130:50010
2015-11-22 23:19:10,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54659, dest: /192.168.54.130:50010, bytes: 100500, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-846491791_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741911_1087, duration: 12996927
2015-11-22 23:19:10,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741911_1087, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:19:19,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2015-11-22 23:19:19,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2015-11-22 23:19:19,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2015-11-22 23:19:19,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741909_1085 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741909
2015-11-22 23:19:19,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741910_1086 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741910
2015-11-22 23:19:19,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741911_1087 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741911
2015-11-22 23:21:47,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741912_1088 src: /192.168.54.130:54671 dest: /192.168.54.130:50010
2015-11-22 23:21:47,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54671, dest: /192.168.54.130:50010, bytes: 120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2007404157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741912_1088, duration: 35177467
2015-11-22 23:21:47,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741912_1088, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:21:47,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741913_1089 src: /192.168.54.130:54673 dest: /192.168.54.130:50010
2015-11-22 23:21:47,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54673, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2007404157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741913_1089, duration: 4915580
2015-11-22 23:21:47,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741913_1089, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:21:47,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741914_1090 src: /192.168.54.130:54675 dest: /192.168.54.130:50010
2015-11-22 23:21:47,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54675, dest: /192.168.54.130:50010, bytes: 100518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2007404157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741914_1090, duration: 16205018
2015-11-22 23:21:47,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741914_1090, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:21:55,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2015-11-22 23:21:55,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2015-11-22 23:21:55,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2015-11-22 23:21:55,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741912_1088 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741912
2015-11-22 23:21:55,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741913_1089 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741913
2015-11-22 23:21:55,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741914_1090 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741914
2015-11-22 23:30:22,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741915_1091 src: /192.168.54.130:54693 dest: /192.168.54.130:50010
2015-11-22 23:30:22,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54693, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_443061096_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741915_1091, duration: 34229753
2015-11-22 23:30:22,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741915_1091, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:30:22,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741916_1092 src: /192.168.54.130:54695 dest: /192.168.54.130:50010
2015-11-22 23:30:22,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54695, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_443061096_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741916_1092, duration: 6003124
2015-11-22 23:30:22,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741916_1092, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:30:22,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741917_1093 src: /192.168.54.130:54697 dest: /192.168.54.130:50010
2015-11-22 23:30:22,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:54697, dest: /192.168.54.130:50010, bytes: 100520, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_443061096_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741917_1093, duration: 16857893
2015-11-22 23:30:22,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741917_1093, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-22 23:30:31,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2015-11-22 23:30:31,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2015-11-22 23:30:31,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2015-11-22 23:30:31,471 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741915_1091 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741915
2015-11-22 23:30:31,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741916_1092 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741916
2015-11-22 23:30:31,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741917_1093 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741917
2015-11-23 07:27:18,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8d0f81ed5634,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-23 07:27:18,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1577938953-192.168.54.130-1448250860506
2015-11-23 09:26:45,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741918_1094 src: /192.168.54.130:55058 dest: /192.168.54.130:50010
2015-11-23 09:26:45,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55058, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1532471482_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741918_1094, duration: 37710647
2015-11-23 09:26:45,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741918_1094, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:26:45,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741919_1095 src: /192.168.54.130:55060 dest: /192.168.54.130:50010
2015-11-23 09:26:45,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55060, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1532471482_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741919_1095, duration: 5399662
2015-11-23 09:26:45,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741919_1095, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:26:45,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741920_1096 src: /192.168.54.130:55062 dest: /192.168.54.130:50010
2015-11-23 09:26:45,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55062, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1532471482_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741920_1096, duration: 18584926
2015-11-23 09:26:45,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741920_1096, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:26:54,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2015-11-23 09:26:54,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2015-11-23 09:26:54,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2015-11-23 09:26:54,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741920_1096 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741920
2015-11-23 09:26:54,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741918_1094 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741918
2015-11-23 09:26:54,469 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741919_1095 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741919
2015-11-23 09:28:17,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741921_1097 src: /192.168.54.130:55071 dest: /192.168.54.130:50010
2015-11-23 09:28:17,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55071, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1151577657_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741921_1097, duration: 38230658
2015-11-23 09:28:17,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741921_1097, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:28:17,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741922_1098 src: /192.168.54.130:55073 dest: /192.168.54.130:50010
2015-11-23 09:28:17,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55073, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1151577657_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741922_1098, duration: 5589199
2015-11-23 09:28:17,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741922_1098, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:28:17,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741923_1099 src: /192.168.54.130:55075 dest: /192.168.54.130:50010
2015-11-23 09:28:17,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55075, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1151577657_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741923_1099, duration: 12773206
2015-11-23 09:28:17,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741923_1099, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:28:36,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2015-11-23 09:28:36,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2015-11-23 09:28:36,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741923_1099 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741923 for deletion
2015-11-23 09:28:36,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741921_1097 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741921
2015-11-23 09:28:36,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741922_1098 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741922
2015-11-23 09:28:36,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741923_1099 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741923
2015-11-23 09:29:07,256 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1577938953-192.168.54.130-1448250860506 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-23 09:31:52,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741924_1100 src: /192.168.54.130:55087 dest: /192.168.54.130:50010
2015-11-23 09:31:52,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55087, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1827161842_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741924_1100, duration: 38590251
2015-11-23 09:31:52,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741924_1100, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:31:52,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741925_1101 src: /192.168.54.130:55089 dest: /192.168.54.130:50010
2015-11-23 09:31:52,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55089, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1827161842_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741925_1101, duration: 5623830
2015-11-23 09:31:52,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741925_1101, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:31:52,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741926_1102 src: /192.168.54.130:55091 dest: /192.168.54.130:50010
2015-11-23 09:31:52,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55091, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1827161842_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741926_1102, duration: 15435358
2015-11-23 09:31:52,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741926_1102, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:32:00,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741924_1100 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741924 for deletion
2015-11-23 09:32:00,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2015-11-23 09:32:00,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2015-11-23 09:32:00,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741924_1100 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741924
2015-11-23 09:32:00,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741925_1101 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741925
2015-11-23 09:32:00,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741926_1102 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741926
2015-11-23 09:33:14,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741927_1103 src: /192.168.54.130:55100 dest: /192.168.54.130:50010
2015-11-23 09:33:14,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55100, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2030088845_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741927_1103, duration: 58231135
2015-11-23 09:33:14,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741927_1103, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:33:14,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741928_1104 src: /192.168.54.130:55102 dest: /192.168.54.130:50010
2015-11-23 09:33:14,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55102, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2030088845_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741928_1104, duration: 6718682
2015-11-23 09:33:14,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741928_1104, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:33:14,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741929_1105 src: /192.168.54.130:55104 dest: /192.168.54.130:50010
2015-11-23 09:33:14,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55104, dest: /192.168.54.130:50010, bytes: 100499, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2030088845_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741929_1105, duration: 11961079
2015-11-23 09:33:14,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:33:30,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2015-11-23 09:33:30,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741928_1104 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741928 for deletion
2015-11-23 09:33:30,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2015-11-23 09:33:30,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741927_1103 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741927
2015-11-23 09:33:30,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741928_1104 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741928
2015-11-23 09:33:30,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741929_1105 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741929
2015-11-23 09:40:07,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741930_1106 src: /192.168.54.130:55120 dest: /192.168.54.130:50010
2015-11-23 09:40:07,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55120, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237954564_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741930_1106, duration: 38608941
2015-11-23 09:40:07,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741930_1106, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:40:07,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741931_1107 src: /192.168.54.130:55122 dest: /192.168.54.130:50010
2015-11-23 09:40:07,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55122, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237954564_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741931_1107, duration: 5381591
2015-11-23 09:40:07,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741931_1107, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:40:07,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741932_1108 src: /192.168.54.130:55124 dest: /192.168.54.130:50010
2015-11-23 09:40:07,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55124, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1237954564_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741932_1108, duration: 6892969
2015-11-23 09:40:07,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741932_1108, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:40:15,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2015-11-23 09:40:15,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2015-11-23 09:40:15,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2015-11-23 09:40:15,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741930_1106 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741930
2015-11-23 09:40:15,463 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741931_1107 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741931
2015-11-23 09:40:15,463 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741932_1108 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741932
2015-11-23 09:40:52,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741933_1109 src: /192.168.54.130:55133 dest: /192.168.54.130:50010
2015-11-23 09:40:52,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55133, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1874363631_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741933_1109, duration: 37867341
2015-11-23 09:40:52,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741933_1109, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:40:52,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741934_1110 src: /192.168.54.130:55135 dest: /192.168.54.130:50010
2015-11-23 09:40:52,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55135, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1874363631_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741934_1110, duration: 6672870
2015-11-23 09:40:52,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741934_1110, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:40:52,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741935_1111 src: /192.168.54.130:55137 dest: /192.168.54.130:50010
2015-11-23 09:40:52,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55137, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1874363631_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741935_1111, duration: 13949007
2015-11-23 09:40:52,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741935_1111, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 09:41:09,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2015-11-23 09:41:09,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741933_1109 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741933
2015-11-23 09:41:09,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2015-11-23 09:41:09,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741934_1110 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741934
2015-11-23 09:41:09,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2015-11-23 09:41:09,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741935_1111 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741935
2015-11-23 10:32:42,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741936_1112 src: /192.168.54.130:55214 dest: /192.168.54.130:50010
2015-11-23 10:32:42,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55214, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1143769441_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741936_1112, duration: 35667073
2015-11-23 10:32:42,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741936_1112, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:32:42,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741937_1113 src: /192.168.54.130:55216 dest: /192.168.54.130:50010
2015-11-23 10:32:42,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55216, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1143769441_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741937_1113, duration: 5832570
2015-11-23 10:32:42,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741937_1113, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:32:42,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741938_1114 src: /192.168.54.130:55218 dest: /192.168.54.130:50010
2015-11-23 10:32:42,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55218, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1143769441_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741938_1114, duration: 19080489
2015-11-23 10:32:42,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741938_1114, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:32:48,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741936_1112 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741936 for deletion
2015-11-23 10:32:48,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2015-11-23 10:32:48,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741938_1114 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741938 for deletion
2015-11-23 10:32:48,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741936_1112 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741936
2015-11-23 10:32:48,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741937_1113 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741937
2015-11-23 10:32:48,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741938_1114 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741938
2015-11-23 10:35:01,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741939_1115 src: /192.168.54.130:55229 dest: /192.168.54.130:50010
2015-11-23 10:35:01,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55229, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629891885_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741939_1115, duration: 33079108
2015-11-23 10:35:01,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741939_1115, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:35:01,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741940_1116 src: /192.168.54.130:55231 dest: /192.168.54.130:50010
2015-11-23 10:35:01,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55231, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629891885_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741940_1116, duration: 7304980
2015-11-23 10:35:01,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741940_1116, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:35:01,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741941_1117 src: /192.168.54.130:55233 dest: /192.168.54.130:50010
2015-11-23 10:35:01,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55233, dest: /192.168.54.130:50010, bytes: 100517, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629891885_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741941_1117, duration: 17529458
2015-11-23 10:35:01,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741941_1117, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:35:12,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741939_1115 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741939 for deletion
2015-11-23 10:35:12,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741940_1116 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741940 for deletion
2015-11-23 10:35:12,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2015-11-23 10:35:12,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741939_1115 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741939
2015-11-23 10:35:12,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741940_1116 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741940
2015-11-23 10:35:12,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741941_1117 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741941
2015-11-23 10:36:18,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741942_1118 src: /192.168.54.130:55244 dest: /192.168.54.130:50010
2015-11-23 10:36:18,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55244, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-358195358_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741942_1118, duration: 37714786
2015-11-23 10:36:18,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741942_1118, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:36:18,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741943_1119 src: /192.168.54.130:55246 dest: /192.168.54.130:50010
2015-11-23 10:36:18,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55246, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-358195358_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741943_1119, duration: 6274871
2015-11-23 10:36:18,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741943_1119, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:36:18,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741944_1120 src: /192.168.54.130:55248 dest: /192.168.54.130:50010
2015-11-23 10:36:18,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55248, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-358195358_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741944_1120, duration: 12025054
2015-11-23 10:36:18,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741944_1120, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 10:36:33,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741942_1118 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741942 for deletion
2015-11-23 10:36:33,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2015-11-23 10:36:33,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2015-11-23 10:36:33,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741942_1118 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741942
2015-11-23 10:36:33,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741943_1119 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741943
2015-11-23 10:36:33,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741944_1120 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741944
2015-11-23 11:35:08,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741945_1121 src: /192.168.54.130:55288 dest: /192.168.54.130:50010
2015-11-23 11:35:08,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55288, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1197561841_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741945_1121, duration: 34768112
2015-11-23 11:35:08,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741945_1121, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:35:08,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741946_1122 src: /192.168.54.130:55290 dest: /192.168.54.130:50010
2015-11-23 11:35:08,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55290, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1197561841_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741946_1122, duration: 8449538
2015-11-23 11:35:08,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741946_1122, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:35:08,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741947_1123 src: /192.168.54.130:55292 dest: /192.168.54.130:50010
2015-11-23 11:35:08,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55292, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1197561841_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741947_1123, duration: 12593966
2015-11-23 11:35:08,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741947_1123, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:35:26,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741945_1121 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741945 for deletion
2015-11-23 11:35:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741946_1122 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741946 for deletion
2015-11-23 11:35:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741947_1123 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741947 for deletion
2015-11-23 11:35:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741945_1121 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741945
2015-11-23 11:35:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741946_1122 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741946
2015-11-23 11:35:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741947_1123 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741947
2015-11-23 11:38:29,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741948_1124 src: /192.168.54.130:55304 dest: /192.168.54.130:50010
2015-11-23 11:38:29,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55304, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330510239_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741948_1124, duration: 37768439
2015-11-23 11:38:29,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741948_1124, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:38:29,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741949_1125 src: /192.168.54.130:55306 dest: /192.168.54.130:50010
2015-11-23 11:38:29,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55306, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330510239_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741949_1125, duration: 5225624
2015-11-23 11:38:29,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741949_1125, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:38:29,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741950_1126 src: /192.168.54.130:55308 dest: /192.168.54.130:50010
2015-11-23 11:38:29,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55308, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330510239_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741950_1126, duration: 14895935
2015-11-23 11:38:29,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741950_1126, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:38:38,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741948_1124 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741948 for deletion
2015-11-23 11:38:38,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741949_1125 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741949 for deletion
2015-11-23 11:38:38,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741950_1126 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741950 for deletion
2015-11-23 11:38:38,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741948_1124 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741948
2015-11-23 11:38:38,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741949_1125 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741949
2015-11-23 11:38:38,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741950_1126 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741950
2015-11-23 11:53:18,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741951_1127 src: /192.168.54.130:55335 dest: /192.168.54.130:50010
2015-11-23 11:53:18,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55335, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1290229507_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741951_1127, duration: 36136684
2015-11-23 11:53:18,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741951_1127, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:53:18,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741952_1128 src: /192.168.54.130:55337 dest: /192.168.54.130:50010
2015-11-23 11:53:18,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55337, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1290229507_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741952_1128, duration: 6736217
2015-11-23 11:53:18,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741952_1128, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:53:18,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741953_1129 src: /192.168.54.130:55339 dest: /192.168.54.130:50010
2015-11-23 11:53:18,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55339, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1290229507_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741953_1129, duration: 14677291
2015-11-23 11:53:18,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741953_1129, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:53:26,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2015-11-23 11:53:26,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2015-11-23 11:53:26,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2015-11-23 11:53:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741952_1128 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741952
2015-11-23 11:53:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741953_1129 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741953
2015-11-23 11:53:26,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741951_1127 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741951
2015-11-23 11:55:14,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741954_1130 src: /192.168.54.130:55349 dest: /192.168.54.130:50010
2015-11-23 11:55:14,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55349, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1871502780_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741954_1130, duration: 43796165
2015-11-23 11:55:14,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:55:14,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741955_1131 src: /192.168.54.130:55351 dest: /192.168.54.130:50010
2015-11-23 11:55:14,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55351, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1871502780_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741955_1131, duration: 6685203
2015-11-23 11:55:14,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741955_1131, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:55:14,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741956_1132 src: /192.168.54.130:55353 dest: /192.168.54.130:50010
2015-11-23 11:55:14,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55353, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1871502780_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741956_1132, duration: 22057291
2015-11-23 11:55:14,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741956_1132, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 11:55:32,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2015-11-23 11:55:32,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2015-11-23 11:55:32,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741956_1132 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741956 for deletion
2015-11-23 11:55:32,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741954_1130 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741954
2015-11-23 11:55:32,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741955_1131 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741955
2015-11-23 11:55:32,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741956_1132 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741956
2015-11-23 12:10:38,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741957_1133 src: /192.168.54.130:55380 dest: /192.168.54.130:50010
2015-11-23 12:10:38,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55380, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_414331922_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741957_1133, duration: 39787124
2015-11-23 12:10:38,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741957_1133, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:10:38,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741958_1134 src: /192.168.54.130:55382 dest: /192.168.54.130:50010
2015-11-23 12:10:38,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55382, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_414331922_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741958_1134, duration: 5205668
2015-11-23 12:10:38,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741958_1134, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:10:38,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741959_1135 src: /192.168.54.130:55384 dest: /192.168.54.130:50010
2015-11-23 12:10:38,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55384, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_414331922_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741959_1135, duration: 11934049
2015-11-23 12:10:38,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741959_1135, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:10:50,604 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2015-11-23 12:10:50,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741958_1134 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741958 for deletion
2015-11-23 12:10:50,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741959_1135 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741959 for deletion
2015-11-23 12:10:50,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741957_1133 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741957
2015-11-23 12:10:50,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741958_1134 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741958
2015-11-23 12:10:50,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741959_1135 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741959
2015-11-23 12:10:51,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741960_1136 src: /192.168.54.130:55392 dest: /192.168.54.130:50010
2015-11-23 12:10:51,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55392, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1913791486_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741960_1136, duration: 38373939
2015-11-23 12:10:51,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741960_1136, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:10:51,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741961_1137 src: /192.168.54.130:55394 dest: /192.168.54.130:50010
2015-11-23 12:10:51,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55394, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1913791486_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741961_1137, duration: 5157449
2015-11-23 12:10:51,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741961_1137, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:10:51,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741962_1138 src: /192.168.54.130:55396 dest: /192.168.54.130:50010
2015-11-23 12:10:51,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55396, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1913791486_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741962_1138, duration: 13916660
2015-11-23 12:10:51,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741962_1138, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:11:08,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741960_1136 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741960 for deletion
2015-11-23 12:11:08,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741960_1136 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741960
2015-11-23 12:11:08,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741961_1137 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741961 for deletion
2015-11-23 12:11:08,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741962_1138 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741962 for deletion
2015-11-23 12:11:08,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741961_1137 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741961
2015-11-23 12:11:08,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741962_1138 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741962
2015-11-23 12:17:39,103 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1021ms
No GCs detected
2015-11-23 12:21:53,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741963_1139 src: /192.168.54.130:55420 dest: /192.168.54.130:50010
2015-11-23 12:21:53,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55420, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_854232665_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741963_1139, duration: 34691546
2015-11-23 12:21:53,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741963_1139, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:21:53,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741964_1140 src: /192.168.54.130:55422 dest: /192.168.54.130:50010
2015-11-23 12:21:53,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55422, dest: /192.168.54.130:50010, bytes: 4000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_854232665_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741964_1140, duration: 4910940
2015-11-23 12:21:54,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741964_1140, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:21:54,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741965_1141 src: /192.168.54.130:55424 dest: /192.168.54.130:50010
2015-11-23 12:21:54,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55424, dest: /192.168.54.130:50010, bytes: 100519, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_854232665_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741965_1141, duration: 13324084
2015-11-23 12:21:54,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741965_1141, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:22:02,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2015-11-23 12:22:02,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2015-11-23 12:22:02,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2015-11-23 12:22:02,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741963_1139 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741963
2015-11-23 12:22:02,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741964_1140 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741964
2015-11-23 12:22:02,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741965_1141 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741965
2015-11-23 12:23:44,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741966_1142 src: /192.168.54.130:55434 dest: /192.168.54.130:50010
2015-11-23 12:23:44,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55434, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2114208157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741966_1142, duration: 35348352
2015-11-23 12:23:44,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741966_1142, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:23:44,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741967_1143 src: /192.168.54.130:55436 dest: /192.168.54.130:50010
2015-11-23 12:23:44,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55436, dest: /192.168.54.130:50010, bytes: 4667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2114208157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741967_1143, duration: 7179330
2015-11-23 12:23:44,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741967_1143, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:23:45,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741968_1144 src: /192.168.54.130:55438 dest: /192.168.54.130:50010
2015-11-23 12:23:45,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55438, dest: /192.168.54.130:50010, bytes: 100501, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2114208157_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741968_1144, duration: 12497650
2015-11-23 12:23:45,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741968_1144, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 12:23:59,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741968_1144 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741968 for deletion
2015-11-23 12:23:59,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2015-11-23 12:23:59,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2015-11-23 12:23:59,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741968_1144 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741968
2015-11-23 12:23:59,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741966_1142 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741966
2015-11-23 12:23:59,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741967_1143 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741967
2015-11-23 14:21:54,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741969_1145 src: /192.168.54.130:55525 dest: /192.168.54.130:50010
2015-11-23 14:21:54,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55525, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1664298610_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741969_1145, duration: 41263984
2015-11-23 14:21:54,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741969_1145, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 14:21:55,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741970_1146 src: /192.168.54.130:55527 dest: /192.168.54.130:50010
2015-11-23 14:21:55,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55527, dest: /192.168.54.130:50010, bytes: 4352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1664298610_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741970_1146, duration: 5275133
2015-11-23 14:21:55,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741970_1146, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 14:21:55,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741971_1147 src: /192.168.54.130:55529 dest: /192.168.54.130:50010
2015-11-23 14:21:55,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55529, dest: /192.168.54.130:50010, bytes: 100503, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1664298610_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741971_1147, duration: 24298624
2015-11-23 14:21:55,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741971_1147, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 14:22:08,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2015-11-23 14:22:08,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741970_1146 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741970 for deletion
2015-11-23 14:22:08,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2015-11-23 14:22:08,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741969_1145 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741969
2015-11-23 14:22:08,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741970_1146 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741970
2015-11-23 14:22:08,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741971_1147 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741971
2015-11-23 14:42:14,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741972_1148 src: /192.168.54.130:55571 dest: /192.168.54.130:50010
2015-11-23 14:42:14,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55571, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015188006_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741972_1148, duration: 53542419
2015-11-23 14:42:14,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741972_1148, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 14:42:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741973_1149 src: /192.168.54.130:55573 dest: /192.168.54.130:50010
2015-11-23 14:42:14,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55573, dest: /192.168.54.130:50010, bytes: 3694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015188006_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741973_1149, duration: 5520822
2015-11-23 14:42:14,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741973_1149, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 14:42:14,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741974_1150 src: /192.168.54.130:55575 dest: /192.168.54.130:50010
2015-11-23 14:42:14,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55575, dest: /192.168.54.130:50010, bytes: 100525, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015188006_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741974_1150, duration: 12983255
2015-11-23 14:42:14,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741974_1150, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 14:42:26,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741972_1148 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741972 for deletion
2015-11-23 14:42:26,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741973_1149 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741973 for deletion
2015-11-23 14:42:26,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741974_1150 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741974 for deletion
2015-11-23 14:42:26,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741972_1148 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741972
2015-11-23 14:42:26,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741973_1149 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741973
2015-11-23 14:42:26,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741974_1150 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741974
2015-11-23 15:12:33,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741975_1151 src: /192.168.54.130:55604 dest: /192.168.54.130:50010
2015-11-23 15:12:33,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55604, dest: /192.168.54.130:50010, bytes: 121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1003011830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741975_1151, duration: 37853600
2015-11-23 15:12:33,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741975_1151, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:12:33,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741976_1152 src: /192.168.54.130:55606 dest: /192.168.54.130:50010
2015-11-23 15:12:34,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55606, dest: /192.168.54.130:50010, bytes: 4193, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1003011830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741976_1152, duration: 3712234
2015-11-23 15:12:34,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741976_1152, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:12:34,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741977_1153 src: /192.168.54.130:55608 dest: /192.168.54.130:50010
2015-11-23 15:12:34,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55608, dest: /192.168.54.130:50010, bytes: 100511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1003011830_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741977_1153, duration: 17079407
2015-11-23 15:12:34,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741977_1153, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:12:51,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741975_1151 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741975 for deletion
2015-11-23 15:12:51,190 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2015-11-23 15:12:51,190 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741977_1153 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741977 for deletion
2015-11-23 15:12:51,190 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741975_1151 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741975
2015-11-23 15:12:51,190 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741976_1152 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741976
2015-11-23 15:12:51,190 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741977_1153 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741977
2015-11-23 15:33:30,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa0b4a65b0ead,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-23 15:33:30,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1577938953-192.168.54.130-1448250860506
2015-11-23 15:44:14,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741978_1154 src: /192.168.54.130:55662 dest: /192.168.54.130:50010
2015-11-23 15:44:14,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55662, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066012179_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741978_1154, duration: 41452584
2015-11-23 15:44:14,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741978_1154, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:44:14,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741979_1155 src: /192.168.54.130:55664 dest: /192.168.54.130:50010
2015-11-23 15:44:14,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55664, dest: /192.168.54.130:50010, bytes: 4193, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066012179_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741979_1155, duration: 5530178
2015-11-23 15:44:14,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741979_1155, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:44:14,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741980_1156 src: /192.168.54.130:55666 dest: /192.168.54.130:50010
2015-11-23 15:44:14,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55666, dest: /192.168.54.130:50010, bytes: 100512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066012179_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741980_1156, duration: 12320946
2015-11-23 15:44:14,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741980_1156, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:45:03,185 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741978_1154 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741978 for deletion
2015-11-23 15:45:03,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741979_1155 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741979 for deletion
2015-11-23 15:45:03,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741980_1156 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741980 for deletion
2015-11-23 15:45:03,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741978_1154 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741978
2015-11-23 15:45:03,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741979_1155 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741979
2015-11-23 15:45:03,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741980_1156 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741980
2015-11-23 15:46:15,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741981_1157 src: /192.168.54.130:55676 dest: /192.168.54.130:50010
2015-11-23 15:46:15,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55676, dest: /192.168.54.130:50010, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1916990411_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741981_1157, duration: 36611293
2015-11-23 15:46:15,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741981_1157, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:15,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741982_1158 src: /192.168.54.130:55678 dest: /192.168.54.130:50010
2015-11-23 15:46:15,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55678, dest: /192.168.54.130:50010, bytes: 3694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1916990411_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741982_1158, duration: 5098416
2015-11-23 15:46:15,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741982_1158, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:15,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1577938953-192.168.54.130-1448250860506:blk_1073741983_1159 src: /192.168.54.130:55680 dest: /192.168.54.130:50010
2015-11-23 15:46:15,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.54.130:55680, dest: /192.168.54.130:50010, bytes: 100526, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1916990411_1, offset: 0, srvID: f89b6cdf-a730-4a11-9e8f-4220150b1d5c, blockid: BP-1577938953-192.168.54.130-1448250860506:blk_1073741983_1159, duration: 13067474
2015-11-23 15:46:15,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1577938953-192.168.54.130-1448250860506:blk_1073741983_1159, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:24,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741981_1157 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741981 for deletion
2015-11-23 15:46:24,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741982_1158 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741982 for deletion
2015-11-23 15:46:24,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741983_1159 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741983 for deletion
2015-11-23 15:46:24,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741981_1157 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741981
2015-11-23 15:46:24,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741982_1158 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741982
2015-11-23 15:46:24,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1577938953-192.168.54.130-1448250860506 blk_1073741983_1159 file /usr/local/hadoop-2.7.1/hdfs/datanode/current/BP-1577938953-192.168.54.130-1448250860506/current/finalized/subdir0/subdir0/blk_1073741983
2015-11-23 20:55:35,783 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 40477ms
No GCs detected
2015-11-23 21:33:06,475 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1577938953-192.168.54.130-1448250860506 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
